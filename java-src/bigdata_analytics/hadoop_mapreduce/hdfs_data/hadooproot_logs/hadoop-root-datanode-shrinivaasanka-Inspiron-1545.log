2015-02-13 16:41:18,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-13 16:41:18,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-13 16:41:18,989 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-13 16:41:19,746 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-13 16:41:19,903 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-13 16:41:19,903 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-13 16:41:19,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-13 16:41:19,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-13 16:41:21,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Shutdown complete.
2015-02-13 16:41:21,170 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.net.BindException: Problem binding to [0.0.0.0:50010] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:720)
	at org.apache.hadoop.ipc.Server.bind(Server.java:424)
	at org.apache.hadoop.ipc.Server.bind(Server.java:396)
	at org.apache.hadoop.hdfs.net.TcpPeerServer.<init>(TcpPeerServer.java:111)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:843)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1056)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:415)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2268)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.hadoop.ipc.Server.bind(Server.java:407)
	... 10 more
2015-02-13 16:41:21,176 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-02-13 16:41:21,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-13 16:49:36,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-13 16:49:36,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-13 16:49:37,332 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-13 16:49:38,000 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-13 16:49:38,179 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-13 16:49:38,179 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-13 16:49:38,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-13 16:49:38,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-13 16:49:39,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Shutdown complete.
2015-02-13 16:49:39,483 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.net.BindException: Problem binding to [0.0.0.0:50010] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:720)
	at org.apache.hadoop.ipc.Server.bind(Server.java:424)
	at org.apache.hadoop.ipc.Server.bind(Server.java:396)
	at org.apache.hadoop.hdfs.net.TcpPeerServer.<init>(TcpPeerServer.java:111)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initDataXceiver(DataNode.java:843)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1056)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:415)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2268)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.hadoop.ipc.Server.bind(Server.java:407)
	... 10 more
2015-02-13 16:49:39,492 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-02-13 16:49:39,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-13 16:53:39,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-13 16:53:39,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-13 16:53:39,857 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-13 16:53:40,379 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-13 16:53:40,546 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-13 16:53:40,546 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-13 16:53:40,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-13 16:53:40,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-13 16:53:40,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-13 16:53:40,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-13 16:53:40,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-13 16:53:41,888 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-13 16:53:41,895 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-13 16:53:41,912 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-13 16:53:41,915 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-13 16:53:41,916 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-13 16:53:41,916 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-13 16:53:41,962 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-13 16:53:41,968 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-13 16:53:41,968 INFO org.mortbay.log: jetty-6.1.26
2015-02-13 16:53:43,009 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-13 16:53:43,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-13 16:53:43,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-13 16:53:44,691 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-13 16:53:44,741 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-13 16:53:44,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-13 16:53:44,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-13 16:53:44,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-13 16:53:45,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-13 16:53:45,066 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-13 16:53:45,069 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-13 16:53:45,879 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-13 16:53:45,926 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 27391@shrinivaasanka-Inspiron-1545
2015-02-13 16:53:45,928 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-284833878-127.0.0.1-1423826544598
2015-02-13 16:53:45,928 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-13 16:53:46,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-284833878-127.0.0.1-1423826544598
2015-02-13 16:53:46,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-13 16:53:46,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-284833878-127.0.0.1-1423826544598 is not formatted.
2015-02-13 16:53:46,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-13 16:53:46,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-284833878-127.0.0.1-1423826544598 directory /tmp/hadoop-root/dfs/data/current/BP-284833878-127.0.0.1-1423826544598/current
2015-02-13 16:53:46,509 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-13 16:53:46,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1195563644;bpid=BP-284833878-127.0.0.1-1423826544598;lv=-56;nsInfo=lv=-60;cid=CID-932bfd8d-d140-442e-869c-a6887d5f3489;nsid=1195563644;c=0;bpid=BP-284833878-127.0.0.1-1423826544598;dnuuid=null
2015-02-13 16:53:46,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID e944d56a-ec40-4c5c-ae75-79c4ae32cd08
2015-02-13 16:53:46,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-13 16:53:46,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-13 16:53:46,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-13 16:53:46,831 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1423827908831 with interval 21600000
2015-02-13 16:53:46,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-284833878-127.0.0.1-1423826544598
2015-02-13 16:53:46,840 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-284833878-127.0.0.1-1423826544598 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-13 16:53:46,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-284833878-127.0.0.1-1423826544598 on /tmp/hadoop-root/dfs/data/current: 89ms
2015-02-13 16:53:46,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-284833878-127.0.0.1-1423826544598: 118ms
2015-02-13 16:53:46,956 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-284833878-127.0.0.1-1423826544598 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-13 16:53:46,957 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-284833878-127.0.0.1-1423826544598 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-13 16:53:46,957 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2015-02-13 16:53:46,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-284833878-127.0.0.1-1423826544598 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-13 16:53:47,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-284833878-127.0.0.1-1423826544598 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-13 16:53:47,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-13 16:53:47,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-284833878-127.0.0.1-1423826544598 (Datanode Uuid e944d56a-ec40-4c5c-ae75-79c4ae32cd08) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-13 16:53:47,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-284833878-127.0.0.1-1423826544598 (Datanode Uuid e944d56a-ec40-4c5c-ae75-79c4ae32cd08) service to localhost/127.0.0.1:9000
2015-02-13 16:53:47,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 7 msec to generate and 235 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@1ba2255
2015-02-13 16:53:47,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-284833878-127.0.0.1-1423826544598
2015-02-13 16:53:47,869 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-13 16:53:47,869 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-13 16:53:47,871 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-13 16:53:47,871 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-13 16:53:47,873 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-284833878-127.0.0.1-1423826544598
2015-02-13 16:53:47,891 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-284833878-127.0.0.1-1423826544598 to blockPoolScannerMap, new size=1
2015-02-13 16:55:04,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-284833878-127.0.0.1-1423826544598:blk_1073741825_1001 src: /127.0.0.1:41538 dest: /127.0.0.1:50010
2015-02-13 16:55:04,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41538, dest: /127.0.0.1:50010, bytes: 1553, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2025665040_1, offset: 0, srvID: e944d56a-ec40-4c5c-ae75-79c4ae32cd08, blockid: BP-284833878-127.0.0.1-1423826544598:blk_1073741825_1001, duration: 276390105
2015-02-13 16:55:04,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-284833878-127.0.0.1-1423826544598:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-13 16:55:11,865 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-284833878-127.0.0.1-1423826544598:blk_1073741825_1001
2015-02-13 16:55:58,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-284833878-127.0.0.1-1423826544598:blk_1073741826_1002 src: /127.0.0.1:41542 dest: /127.0.0.1:50010
2015-02-13 16:55:58,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41542, dest: /127.0.0.1:50010, bytes: 1446, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1766373837_1, offset: 0, srvID: e944d56a-ec40-4c5c-ae75-79c4ae32cd08, blockid: BP-284833878-127.0.0.1-1423826544598:blk_1073741826_1002, duration: 48976898
2015-02-13 16:55:58,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-284833878-127.0.0.1-1423826544598:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-13 16:56:06,897 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-284833878-127.0.0.1-1423826544598:blk_1073741826_1002
2015-02-13 16:57:53,222 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /tmp/hadoop-root/dfs/data/current/BP-284833878-127.0.0.1-1423826544598/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-02-13 16:57:53,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-284833878-127.0.0.1-1423826544598 blk_1073741825_1001 file /tmp/hadoop-root/dfs/data/current/BP-284833878-127.0.0.1-1423826544598/current/finalized/subdir0/subdir0/blk_1073741825
2015-02-13 16:58:08,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-284833878-127.0.0.1-1423826544598:blk_1073741827_1003 src: /127.0.0.1:41555 dest: /127.0.0.1:50010
2015-02-13 16:58:08,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41555, dest: /127.0.0.1:50010, bytes: 217011, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-324667919_1, offset: 0, srvID: e944d56a-ec40-4c5c-ae75-79c4ae32cd08, blockid: BP-284833878-127.0.0.1-1423826544598:blk_1073741827_1003, duration: 102901843
2015-02-13 16:58:08,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-284833878-127.0.0.1-1423826544598:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-13 16:58:16,922 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-284833878-127.0.0.1-1423826544598:blk_1073741827_1003
2015-02-13 16:59:11,201 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "shrinivaasanka-Inspiron-1545/127.0.0.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-02-13 16:59:15,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-13 16:59:16,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-13 16:59:17,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-13 16:59:17,850 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-13 16:59:17,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-13 17:02:33,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-13 17:02:33,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-13 17:02:34,664 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-13 17:02:35,308 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-13 17:02:35,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-13 17:02:35,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-13 17:02:35,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-13 17:02:35,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-13 17:02:35,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-13 17:02:35,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-13 17:02:35,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-13 17:02:37,118 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-13 17:02:37,151 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-13 17:02:37,188 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-13 17:02:37,199 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-13 17:02:37,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-13 17:02:37,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-13 17:02:37,299 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-13 17:02:37,314 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-13 17:02:37,314 INFO org.mortbay.log: jetty-6.1.26
2015-02-13 17:02:38,749 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-13 17:02:38,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-13 17:02:38,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-13 17:02:40,251 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-13 17:02:40,332 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-13 17:02:40,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-13 17:02:40,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-13 17:02:40,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-13 17:02:40,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-13 17:02:40,764 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-13 17:02:40,825 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-13 17:02:42,269 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-13 17:02:42,334 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 29354@shrinivaasanka-Inspiron-1545
2015-02-13 17:02:42,337 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-1336133248-127.0.0.1-1423827120740
2015-02-13 17:02:42,337 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-13 17:02:42,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1336133248-127.0.0.1-1423827120740
2015-02-13 17:02:42,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-13 17:02:42,739 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-1336133248-127.0.0.1-1423827120740 is not formatted.
2015-02-13 17:02:42,739 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-13 17:02:42,743 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1336133248-127.0.0.1-1423827120740 directory /tmp/hadoop-root/dfs/data/current/BP-1336133248-127.0.0.1-1423827120740/current
2015-02-13 17:02:42,770 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-13 17:02:42,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=667767399;bpid=BP-1336133248-127.0.0.1-1423827120740;lv=-56;nsInfo=lv=-60;cid=CID-9a9574cb-85b0-47ef-a85d-5b3dcce25413;nsid=667767399;c=0;bpid=BP-1336133248-127.0.0.1-1423827120740;dnuuid=null
2015-02-13 17:02:42,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 4af6c814-62f2-4c29-b3fe-dede61b82d0f
2015-02-13 17:02:43,060 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-13 17:02:43,063 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-13 17:02:43,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-13 17:02:43,140 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1423847626139 with interval 21600000
2015-02-13 17:02:43,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1336133248-127.0.0.1-1423827120740
2015-02-13 17:02:43,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1336133248-127.0.0.1-1423827120740 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-13 17:02:43,231 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1336133248-127.0.0.1-1423827120740 on /tmp/hadoop-root/dfs/data/current: 78ms
2015-02-13 17:02:43,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1336133248-127.0.0.1-1423827120740: 88ms
2015-02-13 17:02:43,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1336133248-127.0.0.1-1423827120740 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-13 17:02:43,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1336133248-127.0.0.1-1423827120740 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-13 17:02:43,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2015-02-13 17:02:43,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1336133248-127.0.0.1-1423827120740 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-13 17:02:43,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1336133248-127.0.0.1-1423827120740 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-13 17:02:43,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-13 17:02:44,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1336133248-127.0.0.1-1423827120740 (Datanode Uuid 4af6c814-62f2-4c29-b3fe-dede61b82d0f) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-13 17:02:44,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1336133248-127.0.0.1-1423827120740 (Datanode Uuid 4af6c814-62f2-4c29-b3fe-dede61b82d0f) service to localhost/127.0.0.1:9000
2015-02-13 17:02:44,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 242 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@199831f
2015-02-13 17:02:44,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1336133248-127.0.0.1-1423827120740
2015-02-13 17:02:44,454 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-13 17:02:44,454 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-13 17:02:44,456 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-13 17:02:44,456 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-13 17:02:44,477 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1336133248-127.0.0.1-1423827120740
2015-02-13 17:02:44,484 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1336133248-127.0.0.1-1423827120740 to blockPoolScannerMap, new size=1
2015-02-13 17:04:04,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1336133248-127.0.0.1-1423827120740:blk_1073741825_1001 src: /127.0.0.1:41618 dest: /127.0.0.1:50010
2015-02-13 17:04:04,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41618, dest: /127.0.0.1:50010, bytes: 217011, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_129360724_1, offset: 0, srvID: 4af6c814-62f2-4c29-b3fe-dede61b82d0f, blockid: BP-1336133248-127.0.0.1-1423827120740:blk_1073741825_1001, duration: 236894720
2015-02-13 17:04:04,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1336133248-127.0.0.1-1423827120740:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-13 17:04:13,169 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1336133248-127.0.0.1-1423827120740:blk_1073741825_1001
2015-02-13 17:04:23,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1336133248-127.0.0.1-1423827120740:blk_1073741826_1002 src: /127.0.0.1:41621 dest: /127.0.0.1:50010
2015-02-13 17:04:24,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41621, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2034006074_1, offset: 0, srvID: 4af6c814-62f2-4c29-b3fe-dede61b82d0f, blockid: BP-1336133248-127.0.0.1-1423827120740:blk_1073741826_1002, duration: 101391734
2015-02-13 17:04:24,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1336133248-127.0.0.1-1423827120740:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-13 17:04:33,216 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1336133248-127.0.0.1-1423827120740:blk_1073741826_1002
2015-02-13 17:54:22,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 2 blocks total. Took 1 msec to generate and 4 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@7744b2
2015-02-13 17:54:22,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1336133248-127.0.0.1-1423827120740
2015-02-13 18:33:37,962 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-13 18:33:37,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-14 12:33:42,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-14 12:33:42,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-14 12:33:43,218 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-14 12:33:43,981 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-14 12:33:44,275 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-14 12:33:44,275 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-14 12:33:44,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-14 12:33:44,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-14 12:33:44,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-14 12:33:44,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-14 12:33:44,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-14 12:33:44,963 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-14 12:33:44,969 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-14 12:33:44,985 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-14 12:33:44,989 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-14 12:33:44,989 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-14 12:33:44,989 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-14 12:33:45,015 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-14 12:33:45,021 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-14 12:33:45,021 INFO org.mortbay.log: jetty-6.1.26
2015-02-14 12:33:46,006 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-14 12:33:46,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-14 12:33:46,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-14 12:33:46,581 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-14 12:33:46,619 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-14 12:33:46,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-14 12:33:46,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-14 12:33:46,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-14 12:33:47,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-14 12:33:47,115 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-14 12:33:47,173 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-14 12:33:48,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:49,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:50,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:51,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:52,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:53,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:54,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:55,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:56,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:57,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:33:57,504 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:34:03,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:04,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:05,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:06,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:07,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:08,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:09,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:10,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:11,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:12,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:12,515 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:34:18,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:19,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:20,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:21,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:22,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:23,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:24,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:25,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:26,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:27,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:27,525 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:34:33,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:34,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:35,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:36,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:37,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:38,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:39,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:40,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:41,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:42,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:42,867 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:34:48,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:49,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:50,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:51,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:52,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:53,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:54,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:55,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:56,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:57,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:34:57,878 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:35:03,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:04,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:05,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:06,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:07,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:08,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:09,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:10,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:11,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:12,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:12,889 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:35:18,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:19,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:20,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:21,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:22,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:23,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:24,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:25,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:26,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:27,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:27,901 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:35:33,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:34,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:35,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:36,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:37,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:38,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:39,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:40,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:41,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:42,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:42,912 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:35:48,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:49,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:50,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:51,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:52,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:53,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:54,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:55,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:56,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:57,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:35:57,922 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:36:03,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:04,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:05,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:06,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:07,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:08,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:09,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:10,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:11,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:12,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:12,933 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:36:18,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:19,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:20,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:21,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:22,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:23,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:24,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:25,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:26,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:27,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:27,943 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:36:33,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:34,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:35,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:36,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:37,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:38,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:39,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:40,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:41,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:42,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:42,954 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:36:48,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:49,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:50,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:51,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:52,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:53,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:54,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:55,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:56,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:57,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:36:57,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-14 12:37:03,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:04,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:05,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:06,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:07,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:08,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:09,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:10,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 12:37:11,793 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-14 12:37:11,861 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 6610@shrinivaasanka-Inspiron-1545
2015-02-14 12:37:11,863 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-29740077-127.0.0.1-1423897613709
2015-02-14 12:37:11,863 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-14 12:37:12,273 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-29740077-127.0.0.1-1423897613709
2015-02-14 12:37:12,273 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-14 12:37:12,274 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-29740077-127.0.0.1-1423897613709 is not formatted.
2015-02-14 12:37:12,274 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-14 12:37:12,274 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-29740077-127.0.0.1-1423897613709 directory /tmp/hadoop-root/dfs/data/current/BP-29740077-127.0.0.1-1423897613709/current
2015-02-14 12:37:12,308 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-14 12:37:12,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1675974167;bpid=BP-29740077-127.0.0.1-1423897613709;lv=-56;nsInfo=lv=-60;cid=CID-53c226b7-9056-4a81-92dc-7004b923a0f6;nsid=1675974167;c=0;bpid=BP-29740077-127.0.0.1-1423897613709;dnuuid=null
2015-02-14 12:37:12,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID b8347502-76d6-4295-953a-e4074a90fc27
2015-02-14 12:37:12,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-14 12:37:12,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-14 12:37:12,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-14 12:37:12,668 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1423902814668 with interval 21600000
2015-02-14 12:37:12,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-29740077-127.0.0.1-1423897613709
2015-02-14 12:37:12,706 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-29740077-127.0.0.1-1423897613709 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-14 12:37:12,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-29740077-127.0.0.1-1423897613709 on /tmp/hadoop-root/dfs/data/current: 129ms
2015-02-14 12:37:12,836 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-29740077-127.0.0.1-1423897613709: 163ms
2015-02-14 12:37:12,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-29740077-127.0.0.1-1423897613709 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-14 12:37:12,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-29740077-127.0.0.1-1423897613709 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-14 12:37:12,837 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-02-14 12:37:12,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-29740077-127.0.0.1-1423897613709 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-14 12:37:13,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-29740077-127.0.0.1-1423897613709 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-14 12:37:13,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-14 12:37:13,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-29740077-127.0.0.1-1423897613709 (Datanode Uuid b8347502-76d6-4295-953a-e4074a90fc27) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-14 12:37:13,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-29740077-127.0.0.1-1423897613709 (Datanode Uuid b8347502-76d6-4295-953a-e4074a90fc27) service to localhost/127.0.0.1:9000
2015-02-14 12:37:13,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 178 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4602f
2015-02-14 12:37:13,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-29740077-127.0.0.1-1423897613709
2015-02-14 12:37:13,719 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-14 12:37:13,719 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-14 12:37:13,728 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-14 12:37:13,728 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-14 12:37:13,730 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-29740077-127.0.0.1-1423897613709
2015-02-14 12:37:13,757 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-29740077-127.0.0.1-1423897613709 to blockPoolScannerMap, new size=1
2015-02-14 13:47:25,262 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "shrinivaasanka-Inspiron-1545/127.0.0.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-02-14 13:47:29,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-14 13:47:29,836 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-14 13:47:29,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-19 14:57:11,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-19 14:57:11,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-19 14:57:12,992 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-19 14:57:14,483 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-19 14:57:14,919 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-19 14:57:14,919 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-19 14:57:14,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-19 14:57:14,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-19 14:57:15,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-19 14:57:15,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-19 14:57:15,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-19 14:57:16,800 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-19 14:57:16,814 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-19 14:57:16,841 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-19 14:57:16,846 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-19 14:57:16,846 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-19 14:57:16,846 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-19 14:57:16,918 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-19 14:57:16,923 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-19 14:57:16,923 INFO org.mortbay.log: jetty-6.1.26
2015-02-19 14:57:18,348 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-19 14:57:18,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-19 14:57:18,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-19 14:57:20,029 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-19 14:57:20,114 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-19 14:57:20,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-19 14:57:20,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-19 14:57:20,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-19 14:57:20,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-19 14:57:20,772 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-19 14:57:20,785 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-19 14:57:22,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:23,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:24,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:25,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:26,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:27,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:28,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:29,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:30,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:31,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:31,453 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 14:57:37,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:38,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:39,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:40,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:41,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:42,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:43,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:44,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:45,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:46,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:46,463 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 14:57:52,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:53,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:54,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:55,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:56,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:57,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:58,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:57:59,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:00,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:01,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:01,472 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 14:58:07,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:08,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:09,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:10,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:11,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:12,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:13,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:14,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:15,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:16,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:16,482 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 14:58:22,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:23,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:24,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:25,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:26,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:27,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:28,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:29,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:30,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:31,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:31,491 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 14:58:37,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:38,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:39,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:40,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:41,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:42,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:43,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:44,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:45,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:46,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:46,500 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 14:58:52,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:53,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:54,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:55,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:56,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:57,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:58,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 14:58:58,895 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-19 14:58:58,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-19 15:38:51,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-19 15:38:51,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-19 15:38:52,789 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-19 15:38:53,417 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-19 15:38:53,533 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-19 15:38:53,533 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-19 15:38:53,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-19 15:38:53,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-19 15:38:53,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-19 15:38:53,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-19 15:38:53,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-19 15:38:54,020 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-19 15:38:54,026 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-19 15:38:54,042 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-19 15:38:54,045 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-19 15:38:54,045 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-19 15:38:54,046 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-19 15:38:54,071 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-19 15:38:54,077 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-19 15:38:54,077 INFO org.mortbay.log: jetty-6.1.26
2015-02-19 15:38:54,645 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-19 15:38:54,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-19 15:38:54,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-19 15:38:55,108 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-19 15:38:55,149 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-19 15:38:55,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-19 15:38:55,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-19 15:38:55,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-19 15:38:55,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-19 15:38:55,437 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-19 15:38:55,452 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-19 15:38:56,407 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-19 15:38:56,469 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 4819@shrinivaasanka-Inspiron-1545
2015-02-19 15:38:56,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-1949548580-127.0.0.1-1424340485816
2015-02-19 15:38:56,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-19 15:38:56,822 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1949548580-127.0.0.1-1424340485816
2015-02-19 15:38:56,822 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-19 15:38:56,823 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-1949548580-127.0.0.1-1424340485816 is not formatted.
2015-02-19 15:38:56,823 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-19 15:38:56,824 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1949548580-127.0.0.1-1424340485816 directory /tmp/hadoop-root/dfs/data/current/BP-1949548580-127.0.0.1-1424340485816/current
2015-02-19 15:38:56,863 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-19 15:38:56,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1283999979;bpid=BP-1949548580-127.0.0.1-1424340485816;lv=-56;nsInfo=lv=-60;cid=CID-9fe5e13e-0fdd-4493-9894-7050b78d5fad;nsid=1283999979;c=0;bpid=BP-1949548580-127.0.0.1-1424340485816;dnuuid=null
2015-02-19 15:38:56,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 87d5619a-a735-4e28-b13e-0d9935805108
2015-02-19 15:38:57,127 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-19 15:38:57,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-19 15:38:57,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-19 15:38:57,217 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424356257217 with interval 21600000
2015-02-19 15:38:57,222 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1949548580-127.0.0.1-1424340485816
2015-02-19 15:38:57,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1949548580-127.0.0.1-1424340485816 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-19 15:38:57,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1949548580-127.0.0.1-1424340485816 on /tmp/hadoop-root/dfs/data/current: 77ms
2015-02-19 15:38:57,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1949548580-127.0.0.1-1424340485816: 105ms
2015-02-19 15:38:57,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1949548580-127.0.0.1-1424340485816 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-19 15:38:57,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1949548580-127.0.0.1-1424340485816 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-19 15:38:57,338 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 11ms
2015-02-19 15:38:57,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1949548580-127.0.0.1-1424340485816 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-19 15:38:57,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1949548580-127.0.0.1-1424340485816 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-19 15:38:57,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-19 15:38:57,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1949548580-127.0.0.1-1424340485816 (Datanode Uuid 87d5619a-a735-4e28-b13e-0d9935805108) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-19 15:38:57,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1949548580-127.0.0.1-1424340485816 (Datanode Uuid 87d5619a-a735-4e28-b13e-0d9935805108) service to localhost/127.0.0.1:9000
2015-02-19 15:38:58,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 4 msec to generate and 241 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@12adaa8
2015-02-19 15:38:58,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1949548580-127.0.0.1-1424340485816
2015-02-19 15:38:58,220 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-19 15:38:58,220 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-19 15:38:58,234 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-19 15:38:58,235 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-19 15:38:58,236 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1949548580-127.0.0.1-1424340485816
2015-02-19 15:38:58,274 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1949548580-127.0.0.1-1424340485816 to blockPoolScannerMap, new size=1
2015-02-19 16:35:17,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1949548580-127.0.0.1-1424340485816:blk_1073741825_1001 src: /127.0.0.1:40068 dest: /127.0.0.1:50010
2015-02-19 16:35:18,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40068, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1409891907_1, offset: 0, srvID: 87d5619a-a735-4e28-b13e-0d9935805108, blockid: BP-1949548580-127.0.0.1-1424340485816:blk_1073741825_1001, duration: 160055758
2015-02-19 16:35:18,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1949548580-127.0.0.1-1424340485816:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 16:35:27,437 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1949548580-127.0.0.1-1424340485816:blk_1073741825_1001
2015-02-19 17:48:25,302 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: localhost:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:40764 dst: /127.0.0.1:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:212)
	at java.lang.Thread.run(Thread.java:745)
2015-02-19 17:48:25,306 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: localhost:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:40765 dst: /127.0.0.1:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:212)
	at java.lang.Thread.run(Thread.java:745)
2015-02-19 17:48:25,308 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: localhost:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:40766 dst: /127.0.0.1:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:212)
	at java.lang.Thread.run(Thread.java:745)
2015-02-19 17:48:28,070 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: localhost:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:40767 dst: /127.0.0.1:50010
java.io.IOException: Version Mismatch (Expected: 28, Received: 18245 )
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:60)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:212)
	at java.lang.Thread.run(Thread.java:745)
2015-02-19 17:48:43,320 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: localhost:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:40769 dst: /127.0.0.1:50010
java.io.EOFException
	at java.io.DataInputStream.readShort(DataInputStream.java:315)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:58)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:212)
	at java.lang.Thread.run(Thread.java:745)
2015-02-19 17:48:43,320 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: localhost:50010:DataXceiver error processing unknown operation  src: /127.0.0.1:40768 dst: /127.0.0.1:50010
java.io.EOFException
	at java.io.DataInputStream.readShort(DataInputStream.java:315)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.readOp(Receiver.java:58)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:212)
	at java.lang.Thread.run(Thread.java:745)
2015-02-19 18:23:50,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-19 18:23:50,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-19 19:56:38,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-19 19:56:38,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-19 19:56:39,386 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-19 19:56:39,906 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-19 19:56:40,059 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-19 19:56:40,059 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-19 19:56:40,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-19 19:56:40,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-19 19:56:40,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-19 19:56:40,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-19 19:56:40,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-19 19:56:40,697 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-19 19:56:40,703 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-19 19:56:40,719 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-19 19:56:40,723 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-19 19:56:40,723 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-19 19:56:40,723 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-19 19:56:40,751 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-19 19:56:40,756 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-19 19:56:40,756 INFO org.mortbay.log: jetty-6.1.26
2015-02-19 19:56:41,252 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-19 19:56:41,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-19 19:56:41,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-19 19:56:41,888 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-19 19:56:41,947 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-19 19:56:42,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-19 19:56:42,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-19 19:56:42,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-19 19:56:42,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-19 19:56:42,426 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-19 19:56:42,451 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-19 19:56:43,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:44,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:45,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:46,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:47,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:48,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:49,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:50,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:51,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:52,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:52,899 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 19:56:58,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:56:59,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:00,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:01,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:02,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:03,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:04,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:05,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:06,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:07,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:07,909 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 19:57:13,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:14,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:15,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:16,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:17,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:18,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:19,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:20,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:21,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:22,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:22,919 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 19:57:28,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:29,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:30,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:31,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:32,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:33,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:34,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:35,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:36,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:37,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-19 19:57:37,927 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-19 19:57:43,509 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-19 19:57:43,551 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 5453@shrinivaasanka-Inspiron-1545
2015-02-19 19:57:43,553 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-397608748-127.0.0.1-1424356043778
2015-02-19 19:57:43,553 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-19 19:57:43,886 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-397608748-127.0.0.1-1424356043778
2015-02-19 19:57:43,886 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-19 19:57:43,887 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778 is not formatted.
2015-02-19 19:57:43,887 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-19 19:57:43,887 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-397608748-127.0.0.1-1424356043778 directory /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current
2015-02-19 19:57:43,931 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-19 19:57:43,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=282378801;bpid=BP-397608748-127.0.0.1-1424356043778;lv=-56;nsInfo=lv=-60;cid=CID-e73b157c-293a-4286-a743-343c70262e76;nsid=282378801;c=0;bpid=BP-397608748-127.0.0.1-1424356043778;dnuuid=null
2015-02-19 19:57:44,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 9d80264b-c676-446c-9e91-7e9df4f8a099
2015-02-19 19:57:44,237 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-19 19:57:44,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-19 19:57:44,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-19 19:57:44,300 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424376978300 with interval 21600000
2015-02-19 19:57:44,309 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-397608748-127.0.0.1-1424356043778
2015-02-19 19:57:44,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-397608748-127.0.0.1-1424356043778 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-19 19:57:44,392 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-397608748-127.0.0.1-1424356043778 on /tmp/hadoop-root/dfs/data/current: 76ms
2015-02-19 19:57:44,392 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-397608748-127.0.0.1-1424356043778: 83ms
2015-02-19 19:57:44,393 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-397608748-127.0.0.1-1424356043778 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-19 19:57:44,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-397608748-127.0.0.1-1424356043778 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-19 19:57:44,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-02-19 19:57:44,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-397608748-127.0.0.1-1424356043778 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-19 19:57:44,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-397608748-127.0.0.1-1424356043778 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-19 19:57:44,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-19 19:57:44,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-397608748-127.0.0.1-1424356043778 (Datanode Uuid 9d80264b-c676-446c-9e91-7e9df4f8a099) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-19 19:57:44,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-397608748-127.0.0.1-1424356043778 (Datanode Uuid 9d80264b-c676-446c-9e91-7e9df4f8a099) service to localhost/127.0.0.1:9000
2015-02-19 19:57:45,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 3 msec to generate and 105 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@92bdf6
2015-02-19 19:57:45,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-397608748-127.0.0.1-1424356043778
2015-02-19 19:57:45,111 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-19 19:57:45,111 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-19 19:57:45,113 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-19 19:57:45,113 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-19 19:57:45,115 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-397608748-127.0.0.1-1424356043778
2015-02-19 19:57:45,121 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-397608748-127.0.0.1-1424356043778 to blockPoolScannerMap, new size=1
2015-02-19 20:00:22,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741825_1001 src: /127.0.0.1:49618 dest: /127.0.0.1:50010
2015-02-19 20:00:23,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49618, dest: /127.0.0.1:50010, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741825_1001, duration: 164476991
2015-02-19 20:00:23,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:23,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741826_1002 src: /127.0.0.1:49619 dest: /127.0.0.1:50010
2015-02-19 20:00:23,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49619, dest: /127.0.0.1:50010, bytes: 42, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741826_1002, duration: 1867486
2015-02-19 20:00:23,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:24,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741827_1003 src: /127.0.0.1:49621 dest: /127.0.0.1:50010
2015-02-19 20:00:24,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49621, dest: /127.0.0.1:50010, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741827_1003, duration: 27396986
2015-02-19 20:00:24,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:24,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741828_1004 src: /127.0.0.1:49622 dest: /127.0.0.1:50010
2015-02-19 20:00:24,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49622, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741828_1004, duration: 410502691
2015-02-19 20:00:24,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:24,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741829_1005 src: /127.0.0.1:49623 dest: /127.0.0.1:50010
2015-02-19 20:00:24,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49623, dest: /127.0.0.1:50010, bytes: 372, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741829_1005, duration: 7575473
2015-02-19 20:00:24,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:28,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741830_1006 src: /127.0.0.1:49628 dest: /127.0.0.1:50010
2015-02-19 20:00:29,348 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741827_1003
2015-02-19 20:00:29,364 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741826_1002
2015-02-19 20:00:29,365 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741825_1001
2015-02-19 20:00:31,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741831_1007 src: /127.0.0.1:49632 dest: /127.0.0.1:50010
2015-02-19 20:00:32,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741832_1008 src: /127.0.0.1:49634 dest: /127.0.0.1:50010
2015-02-19 20:00:32,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49634, dest: /127.0.0.1:50010, bytes: 286, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741832_1008, duration: 4423467
2015-02-19 20:00:32,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:32,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741833_1009 src: /127.0.0.1:49635 dest: /127.0.0.1:50010
2015-02-19 20:00:32,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49635, dest: /127.0.0.1:50010, bytes: 40, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1318535235_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741833_1009, duration: 2956869
2015-02-19 20:00:32,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:33,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741834_1010 src: /127.0.0.1:49636 dest: /127.0.0.1:50010
2015-02-19 20:00:33,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49632, dest: /127.0.0.1:50010, bytes: 268, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741831_1007, duration: 1800856456
2015-02-19 20:00:33,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:34,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741835_1011 src: /127.0.0.1:49640 dest: /127.0.0.1:50010
2015-02-19 20:00:34,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49636, dest: /127.0.0.1:50010, bytes: 471, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741834_1010, duration: 858995898
2015-02-19 20:00:34,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:34,487 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741828_1004
2015-02-19 20:00:34,487 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741829_1005
2015-02-19 20:00:34,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741836_1012 src: /127.0.0.1:49641 dest: /127.0.0.1:50010
2015-02-19 20:00:34,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49628, dest: /127.0.0.1:50010, bytes: 303, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741830_1006, duration: 5820947761
2015-02-19 20:00:34,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:00:39,515 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741832_1008
2015-02-19 20:00:39,516 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741834_1010
2015-02-19 20:00:39,516 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741833_1009
2015-02-19 20:00:39,521 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741831_1007
2015-02-19 20:00:44,538 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741830_1006
2015-02-19 20:01:14,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741837_1013 src: /127.0.0.1:49649 dest: /127.0.0.1:50010
2015-02-19 20:01:14,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49649, dest: /127.0.0.1:50010, bytes: 1045, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741837_1013, duration: 3657102
2015-02-19 20:01:14,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:01:20,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741838_1014 src: /127.0.0.1:49651 dest: /127.0.0.1:50010
2015-02-19 20:01:20,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49651, dest: /127.0.0.1:50010, bytes: 1336, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741838_1014, duration: 2750351
2015-02-19 20:01:20,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:01:20,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49640, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741835_1011, duration: 46792643461
2015-02-19 20:01:20,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:01:20,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49641, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356122840_534174741_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741836_1012, duration: 46375270348
2015-02-19 20:01:20,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:01:24,555 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741837_1013
2015-02-19 20:01:29,572 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741838_1014
2015-02-19 20:01:29,573 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741835_1011
2015-02-19 20:01:29,573 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741836_1012
2015-02-19 20:02:02,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741839_1015 src: /127.0.0.1:49667 dest: /127.0.0.1:50010
2015-02-19 20:02:05,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741840_1016 src: /127.0.0.1:49671 dest: /127.0.0.1:50010
2015-02-19 20:02:07,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741841_1017 src: /127.0.0.1:49676 dest: /127.0.0.1:50010
2015-02-19 20:02:07,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49671, dest: /127.0.0.1:50010, bytes: 471, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741840_1016, duration: 1919296441
2015-02-19 20:02:07,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:02:14,593 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741840_1016
2015-02-19 20:06:56,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741842_1018 src: /127.0.0.1:49762 dest: /127.0.0.1:50010
2015-02-19 20:06:56,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49762, dest: /127.0.0.1:50010, bytes: 289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1768642217_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741842_1018, duration: 9821574
2015-02-19 20:06:56,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:06:57,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741843_1019 src: /127.0.0.1:49763 dest: /127.0.0.1:50010
2015-02-19 20:06:57,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49763, dest: /127.0.0.1:50010, bytes: 44, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1768642217_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741843_1019, duration: 2359165
2015-02-19 20:06:57,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:06:57,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741844_1020 src: /127.0.0.1:49765 dest: /127.0.0.1:50010
2015-02-19 20:06:57,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49676, dest: /127.0.0.1:50010, bytes: 268, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741841_1017, duration: 289939704980
2015-02-19 20:06:57,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:06:57,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741845_1021 src: /127.0.0.1:49770 dest: /127.0.0.1:50010
2015-02-19 20:06:57,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49765, dest: /127.0.0.1:50010, bytes: 459, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741844_1020, duration: 239635702
2015-02-19 20:06:57,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:07:04,619 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741843_1019
2015-02-19 20:07:04,619 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741841_1017
2015-02-19 20:07:04,620 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741844_1020
2015-02-19 20:07:04,620 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741842_1018
2015-02-19 20:08:47,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741846_1022 src: /127.0.0.1:49780 dest: /127.0.0.1:50010
2015-02-19 20:08:47,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49667, dest: /127.0.0.1:50010, bytes: 196, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741839_1015, duration: 405714120362
2015-02-19 20:08:47,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:08:54,640 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741839_1015
2015-02-19 20:08:56,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741847_1023 src: /127.0.0.1:49783 dest: /127.0.0.1:50010
2015-02-19 20:08:56,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49780, dest: /127.0.0.1:50010, bytes: 194, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741846_1022, duration: 8670134942
2015-02-19 20:08:56,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:09:04,656 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741846_1022
2015-02-19 20:09:09,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741848_1024 src: /127.0.0.1:49786 dest: /127.0.0.1:50010
2015-02-19 20:09:09,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49783, dest: /127.0.0.1:50010, bytes: 198, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741847_1023, duration: 12494985079
2015-02-19 20:09:09,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:09:14,676 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741847_1023
2015-02-19 20:12:05,699 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-02-19 20:12:05,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-02-19 20:12:05,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-02-19 20:12:05,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2015-02-19 20:12:05,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741830_1006 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741830
2015-02-19 20:12:05,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-02-19 20:12:05,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741831_1007 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741831
2015-02-19 20:12:05,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741834_1010 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741834
2015-02-19 20:12:05,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741835_1011 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741835
2015-02-19 20:12:05,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741836
2015-02-19 20:46:33,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741849_1025 src: /127.0.0.1:50364 dest: /127.0.0.1:50010
2015-02-19 20:46:34,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50364, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-343248970_1, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741849_1025, duration: 140795904
2015-02-19 20:46:34,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 20:46:39,834 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741849_1025
2015-02-19 21:02:27,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741850_1026 src: /127.0.0.1:50520 dest: /127.0.0.1:50010
2015-02-19 21:02:27,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50520, dest: /127.0.0.1:50010, bytes: 1661, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741850_1026, duration: 3847910
2015-02-19 21:02:27,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 21:02:34,897 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741850_1026
2015-02-19 21:06:57,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741851_1027 src: /127.0.0.1:50546 dest: /127.0.0.1:50010
2015-02-19 21:06:57,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49770, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741845_1021, duration: 3600171521967
2015-02-19 21:06:57,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 21:07:04,926 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741845_1021
2015-02-19 21:09:07,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741852_1028 src: /127.0.0.1:50561 dest: /127.0.0.1:50010
2015-02-19 21:09:07,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50561, dest: /127.0.0.1:50010, bytes: 1100, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741852_1028, duration: 2286046
2015-02-19 21:09:07,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 21:09:09,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-397608748-127.0.0.1-1424356043778:blk_1073741853_1029 src: /127.0.0.1:50563 dest: /127.0.0.1:50010
2015-02-19 21:09:09,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49786, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424356314620_-91632144_32, offset: 0, srvID: 9d80264b-c676-446c-9e91-7e9df4f8a099, blockid: BP-397608748-127.0.0.1-1424356043778:blk_1073741848_1024, duration: 3600606739471
2015-02-19 21:09:09,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-397608748-127.0.0.1-1424356043778:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-19 21:09:14,943 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741848_1024
2015-02-19 21:09:14,944 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-397608748-127.0.0.1-1424356043778:blk_1073741852_1028
2015-02-19 21:17:02,895 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-02-19 21:17:02,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-02-19 21:17:02,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-02-19 21:17:02,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-02-19 21:17:02,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741840_1016 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741840
2015-02-19 21:17:02,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741841_1017 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741841
2015-02-19 21:17:02,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741844_1020 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741844
2015-02-19 21:17:02,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741845_1021 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741845
2015-02-19 21:20:02,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-02-19 21:20:02,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-02-19 21:20:02,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-02-19 21:20:02,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-02-19 21:20:02,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741846_1022 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741846
2015-02-19 21:20:02,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741847_1023 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741847
2015-02-19 21:20:02,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741848_1024 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741848
2015-02-19 21:20:02,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-397608748-127.0.0.1-1424356043778 blk_1073741839_1015 file /tmp/hadoop-root/dfs/data/current/BP-397608748-127.0.0.1-1424356043778/current/finalized/subdir0/subdir0/blk_1073741839
2015-02-19 21:44:13,410 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-19 21:44:13,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-20 09:31:09,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-20 09:31:09,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-20 09:31:10,576 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-20 09:31:11,476 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-20 09:31:11,801 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-20 09:31:11,802 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-20 09:31:11,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-20 09:31:11,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-20 09:31:11,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-20 09:31:12,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-20 09:31:12,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-20 09:31:13,487 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-20 09:31:13,502 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-20 09:31:13,536 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-20 09:31:13,545 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-20 09:31:13,545 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-20 09:31:13,546 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-20 09:31:13,623 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-20 09:31:13,637 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-20 09:31:13,637 INFO org.mortbay.log: jetty-6.1.26
2015-02-20 09:31:15,050 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-20 09:31:15,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-20 09:31:15,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-20 09:31:16,614 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-20 09:31:16,673 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-20 09:31:17,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-20 09:31:17,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-20 09:31:17,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-20 09:31:17,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-20 09:31:17,169 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-20 09:31:17,175 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-20 09:31:18,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:19,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:20,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:21,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:22,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:23,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:24,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:25,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:26,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:27,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:27,848 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:31:33,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:34,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:35,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:36,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:37,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:38,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:39,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:40,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:41,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:42,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:42,857 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:31:48,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:49,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:50,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:51,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:52,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:53,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:54,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:55,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:56,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:57,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:31:57,865 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:32:03,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:04,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:05,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:06,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:07,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:08,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:09,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:10,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:11,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:12,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:12,874 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:32:18,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:19,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:20,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:21,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:22,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:23,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:24,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:25,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:26,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:27,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:27,885 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:32:33,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:34,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:35,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:36,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:37,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:38,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:39,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:40,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:41,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:42,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:42,896 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:32:48,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:49,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:50,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:51,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:52,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:53,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:54,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:55,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:56,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:57,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:32:57,906 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 09:33:03,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:33:04,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:33:05,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:33:06,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:33:07,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:33:08,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 09:33:09,686 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-20 09:33:09,733 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 4261@shrinivaasanka-Inspiron-1545
2015-02-20 09:33:09,735 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:33:09,735 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-20 09:33:10,104 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:33:10,104 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-20 09:33:10,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609 is not formatted.
2015-02-20 09:33:10,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-20 09:33:10,105 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1205955254-127.0.0.1-1424404974609 directory /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current
2015-02-20 09:33:10,142 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-20 09:33:10,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=575047396;bpid=BP-1205955254-127.0.0.1-1424404974609;lv=-56;nsInfo=lv=-60;cid=CID-02698a20-b025-4732-be7a-2bceb022a997;nsid=575047396;c=0;bpid=BP-1205955254-127.0.0.1-1424404974609;dnuuid=null
2015-02-20 09:33:10,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847
2015-02-20 09:33:10,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-20 09:33:10,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-20 09:33:10,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-20 09:33:10,509 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424413910509 with interval 21600000
2015-02-20 09:33:10,534 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:33:10,536 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1205955254-127.0.0.1-1424404974609 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-20 09:33:10,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1205955254-127.0.0.1-1424404974609 on /tmp/hadoop-root/dfs/data/current: 91ms
2015-02-20 09:33:10,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1205955254-127.0.0.1-1424404974609: 99ms
2015-02-20 09:33:10,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1205955254-127.0.0.1-1424404974609 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-20 09:33:10,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1205955254-127.0.0.1-1424404974609 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-20 09:33:10,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-02-20 09:33:10,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-20 09:33:10,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-20 09:33:10,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-20 09:33:11,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-20 09:33:11,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847) service to localhost/127.0.0.1:9000
2015-02-20 09:33:11,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 18 msec to generate and 231 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@f965eb
2015-02-20 09:33:11,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:33:11,557 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-20 09:33:11,557 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-20 09:33:11,563 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-20 09:33:11,563 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-20 09:33:11,564 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:33:11,571 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1205955254-127.0.0.1-1424404974609 to blockPoolScannerMap, new size=1
2015-02-20 09:33:34,792 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "shrinivaasanka-Inspiron-1545/127.0.0.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-02-20 09:33:38,442 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-20 09:33:38,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-20 09:34:07,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-20 09:34:07,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-20 09:34:08,496 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-20 09:34:08,968 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-20 09:34:09,177 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-20 09:34:09,177 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-20 09:34:09,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-20 09:34:09,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-20 09:34:09,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-20 09:34:09,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-20 09:34:09,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-20 09:34:09,716 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-20 09:34:09,722 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-20 09:34:09,738 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-20 09:34:09,742 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-20 09:34:09,742 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-20 09:34:09,742 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-20 09:34:09,768 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-20 09:34:09,773 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-20 09:34:09,773 INFO org.mortbay.log: jetty-6.1.26
2015-02-20 09:34:10,218 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-20 09:34:10,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-20 09:34:10,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-20 09:34:11,214 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-20 09:34:11,285 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-20 09:34:11,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-20 09:34:11,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-20 09:34:11,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-20 09:34:11,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-20 09:34:11,507 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-20 09:34:11,509 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-20 09:34:12,377 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-20 09:34:12,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 6275@shrinivaasanka-Inspiron-1545
2015-02-20 09:34:12,801 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:34:12,801 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-20 09:34:12,803 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-20 09:34:12,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=575047396;bpid=BP-1205955254-127.0.0.1-1424404974609;lv=-56;nsInfo=lv=-60;cid=CID-02698a20-b025-4732-be7a-2bceb022a997;nsid=575047396;c=0;bpid=BP-1205955254-127.0.0.1-1424404974609;dnuuid=b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847
2015-02-20 09:34:13,033 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-20 09:34:13,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-20 09:34:13,090 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-20 09:34:13,107 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424422596107 with interval 21600000
2015-02-20 09:34:13,118 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:34:13,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1205955254-127.0.0.1-1424404974609 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-20 09:34:13,200 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current: 24576
2015-02-20 09:34:13,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1205955254-127.0.0.1-1424404974609 on /tmp/hadoop-root/dfs/data/current: 89ms
2015-02-20 09:34:13,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1205955254-127.0.0.1-1424404974609: 97ms
2015-02-20 09:34:13,222 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1205955254-127.0.0.1-1424404974609 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-20 09:34:13,222 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1205955254-127.0.0.1-1424404974609 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-20 09:34:13,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2015-02-20 09:34:13,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-20 09:34:13,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-20 09:34:13,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-20 09:34:13,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=2
2015-02-20 09:34:13,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1205955254-127.0.0.1-1424404974609 (Datanode Uuid b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847) service to localhost/127.0.0.1:9000
2015-02-20 09:34:14,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 10 msec to generate and 218 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@9025a4
2015-02-20 09:34:14,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:34:14,119 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-20 09:34:14,119 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-20 09:34:14,121 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-20 09:34:14,121 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-20 09:34:14,131 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 09:34:14,155 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1205955254-127.0.0.1-1424404974609 to blockPoolScannerMap, new size=1
2015-02-20 09:39:32,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741825_1001 src: /127.0.0.1:44434 dest: /127.0.0.1:50010
2015-02-20 09:39:32,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44434, dest: /127.0.0.1:50010, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1272722532_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741825_1001, duration: 65267552
2015-02-20 09:39:32,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:39:33,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741826_1002 src: /127.0.0.1:44435 dest: /127.0.0.1:50010
2015-02-20 09:39:33,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44435, dest: /127.0.0.1:50010, bytes: 42, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1272722532_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741826_1002, duration: 6054519
2015-02-20 09:39:33,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:39:33,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741827_1003 src: /127.0.0.1:44437 dest: /127.0.0.1:50010
2015-02-20 09:39:33,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44437, dest: /127.0.0.1:50010, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1272722532_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741827_1003, duration: 21460709
2015-02-20 09:39:33,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:39:34,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741828_1004 src: /127.0.0.1:44438 dest: /127.0.0.1:50010
2015-02-20 09:39:34,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44438, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1272722532_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741828_1004, duration: 402345312
2015-02-20 09:39:34,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:39:34,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741829_1005 src: /127.0.0.1:44439 dest: /127.0.0.1:50010
2015-02-20 09:39:34,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44439, dest: /127.0.0.1:50010, bytes: 372, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1272722532_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741829_1005, duration: 2779760
2015-02-20 09:39:34,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:39:38,148 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741825_1001
2015-02-20 09:39:38,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741830_1006 src: /127.0.0.1:44443 dest: /127.0.0.1:50010
2015-02-20 09:39:40,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741831_1007 src: /127.0.0.1:44447 dest: /127.0.0.1:50010
2015-02-20 09:39:43,233 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741826_1002
2015-02-20 09:44:39,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44447, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405365456_174618294_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741831_1007, duration: 299353464875
2015-02-20 09:44:39,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:44:39,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44443, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405365456_174618294_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741830_1006, duration: 301413212057
2015-02-20 09:44:39,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:44:48,275 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741831_1007
2015-02-20 09:46:48,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741832_1008 src: /127.0.0.1:44689 dest: /127.0.0.1:50010
2015-02-20 09:46:51,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741833_1009 src: /127.0.0.1:44693 dest: /127.0.0.1:50010
2015-02-20 09:46:53,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741834_1010 src: /127.0.0.1:44696 dest: /127.0.0.1:50010
2015-02-20 09:46:53,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44696, dest: /127.0.0.1:50010, bytes: 286, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-525212036_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741834_1010, duration: 105015240
2015-02-20 09:46:53,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:46:53,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741835_1011 src: /127.0.0.1:44697 dest: /127.0.0.1:50010
2015-02-20 09:46:53,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44697, dest: /127.0.0.1:50010, bytes: 40, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-525212036_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741835_1011, duration: 29213839
2015-02-20 09:46:53,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:46:54,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741836_1012 src: /127.0.0.1:44698 dest: /127.0.0.1:50010
2015-02-20 09:46:54,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44693, dest: /127.0.0.1:50010, bytes: 268, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741833_1009, duration: 3614949914
2015-02-20 09:46:54,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:46:55,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741837_1013 src: /127.0.0.1:44702 dest: /127.0.0.1:50010
2015-02-20 09:46:55,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44698, dest: /127.0.0.1:50010, bytes: 471, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741836_1012, duration: 543575899
2015-02-20 09:46:55,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:46:55,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741838_1014 src: /127.0.0.1:44703 dest: /127.0.0.1:50010
2015-02-20 09:46:55,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44689, dest: /127.0.0.1:50010, bytes: 303, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741832_1008, duration: 6825289655
2015-02-20 09:46:55,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:47:03,301 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741833_1009
2015-02-20 09:47:03,302 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741836_1012
2015-02-20 09:47:12,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741839_1015 src: /127.0.0.1:44708 dest: /127.0.0.1:50010
2015-02-20 09:47:12,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44708, dest: /127.0.0.1:50010, bytes: 289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-525212036_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741839_1015, duration: 20613373
2015-02-20 09:47:12,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:47:12,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741840_1016 src: /127.0.0.1:44709 dest: /127.0.0.1:50010
2015-02-20 09:47:12,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44709, dest: /127.0.0.1:50010, bytes: 44, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-525212036_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741840_1016, duration: 9341272
2015-02-20 09:47:12,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:47:12,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741841_1017 src: /127.0.0.1:44713 dest: /127.0.0.1:50010
2015-02-20 09:47:12,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44702, dest: /127.0.0.1:50010, bytes: 268, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741837_1013, duration: 17414137371
2015-02-20 09:47:12,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:47:18,332 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741839_1015
2015-02-20 09:47:18,333 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741840_1016
2015-02-20 09:47:54,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741842_1018 src: /127.0.0.1:44722 dest: /127.0.0.1:50010
2015-02-20 09:47:54,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44703, dest: /127.0.0.1:50010, bytes: 193, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741838_1014, duration: 58980675820
2015-02-20 09:47:54,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:47:59,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741843_1019 src: /127.0.0.1:44723 dest: /127.0.0.1:50010
2015-02-20 09:47:59,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44722, dest: /127.0.0.1:50010, bytes: 193, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741842_1018, duration: 5224289350
2015-02-20 09:47:59,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:48:03,342 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741838_1014
2015-02-20 09:48:04,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741844_1020 src: /127.0.0.1:44724 dest: /127.0.0.1:50010
2015-02-20 09:48:04,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44723, dest: /127.0.0.1:50010, bytes: 193, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741843_1019, duration: 4991175489
2015-02-20 09:48:04,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:48:08,348 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741842_1018
2015-02-20 09:48:13,353 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741843_1019
2015-02-20 09:55:40,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741845_1021 src: /127.0.0.1:44887 dest: /127.0.0.1:50010
2015-02-20 09:55:40,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44887, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1266213918_1, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741845_1021, duration: 145159943
2015-02-20 09:55:40,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 09:55:48,408 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741845_1021
2015-02-20 09:57:46,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-02-20 09:57:46,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741830_1006 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741830
2015-02-20 10:45:07,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741846_1022 src: /127.0.0.1:46099 dest: /127.0.0.1:50010
2015-02-20 10:45:07,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44724, dest: /127.0.0.1:50010, bytes: 197, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741844_1020, duration: 3422750928339
2015-02-20 10:45:07,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 10:45:13,551 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741844_1020
2015-02-20 10:47:02,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741847_1023 src: /127.0.0.1:46123 dest: /127.0.0.1:50010
2015-02-20 10:47:02,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46123, dest: /127.0.0.1:50010, bytes: 1793, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741847_1023, duration: 2648246
2015-02-20 10:47:02,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 10:47:08,571 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741847_1023
2015-02-20 10:47:13,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741848_1024 src: /127.0.0.1:46127 dest: /127.0.0.1:50010
2015-02-20 10:47:13,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44713, dest: /127.0.0.1:50010, bytes: 459, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741841_1017, duration: 3600592899717
2015-02-20 10:47:13,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 10:47:14,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741849_1025 src: /127.0.0.1:46128 dest: /127.0.0.1:50010
2015-02-20 10:47:14,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46128, dest: /127.0.0.1:50010, bytes: 1045, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741849_1025, duration: 3785606
2015-02-20 10:47:14,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 10:47:18,580 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741841_1017
2015-02-20 10:47:23,585 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741849_1025
2015-02-20 10:48:08,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741850_1026 src: /127.0.0.1:46141 dest: /127.0.0.1:50010
2015-02-20 10:48:08,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46141, dest: /127.0.0.1:50010, bytes: 1885, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741850_1026, duration: 2450312
2015-02-20 10:48:08,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 10:48:13,596 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741850_1026
2015-02-20 10:57:49,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-02-20 10:57:49,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-02-20 10:57:49,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-02-20 10:57:49,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741841_1017 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741841
2015-02-20 10:57:49,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-02-20 10:57:49,691 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741833_1009 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741833
2015-02-20 10:57:49,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741836
2015-02-20 10:57:49,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741837_1013 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741837
2015-02-20 11:45:07,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741851_1027 src: /127.0.0.1:47229 dest: /127.0.0.1:50010
2015-02-20 11:45:07,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46099, dest: /127.0.0.1:50010, bytes: 1809, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741846_1022, duration: 3600250146847
2015-02-20 11:45:07,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 11:45:13,761 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741846_1022
2015-02-20 11:47:13,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741852_1028 src: /127.0.0.1:47247 dest: /127.0.0.1:50010
2015-02-20 11:47:13,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46127, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741848_1024, duration: 3600263943922
2015-02-20 11:47:13,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 11:47:18,784 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741848_1024
2015-02-20 11:55:49,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-02-20 11:55:49,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-02-20 11:55:49,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-02-20 11:55:49,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-02-20 11:55:49,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-02-20 11:55:49,869 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-02-20 11:55:49,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741842_1018 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741842
2015-02-20 11:55:49,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741843_1019 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741843
2015-02-20 11:55:49,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741844_1020 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741844
2015-02-20 11:55:49,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741846_1022 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741846
2015-02-20 11:55:49,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741832_1008 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741832
2015-02-20 11:55:49,893 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741838_1014 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741838
2015-02-20 11:57:49,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-02-20 11:57:49,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741848_1024 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741848
2015-02-20 12:45:07,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741853_1029 src: /127.0.0.1:48256 dest: /127.0.0.1:50010
2015-02-20 12:45:07,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47229, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741851_1027, duration: 3600295416987
2015-02-20 12:45:07,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 12:45:13,944 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741851_1027
2015-02-20 12:47:13,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741854_1030 src: /127.0.0.1:48279 dest: /127.0.0.1:50010
2015-02-20 12:47:13,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47247, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741852_1028, duration: 3600254030650
2015-02-20 12:47:13,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 12:47:18,957 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741852_1028
2015-02-20 12:55:50,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2015-02-20 12:55:50,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741851_1027 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741851
2015-02-20 12:57:50,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-02-20 12:57:50,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741852_1028 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741852
2015-02-20 13:34:29,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 16 blocks total. Took 101 msec to generate and 16 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@1de0f57
2015-02-20 13:34:29,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1205955254-127.0.0.1-1424404974609
2015-02-20 13:45:08,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741855_1031 src: /127.0.0.1:48736 dest: /127.0.0.1:50010
2015-02-20 13:45:08,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48256, dest: /127.0.0.1:50010, bytes: 11272609, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741853_1029, duration: 3600159122774
2015-02-20 13:45:08,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 13:45:24,329 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741853_1029
2015-02-20 13:45:58,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741856_1032 src: /127.0.0.1:48741 dest: /127.0.0.1:50010
2015-02-20 13:45:59,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48741, dest: /127.0.0.1:50010, bytes: 5027427, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741856_1032, duration: 439728264
2015-02-20 13:45:59,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 13:46:08,940 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741856_1032
2015-02-20 13:47:14,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741857_1033 src: /127.0.0.1:48754 dest: /127.0.0.1:50010
2015-02-20 13:47:14,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48279, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741854_1030, duration: 3600586915299
2015-02-20 13:47:14,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 13:47:23,954 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741854_1030
2015-02-20 13:57:47,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-02-20 13:57:47,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741854_1030 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741854
2015-02-20 14:26:36,280 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1205955254-127.0.0.1-1424404974609 Total blocks: 16, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-02-20 14:45:08,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741858_1034 src: /127.0.0.1:49834 dest: /127.0.0.1:50010
2015-02-20 14:45:08,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48736, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741855_1031, duration: 3600160417002
2015-02-20 14:45:08,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 14:45:14,113 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741855_1031
2015-02-20 14:47:14,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741859_1035 src: /127.0.0.1:49851 dest: /127.0.0.1:50010
2015-02-20 14:47:14,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48754, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741857_1033, duration: 3600199015625
2015-02-20 14:47:14,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 14:47:24,132 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741857_1033
2015-02-20 14:55:47,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-02-20 14:55:47,451 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-02-20 14:55:47,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741853_1029 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741853
2015-02-20 14:55:47,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741855_1031 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741855
2015-02-20 14:57:47,457 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-02-20 14:57:47,457 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741857_1033 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741857
2015-02-20 15:45:08,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741860_1036 src: /127.0.0.1:50897 dest: /127.0.0.1:50010
2015-02-20 15:45:09,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49834, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741858_1034, duration: 3600825907217
2015-02-20 15:45:09,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 15:45:14,308 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741858_1034
2015-02-20 15:47:14,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741861_1037 src: /127.0.0.1:50931 dest: /127.0.0.1:50010
2015-02-20 15:47:14,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49851, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741859_1035, duration: 3600206519443
2015-02-20 15:47:14,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 15:47:24,329 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741859_1035
2015-02-20 15:55:47,688 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-02-20 15:55:47,689 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741858_1034 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741858
2015-02-20 15:57:47,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-02-20 15:57:47,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741859_1035 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741859
2015-02-20 16:45:09,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741862_1038 src: /127.0.0.1:51461 dest: /127.0.0.1:50010
2015-02-20 16:45:09,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50897, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741860_1036, duration: 3600680103474
2015-02-20 16:45:09,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 16:45:19,519 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741860_1036
2015-02-20 16:47:14,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1205955254-127.0.0.1-1424404974609:blk_1073741863_1039 src: /127.0.0.1:51478 dest: /127.0.0.1:50010
2015-02-20 16:47:14,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:50931, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424405798441_-1396193167_32, offset: 0, srvID: b4b1d1bf-2e9a-4b97-b50d-9f9c93e49847, blockid: BP-1205955254-127.0.0.1-1424404974609:blk_1073741861_1037, duration: 3600276243251
2015-02-20 16:47:14,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1205955254-127.0.0.1-1424404974609:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 16:47:24,553 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1205955254-127.0.0.1-1424404974609:blk_1073741861_1037
2015-02-20 16:55:47,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2015-02-20 16:55:47,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741860_1036 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741860
2015-02-20 16:57:47,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2015-02-20 16:57:47,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1205955254-127.0.0.1-1424404974609 blk_1073741861_1037 file /tmp/hadoop-root/dfs/data/current/BP-1205955254-127.0.0.1-1424404974609/current/finalized/subdir0/subdir0/blk_1073741861
2015-02-20 17:12:51,738 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-20 17:12:53,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-20 18:24:39,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-20 18:24:39,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-20 18:24:40,434 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-20 18:24:41,050 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-20 18:24:41,264 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-20 18:24:41,264 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-20 18:24:41,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-20 18:24:41,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-20 18:24:41,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-20 18:24:41,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-20 18:24:41,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-20 18:24:41,781 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-20 18:24:41,787 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-20 18:24:41,806 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-20 18:24:41,810 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-20 18:24:41,810 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-20 18:24:41,810 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-20 18:24:41,836 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-20 18:24:41,842 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-20 18:24:41,842 INFO org.mortbay.log: jetty-6.1.26
2015-02-20 18:24:42,372 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-20 18:24:42,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-20 18:24:42,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-20 18:24:43,009 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-20 18:24:43,042 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-20 18:24:43,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-20 18:24:43,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-20 18:24:43,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-20 18:24:43,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-20 18:24:43,654 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-20 18:24:43,656 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-20 18:24:45,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:46,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:47,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:48,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:49,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:50,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:51,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:52,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:53,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:54,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:24:54,020 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:25:00,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:01,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:02,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:03,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:04,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:05,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:06,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:07,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:08,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:09,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:09,029 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:25:15,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:16,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:17,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:18,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:19,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:20,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:21,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:22,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:23,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:24,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:24,038 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:25:30,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:31,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:32,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:33,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:34,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:35,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:36,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:37,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:38,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:39,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:25:39,048 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:25:40,151 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-20 18:25:40,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-20 18:26:31,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-20 18:26:31,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-20 18:26:32,399 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-20 18:26:32,878 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-20 18:26:33,042 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-20 18:26:33,042 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-20 18:26:33,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-20 18:26:33,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-20 18:26:33,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-20 18:26:33,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-20 18:26:33,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-20 18:26:33,600 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-20 18:26:33,606 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-20 18:26:33,622 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-20 18:26:33,626 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-20 18:26:33,626 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-20 18:26:33,626 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-20 18:26:33,651 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-20 18:26:33,655 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-20 18:26:33,656 INFO org.mortbay.log: jetty-6.1.26
2015-02-20 18:26:34,199 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-20 18:26:34,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-20 18:26:34,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-20 18:26:34,619 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-20 18:26:34,643 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-20 18:26:34,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-20 18:26:34,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-20 18:26:34,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-20 18:26:34,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-20 18:26:34,809 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-20 18:26:34,812 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-20 18:26:36,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:37,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:38,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:39,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:40,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:41,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:42,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:43,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:44,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:45,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:45,102 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:26:51,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:52,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:53,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:54,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:55,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:56,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:57,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:58,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:26:59,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:00,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:00,113 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:27:06,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:07,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:08,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:09,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:10,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:11,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:12,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:13,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:14,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:15,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:15,124 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:27:21,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:22,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:23,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:24,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:25,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:26,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:27,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:28,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:29,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:30,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:30,135 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:27:36,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:37,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:38,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:39,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:40,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:41,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:42,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:43,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:44,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:45,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:45,145 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:27:51,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:52,147 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:53,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:54,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:55,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:56,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:57,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:58,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:27:59,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:00,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:00,154 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:28:06,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:07,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:08,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:09,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:10,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:11,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:12,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:13,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:14,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:15,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:15,166 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:28:21,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:22,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:23,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:24,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:25,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:26,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:27,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:28,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:29,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:30,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:30,174 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:28:36,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:37,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:38,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:39,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:40,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:41,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:42,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:43,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:44,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:45,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:45,183 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:28:51,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:52,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:53,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:54,186 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:55,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:56,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:57,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:58,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:28:59,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:00,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:00,192 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:29:06,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:07,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:08,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:09,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:10,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:11,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:12,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:13,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:14,199 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:15,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:15,201 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-20 18:29:21,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:22,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-20 18:29:22,749 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-20 18:29:22,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-20 18:30:00,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-20 18:30:00,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-20 18:30:01,809 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-20 18:30:02,392 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-20 18:30:02,621 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-20 18:30:02,621 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-20 18:30:02,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-20 18:30:02,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-20 18:30:02,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-20 18:30:02,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-20 18:30:02,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-20 18:30:03,223 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-20 18:30:03,229 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-20 18:30:03,245 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-20 18:30:03,249 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-20 18:30:03,249 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-20 18:30:03,249 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-20 18:30:03,275 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-20 18:30:03,281 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-20 18:30:03,281 INFO org.mortbay.log: jetty-6.1.26
2015-02-20 18:30:03,923 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-20 18:30:03,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-20 18:30:03,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-20 18:30:04,404 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-20 18:30:04,436 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-20 18:30:04,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-20 18:30:04,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-20 18:30:04,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-20 18:30:04,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-20 18:30:04,785 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-20 18:30:04,787 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-20 18:30:05,591 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-20 18:30:05,642 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 7004@shrinivaasanka-Inspiron-1545
2015-02-20 18:30:05,645 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-1364977614-127.0.0.1-1424437145093
2015-02-20 18:30:05,645 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-20 18:30:05,972 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1364977614-127.0.0.1-1424437145093
2015-02-20 18:30:05,972 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-20 18:30:05,973 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093 is not formatted.
2015-02-20 18:30:05,973 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-20 18:30:05,973 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1364977614-127.0.0.1-1424437145093 directory /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current
2015-02-20 18:30:06,013 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-20 18:30:06,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=733806595;bpid=BP-1364977614-127.0.0.1-1424437145093;lv=-56;nsInfo=lv=-60;cid=CID-88a44617-a33a-46f4-a7d1-5fbc1228e0ec;nsid=733806595;c=0;bpid=BP-1364977614-127.0.0.1-1424437145093;dnuuid=null
2015-02-20 18:30:06,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 20b8d798-a3bc-410d-9bab-b222410cbf7f
2015-02-20 18:30:06,297 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-20 18:30:06,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-20 18:30:06,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-20 18:30:06,347 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424450490347 with interval 21600000
2015-02-20 18:30:06,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1364977614-127.0.0.1-1424437145093
2015-02-20 18:30:06,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1364977614-127.0.0.1-1424437145093 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-20 18:30:06,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1364977614-127.0.0.1-1424437145093 on /tmp/hadoop-root/dfs/data/current: 61ms
2015-02-20 18:30:06,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1364977614-127.0.0.1-1424437145093: 71ms
2015-02-20 18:30:06,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1364977614-127.0.0.1-1424437145093 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-20 18:30:06,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1364977614-127.0.0.1-1424437145093 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-20 18:30:06,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2015-02-20 18:30:06,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1364977614-127.0.0.1-1424437145093 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-20 18:30:06,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1364977614-127.0.0.1-1424437145093 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-20 18:30:06,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-20 18:30:06,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1364977614-127.0.0.1-1424437145093 (Datanode Uuid 20b8d798-a3bc-410d-9bab-b222410cbf7f) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-20 18:30:06,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1364977614-127.0.0.1-1424437145093 (Datanode Uuid 20b8d798-a3bc-410d-9bab-b222410cbf7f) service to localhost/127.0.0.1:9000
2015-02-20 18:30:07,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 7 msec to generate and 128 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@797c67
2015-02-20 18:30:07,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1364977614-127.0.0.1-1424437145093
2015-02-20 18:30:07,127 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-20 18:30:07,127 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-20 18:30:07,134 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-20 18:30:07,134 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-20 18:30:07,138 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1364977614-127.0.0.1-1424437145093
2015-02-20 18:30:07,149 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1364977614-127.0.0.1-1424437145093 to blockPoolScannerMap, new size=1
2015-02-20 18:35:04,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741825_1001 src: /127.0.0.1:38335 dest: /127.0.0.1:50010
2015-02-20 18:35:04,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38335, dest: /127.0.0.1:50010, bytes: 7, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-613491438_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741825_1001, duration: 110443093
2015-02-20 18:35:05,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:35:05,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741826_1002 src: /127.0.0.1:38336 dest: /127.0.0.1:50010
2015-02-20 18:35:05,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38336, dest: /127.0.0.1:50010, bytes: 42, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-613491438_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741826_1002, duration: 6049863
2015-02-20 18:35:05,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:35:06,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741827_1003 src: /127.0.0.1:38338 dest: /127.0.0.1:50010
2015-02-20 18:35:06,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38338, dest: /127.0.0.1:50010, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-613491438_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741827_1003, duration: 4472631
2015-02-20 18:35:06,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:35:06,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741828_1004 src: /127.0.0.1:38339 dest: /127.0.0.1:50010
2015-02-20 18:35:06,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38339, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-613491438_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741828_1004, duration: 417596262
2015-02-20 18:35:06,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:35:06,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741829_1005 src: /127.0.0.1:38340 dest: /127.0.0.1:50010
2015-02-20 18:35:06,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38340, dest: /127.0.0.1:50010, bytes: 372, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-613491438_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741829_1005, duration: 3943521
2015-02-20 18:35:06,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:35:10,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741830_1006 src: /127.0.0.1:38344 dest: /127.0.0.1:50010
2015-02-20 18:35:11,405 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741826_1002
2015-02-20 18:35:11,424 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741827_1003
2015-02-20 18:35:12,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741831_1007 src: /127.0.0.1:38348 dest: /127.0.0.1:50010
2015-02-20 18:35:16,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741829_1005
2015-02-20 18:38:43,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38348, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437497297_-1002918164_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741831_1007, duration: 211066757934
2015-02-20 18:38:43,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:38:43,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38344, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437497297_-1002918164_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741830_1006, duration: 213975386682
2015-02-20 18:38:43,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:38:51,585 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741831_1007
2015-02-20 18:38:58,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741832_1008 src: /127.0.0.1:38447 dest: /127.0.0.1:50010
2015-02-20 18:39:00,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741833_1009 src: /127.0.0.1:38452 dest: /127.0.0.1:50010
2015-02-20 18:39:02,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741834_1010 src: /127.0.0.1:38454 dest: /127.0.0.1:50010
2015-02-20 18:39:02,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38454, dest: /127.0.0.1:50010, bytes: 286, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1062663197_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741834_1010, duration: 21962496
2015-02-20 18:39:02,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:02,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741835_1011 src: /127.0.0.1:38456 dest: /127.0.0.1:50010
2015-02-20 18:39:02,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38456, dest: /127.0.0.1:50010, bytes: 40, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1062663197_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741835_1011, duration: 12409052
2015-02-20 18:39:02,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:02,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741836_1012 src: /127.0.0.1:38457 dest: /127.0.0.1:50010
2015-02-20 18:39:02,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38452, dest: /127.0.0.1:50010, bytes: 268, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741833_1009, duration: 2349619947
2015-02-20 18:39:02,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:03,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741837_1013 src: /127.0.0.1:38461 dest: /127.0.0.1:50010
2015-02-20 18:39:03,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38457, dest: /127.0.0.1:50010, bytes: 471, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741836_1012, duration: 338153125
2015-02-20 18:39:03,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:03,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741838_1014 src: /127.0.0.1:38462 dest: /127.0.0.1:50010
2015-02-20 18:39:03,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38447, dest: /127.0.0.1:50010, bytes: 303, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741832_1008, duration: 4862280606
2015-02-20 18:39:03,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:11,620 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741832_1008
2015-02-20 18:39:35,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741839_1015 src: /127.0.0.1:38469 dest: /127.0.0.1:50010
2015-02-20 18:39:35,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38469, dest: /127.0.0.1:50010, bytes: 289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1062663197_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741839_1015, duration: 9060860
2015-02-20 18:39:35,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:35,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741840_1016 src: /127.0.0.1:38470 dest: /127.0.0.1:50010
2015-02-20 18:39:35,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38470, dest: /127.0.0.1:50010, bytes: 44, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1062663197_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741840_1016, duration: 18168583
2015-02-20 18:39:35,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:35,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741841_1017 src: /127.0.0.1:38472 dest: /127.0.0.1:50010
2015-02-20 18:39:35,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38461, dest: /127.0.0.1:50010, bytes: 268, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741837_1013, duration: 32287174651
2015-02-20 18:39:35,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:35,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741842_1018 src: /127.0.0.1:38477 dest: /127.0.0.1:50010
2015-02-20 18:39:35,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38472, dest: /127.0.0.1:50010, bytes: 459, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741841_1017, duration: 211351526
2015-02-20 18:39:35,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:39:41,632 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741839_1015
2015-02-20 18:40:25,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741843_1019 src: /127.0.0.1:38486 dest: /127.0.0.1:50010
2015-02-20 18:40:25,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38462, dest: /127.0.0.1:50010, bytes: 519, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741838_1014, duration: 82424466553
2015-02-20 18:40:25,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:40:25,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741844_1020 src: /127.0.0.1:38487 dest: /127.0.0.1:50010
2015-02-20 18:40:25,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38486, dest: /127.0.0.1:50010, bytes: 734, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741843_1019, duration: 165772547
2015-02-20 18:40:25,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:40:25,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741845_1021 src: /127.0.0.1:38488 dest: /127.0.0.1:50010
2015-02-20 18:40:25,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38487, dest: /127.0.0.1:50010, bytes: 739, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741844_1020, duration: 129069272
2015-02-20 18:40:25,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:40:25,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741846_1022 src: /127.0.0.1:38489 dest: /127.0.0.1:50010
2015-02-20 18:40:26,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38488, dest: /127.0.0.1:50010, bytes: 847, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741845_1021, duration: 145264002
2015-02-20 18:40:26,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 18:40:31,653 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741845_1021
2015-02-20 18:40:31,653 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741838_1014
2015-02-20 18:49:39,708 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-02-20 18:49:39,710 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741830_1006 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741830
2015-02-20 19:39:12,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741847_1023 src: /127.0.0.1:38855 dest: /127.0.0.1:50010
2015-02-20 19:39:12,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38855, dest: /127.0.0.1:50010, bytes: 1045, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741847_1023, duration: 984414
2015-02-20 19:39:12,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 19:39:21,839 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741847_1023
2015-02-20 19:39:23,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741848_1024 src: /127.0.0.1:38859 dest: /127.0.0.1:50010
2015-02-20 19:39:23,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38859, dest: /127.0.0.1:50010, bytes: 1793, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741848_1024, duration: 2472312
2015-02-20 19:39:23,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 19:39:31,856 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741848_1024
2015-02-20 19:39:35,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741849_1025 src: /127.0.0.1:38861 dest: /127.0.0.1:50010
2015-02-20 19:39:35,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38477, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741842_1018, duration: 3600190188735
2015-02-20 19:39:35,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 19:39:41,871 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741842_1018
2015-02-20 19:40:26,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741850_1026 src: /127.0.0.1:38866 dest: /127.0.0.1:50010
2015-02-20 19:40:26,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38489, dest: /127.0.0.1:50010, bytes: 11270003, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741846_1022, duration: 3600188925165
2015-02-20 19:40:26,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 19:40:37,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741851_1027 src: /127.0.0.1:38868 dest: /127.0.0.1:50010
2015-02-20 19:40:38,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38868, dest: /127.0.0.1:50010, bytes: 5027332, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741851_1027, duration: 416930253
2015-02-20 19:40:38,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 19:40:41,132 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741846_1022
2015-02-20 19:40:50,744 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741851_1027
2015-02-20 19:44:07,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741852_1028 src: /127.0.0.1:38890 dest: /127.0.0.1:50010
2015-02-20 19:44:08,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38890, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1870024945_1, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741852_1028, duration: 81571828
2015-02-20 19:44:08,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 19:44:16,565 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741852_1028
2015-02-20 19:49:39,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-02-20 19:49:39,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-02-20 19:49:39,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-02-20 19:49:39,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741841_1017 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741841
2015-02-20 19:49:39,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-02-20 19:49:39,942 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-02-20 19:49:39,942 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741842_1018 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741842
2015-02-20 19:49:39,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741833_1009 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741833
2015-02-20 19:49:39,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741836
2015-02-20 19:49:39,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741837_1013 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741837
2015-02-20 19:50:39,944 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-02-20 19:50:39,945 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741832_1008 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741832
2015-02-20 20:07:10,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 21 blocks total. Took 11 msec to generate and 30 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@1f709a2
2015-02-20 20:07:10,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1364977614-127.0.0.1-1424437145093
2015-02-20 20:39:35,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741853_1029 src: /127.0.0.1:39188 dest: /127.0.0.1:50010
2015-02-20 20:39:35,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38861, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741849_1025, duration: 3600269389731
2015-02-20 20:39:35,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 20:39:41,763 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741849_1025
2015-02-20 20:40:26,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741854_1030 src: /127.0.0.1:39200 dest: /127.0.0.1:50010
2015-02-20 20:40:26,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38866, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741850_1026, duration: 3600194471127
2015-02-20 20:40:26,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 20:40:31,775 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741850_1026
2015-02-20 20:49:40,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-02-20 20:49:40,177 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741849_1025 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741849
2015-02-20 20:50:40,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-02-20 20:50:40,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-02-20 20:50:40,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-02-20 20:50:40,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-02-20 20:50:40,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-02-20 20:50:40,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-02-20 20:50:40,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741843_1019 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741843
2015-02-20 20:50:40,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741844_1020 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741844
2015-02-20 20:50:40,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741845_1021 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741845
2015-02-20 20:50:40,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741846_1022 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741846
2015-02-20 20:50:40,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741850_1026 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741850
2015-02-20 20:50:40,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741838_1014 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741838
2015-02-20 21:39:36,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741855_1031 src: /127.0.0.1:40134 dest: /127.0.0.1:50010
2015-02-20 21:39:36,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39188, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741853_1029, duration: 3600951974019
2015-02-20 21:39:36,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 21:39:42,185 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741853_1029
2015-02-20 21:40:26,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1364977614-127.0.0.1-1424437145093:blk_1073741856_1032 src: /127.0.0.1:40139 dest: /127.0.0.1:50010
2015-02-20 21:40:27,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39200, dest: /127.0.0.1:50010, bytes: 91, op: HDFS_WRITE, cliID: DFSClient_hb_rs_localhost,60020,1424437734753_1595150085_32, offset: 0, srvID: 20b8d798-a3bc-410d-9bab-b222410cbf7f, blockid: BP-1364977614-127.0.0.1-1424437145093:blk_1073741854_1030, duration: 3600675328286
2015-02-20 21:40:27,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1364977614-127.0.0.1-1424437145093:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-20 21:40:32,195 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1364977614-127.0.0.1-1424437145093:blk_1073741854_1030
2015-02-20 21:49:40,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-02-20 21:49:40,432 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741853_1029 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741853
2015-02-20 21:50:40,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-02-20 21:50:40,364 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1364977614-127.0.0.1-1424437145093 blk_1073741854_1030 file /tmp/hadoop-root/dfs/data/current/BP-1364977614-127.0.0.1-1424437145093/current/finalized/subdir0/subdir0/blk_1073741854
2015-02-20 22:11:30,688 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1364977614-127.0.0.1-1424437145093 Total blocks: 14, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-02-20 22:29:27,602 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-20 22:29:27,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-23 16:50:39,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-23 16:50:39,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-23 16:50:41,141 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-23 16:50:42,009 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-23 16:50:42,132 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-23 16:50:42,132 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-23 16:50:42,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-23 16:50:42,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-23 16:50:42,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-23 16:50:42,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-23 16:50:42,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-23 16:50:42,736 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-23 16:50:42,743 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-23 16:50:42,766 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-23 16:50:42,770 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-23 16:50:42,770 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-23 16:50:42,770 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-23 16:50:42,822 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-23 16:50:42,826 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-23 16:50:42,827 INFO org.mortbay.log: jetty-6.1.26
2015-02-23 16:50:43,791 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-23 16:50:43,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-23 16:50:43,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-23 16:50:44,418 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-23 16:50:44,467 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-23 16:50:44,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-23 16:50:44,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-23 16:50:44,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-23 16:50:44,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-23 16:50:44,849 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-23 16:50:44,851 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-23 16:50:46,151 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-23 16:50:46,214 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 6997@shrinivaasanka-Inspiron-1545
2015-02-23 16:50:46,216 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-55626325-127.0.0.1-1424690419215
2015-02-23 16:50:46,216 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-23 16:50:46,553 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-55626325-127.0.0.1-1424690419215
2015-02-23 16:50:46,553 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-23 16:50:46,554 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-55626325-127.0.0.1-1424690419215 is not formatted.
2015-02-23 16:50:46,554 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-23 16:50:46,554 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-55626325-127.0.0.1-1424690419215 directory /tmp/hadoop-root/dfs/data/current/BP-55626325-127.0.0.1-1424690419215/current
2015-02-23 16:50:46,586 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-23 16:50:46,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1267576813;bpid=BP-55626325-127.0.0.1-1424690419215;lv=-56;nsInfo=lv=-60;cid=CID-c53b56c4-8bcc-4233-816d-bc273bbda693;nsid=1267576813;c=0;bpid=BP-55626325-127.0.0.1-1424690419215;dnuuid=null
2015-02-23 16:50:46,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 4ed15a45-c6a4-47d9-96c9-67e1a61ded84
2015-02-23 16:50:46,950 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-23 16:50:46,952 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-23 16:50:46,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-23 16:50:47,043 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424706692043 with interval 21600000
2015-02-23 16:50:47,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-55626325-127.0.0.1-1424690419215
2015-02-23 16:50:47,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-55626325-127.0.0.1-1424690419215 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-23 16:50:47,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-55626325-127.0.0.1-1424690419215 on /tmp/hadoop-root/dfs/data/current: 129ms
2015-02-23 16:50:47,188 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-55626325-127.0.0.1-1424690419215: 140ms
2015-02-23 16:50:47,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-55626325-127.0.0.1-1424690419215 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-23 16:50:47,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-55626325-127.0.0.1-1424690419215 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-23 16:50:47,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2015-02-23 16:50:47,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-55626325-127.0.0.1-1424690419215 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-23 16:50:47,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-55626325-127.0.0.1-1424690419215 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-23 16:50:47,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-23 16:50:47,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-55626325-127.0.0.1-1424690419215 (Datanode Uuid 4ed15a45-c6a4-47d9-96c9-67e1a61ded84) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-23 16:50:47,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-55626325-127.0.0.1-1424690419215 (Datanode Uuid 4ed15a45-c6a4-47d9-96c9-67e1a61ded84) service to localhost/127.0.0.1:9000
2015-02-23 16:50:48,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 3 msec to generate and 226 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@ecdc51
2015-02-23 16:50:48,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-55626325-127.0.0.1-1424690419215
2015-02-23 16:50:48,226 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-23 16:50:48,226 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-23 16:50:48,228 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-23 16:50:48,228 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-23 16:50:48,230 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-55626325-127.0.0.1-1424690419215
2015-02-23 16:50:48,237 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-55626325-127.0.0.1-1424690419215 to blockPoolScannerMap, new size=1
2015-02-23 18:29:18,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 3086 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@19a32ef
2015-02-23 18:29:19,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-55626325-127.0.0.1-1424690419215
2015-02-23 18:29:18,876 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1376ms
No GCs detected
2015-02-23 19:52:19,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741825_1001 src: /127.0.0.1:57133 dest: /127.0.0.1:50010
2015-02-23 19:52:20,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57133, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741825_1001, duration: 233434786
2015-02-23 19:52:20,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:20,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741826_1002 src: /127.0.0.1:57134 dest: /127.0.0.1:50010
2015-02-23 19:52:20,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57134, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741826_1002, duration: 23745335
2015-02-23 19:52:20,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:20,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741827_1003 src: /127.0.0.1:57135 dest: /127.0.0.1:50010
2015-02-23 19:52:21,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57135, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741827_1003, duration: 370218379
2015-02-23 19:52:21,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:21,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741828_1004 src: /127.0.0.1:57136 dest: /127.0.0.1:50010
2015-02-23 19:52:21,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57136, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741828_1004, duration: 17854297
2015-02-23 19:52:21,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:21,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741829_1005 src: /127.0.0.1:57137 dest: /127.0.0.1:50010
2015-02-23 19:52:21,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57137, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741829_1005, duration: 5604412
2015-02-23 19:52:21,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:21,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741830_1006 src: /127.0.0.1:57138 dest: /127.0.0.1:50010
2015-02-23 19:52:21,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57138, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741830_1006, duration: 3849723
2015-02-23 19:52:21,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:21,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741831_1007 src: /127.0.0.1:57139 dest: /127.0.0.1:50010
2015-02-23 19:52:21,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57139, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741831_1007, duration: 2981943
2015-02-23 19:52:21,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:22,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741832_1008 src: /127.0.0.1:57140 dest: /127.0.0.1:50010
2015-02-23 19:52:22,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57140, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741832_1008, duration: 288890437
2015-02-23 19:52:22,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:22,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741833_1009 src: /127.0.0.1:57141 dest: /127.0.0.1:50010
2015-02-23 19:52:22,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57141, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741833_1009, duration: 6025490
2015-02-23 19:52:22,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:22,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741834_1010 src: /127.0.0.1:57142 dest: /127.0.0.1:50010
2015-02-23 19:52:22,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57142, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741834_1010, duration: 7191067
2015-02-23 19:52:22,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:22,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741835_1011 src: /127.0.0.1:57143 dest: /127.0.0.1:50010
2015-02-23 19:52:22,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57143, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_548594542_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741835_1011, duration: 12026462
2015-02-23 19:52:22,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:52:27,518 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741830_1006
2015-02-23 19:52:27,554 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741828_1004
2015-02-23 19:52:42,973 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741827_1003
2015-02-23 19:52:43,372 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741826_1002
2015-02-23 19:52:48,372 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741825_1001
2015-02-23 19:52:48,375 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741829_1005
2015-02-23 19:52:48,572 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741831_1007
2015-02-23 19:52:52,172 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741832_1008
2015-02-23 19:52:57,629 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741835_1011
2015-02-23 19:52:57,829 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741834_1010
2015-02-23 19:52:58,028 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741833_1009
2015-02-23 19:54:37,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741836_1012 src: /127.0.0.1:57147 dest: /127.0.0.1:50010
2015-02-23 19:54:37,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57147, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1705977304_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741836_1012, duration: 140098689
2015-02-23 19:54:37,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 19:54:43,677 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741836_1012
2015-02-23 20:00:24,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741837_1013 src: /127.0.0.1:57163 dest: /127.0.0.1:50010
2015-02-23 20:00:24,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57163, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741837_1013, duration: 208143644
2015-02-23 20:00:24,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:24,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741838_1014 src: /127.0.0.1:57164 dest: /127.0.0.1:50010
2015-02-23 20:00:24,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57164, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741838_1014, duration: 22618658
2015-02-23 20:00:24,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:25,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741839_1015 src: /127.0.0.1:57165 dest: /127.0.0.1:50010
2015-02-23 20:00:25,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57165, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741839_1015, duration: 451669587
2015-02-23 20:00:25,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:25,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741840_1016 src: /127.0.0.1:57166 dest: /127.0.0.1:50010
2015-02-23 20:00:25,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57166, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741840_1016, duration: 9032573
2015-02-23 20:00:25,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741841_1017 src: /127.0.0.1:57167 dest: /127.0.0.1:50010
2015-02-23 20:00:26,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57167, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741841_1017, duration: 8404913
2015-02-23 20:00:26,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741842_1018 src: /127.0.0.1:57168 dest: /127.0.0.1:50010
2015-02-23 20:00:26,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57168, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741842_1018, duration: 3146073
2015-02-23 20:00:26,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741843_1019 src: /127.0.0.1:57169 dest: /127.0.0.1:50010
2015-02-23 20:00:26,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57169, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741843_1019, duration: 8366422
2015-02-23 20:00:26,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741844_1020 src: /127.0.0.1:57170 dest: /127.0.0.1:50010
2015-02-23 20:00:26,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57170, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741844_1020, duration: 89836984
2015-02-23 20:00:26,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741845_1021 src: /127.0.0.1:57171 dest: /127.0.0.1:50010
2015-02-23 20:00:26,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57171, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741845_1021, duration: 4703045
2015-02-23 20:00:26,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741846_1022 src: /127.0.0.1:57172 dest: /127.0.0.1:50010
2015-02-23 20:00:26,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57172, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741846_1022, duration: 7785629
2015-02-23 20:00:26,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:26,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741847_1023 src: /127.0.0.1:57173 dest: /127.0.0.1:50010
2015-02-23 20:00:26,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57173, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741847_1023, duration: 19274447
2015-02-23 20:00:26,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:00:33,755 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741842_1018
2015-02-23 20:00:34,355 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741847_1023
2015-02-23 20:00:37,956 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741844_1020
2015-02-23 20:00:38,155 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741841_1017
2015-02-23 20:00:38,355 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741846_1022
2015-02-23 20:00:55,154 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741839_1015
2015-02-23 20:00:59,955 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741837_1013
2015-02-23 20:01:00,355 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741838_1014
2015-02-23 20:01:00,355 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741843_1019
2015-02-23 20:01:00,755 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741840_1016
2015-02-23 20:01:00,756 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741845_1021
2015-02-23 20:01:41,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741848_1024 src: /127.0.0.1:57176 dest: /127.0.0.1:50010
2015-02-23 20:01:41,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57176, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741848_1024, duration: 124453888
2015-02-23 20:01:41,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:41,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741849_1025 src: /127.0.0.1:57177 dest: /127.0.0.1:50010
2015-02-23 20:01:41,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57177, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741849_1025, duration: 9733223
2015-02-23 20:01:41,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:41,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741850_1026 src: /127.0.0.1:57178 dest: /127.0.0.1:50010
2015-02-23 20:01:41,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57178, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741850_1026, duration: 312224549
2015-02-23 20:01:41,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:42,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741851_1027 src: /127.0.0.1:57179 dest: /127.0.0.1:50010
2015-02-23 20:01:42,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57179, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741851_1027, duration: 13383618
2015-02-23 20:01:42,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:42,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741852_1028 src: /127.0.0.1:57180 dest: /127.0.0.1:50010
2015-02-23 20:01:42,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57180, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741852_1028, duration: 7204614
2015-02-23 20:01:42,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:42,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741853_1029 src: /127.0.0.1:57181 dest: /127.0.0.1:50010
2015-02-23 20:01:42,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57181, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741853_1029, duration: 6542941
2015-02-23 20:01:42,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:42,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741854_1030 src: /127.0.0.1:57182 dest: /127.0.0.1:50010
2015-02-23 20:01:42,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57182, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741854_1030, duration: 4489465
2015-02-23 20:01:42,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:42,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741855_1031 src: /127.0.0.1:57183 dest: /127.0.0.1:50010
2015-02-23 20:01:42,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57183, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741855_1031, duration: 41428874
2015-02-23 20:01:42,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:43,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741856_1032 src: /127.0.0.1:57184 dest: /127.0.0.1:50010
2015-02-23 20:01:43,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57184, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741856_1032, duration: 6337044
2015-02-23 20:01:43,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:43,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741857_1033 src: /127.0.0.1:57185 dest: /127.0.0.1:50010
2015-02-23 20:01:43,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57185, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741857_1033, duration: 12219640
2015-02-23 20:01:43,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:43,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741858_1034 src: /127.0.0.1:57186 dest: /127.0.0.1:50010
2015-02-23 20:01:43,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57186, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1618562164_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741858_1034, duration: 17751907
2015-02-23 20:01:43,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:01:50,789 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741857_1033
2015-02-23 20:01:55,790 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741848_1024
2015-02-23 20:01:55,791 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741853_1029
2015-02-23 20:01:55,990 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741852_1028
2015-02-23 20:01:59,789 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741855_1031
2015-02-23 20:02:16,590 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741850_1026
2015-02-23 20:02:16,790 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741849_1025
2015-02-23 20:02:16,989 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741854_1030
2015-02-23 20:02:17,389 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741858_1034
2015-02-23 20:02:17,589 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741856_1032
2015-02-23 20:02:17,989 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741851_1027
2015-02-23 20:04:37,395 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-55626325-127.0.0.1-1424690419215/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-02-23 20:04:37,397 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-55626325-127.0.0.1-1424690419215 blk_1073741836_1012 file /tmp/hadoop-root/dfs/data/current/BP-55626325-127.0.0.1-1424690419215/current/finalized/subdir0/subdir0/blk_1073741836
2015-02-23 20:05:17,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741859_1035 src: /127.0.0.1:57219 dest: /127.0.0.1:50010
2015-02-23 20:05:17,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57219, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1702077570_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741859_1035, duration: 181358119
2015-02-23 20:05:17,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:23,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741859_1035
2015-02-23 20:05:49,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741860_1036 src: /127.0.0.1:57221 dest: /127.0.0.1:50010
2015-02-23 20:05:49,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57221, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741860_1036, duration: 119034342
2015-02-23 20:05:49,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:49,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741861_1037 src: /127.0.0.1:57222 dest: /127.0.0.1:50010
2015-02-23 20:05:49,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57222, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741861_1037, duration: 14741539
2015-02-23 20:05:49,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:50,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741862_1038 src: /127.0.0.1:57223 dest: /127.0.0.1:50010
2015-02-23 20:05:50,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57223, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741862_1038, duration: 273521094
2015-02-23 20:05:50,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:51,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741863_1039 src: /127.0.0.1:57224 dest: /127.0.0.1:50010
2015-02-23 20:05:51,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57224, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741863_1039, duration: 17473729
2015-02-23 20:05:51,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:51,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741864_1040 src: /127.0.0.1:57225 dest: /127.0.0.1:50010
2015-02-23 20:05:51,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57225, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741864_1040, duration: 8074773
2015-02-23 20:05:51,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:51,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741865_1041 src: /127.0.0.1:57226 dest: /127.0.0.1:50010
2015-02-23 20:05:51,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57226, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741865_1041, duration: 6234735
2015-02-23 20:05:51,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:51,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741866_1042 src: /127.0.0.1:57227 dest: /127.0.0.1:50010
2015-02-23 20:05:51,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57227, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741866_1042, duration: 4672594
2015-02-23 20:05:51,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:51,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741867_1043 src: /127.0.0.1:57228 dest: /127.0.0.1:50010
2015-02-23 20:05:51,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57228, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741867_1043, duration: 104425571
2015-02-23 20:05:51,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:52,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741868_1044 src: /127.0.0.1:57229 dest: /127.0.0.1:50010
2015-02-23 20:05:52,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57229, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741868_1044, duration: 6217552
2015-02-23 20:05:52,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:52,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741869_1045 src: /127.0.0.1:57230 dest: /127.0.0.1:50010
2015-02-23 20:05:52,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57230, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741869_1045, duration: 18613119
2015-02-23 20:05:52,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:52,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741870_1046 src: /127.0.0.1:57231 dest: /127.0.0.1:50010
2015-02-23 20:05:52,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57231, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-558626354_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741870_1046, duration: 14478447
2015-02-23 20:05:52,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:05:58,658 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741865_1041
2015-02-23 20:05:58,858 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741864_1040
2015-02-23 20:05:58,859 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741866_1042
2015-02-23 20:05:59,059 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741868_1044
2015-02-23 20:06:02,659 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741867_1043
2015-02-23 20:06:07,659 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741860_1036
2015-02-23 20:06:08,058 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741861_1037
2015-02-23 20:06:24,858 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741862_1038
2015-02-23 20:06:25,058 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741863_1039
2015-02-23 20:06:25,658 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741870_1046
2015-02-23 20:06:25,659 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741869_1045
2015-02-23 20:08:09,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741871_1047 src: /127.0.0.1:57241 dest: /127.0.0.1:50010
2015-02-23 20:08:09,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57241, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741871_1047, duration: 130376285
2015-02-23 20:08:09,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:09,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741872_1048 src: /127.0.0.1:57242 dest: /127.0.0.1:50010
2015-02-23 20:08:09,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57242, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741872_1048, duration: 11034915
2015-02-23 20:08:09,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:09,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741873_1049 src: /127.0.0.1:57243 dest: /127.0.0.1:50010
2015-02-23 20:08:10,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57243, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741873_1049, duration: 491489999
2015-02-23 20:08:10,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:10,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741874_1050 src: /127.0.0.1:57244 dest: /127.0.0.1:50010
2015-02-23 20:08:10,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57244, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741874_1050, duration: 13222419
2015-02-23 20:08:10,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:10,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741875_1051 src: /127.0.0.1:57245 dest: /127.0.0.1:50010
2015-02-23 20:08:10,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57245, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741875_1051, duration: 5973805
2015-02-23 20:08:10,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:10,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741876_1052 src: /127.0.0.1:57246 dest: /127.0.0.1:50010
2015-02-23 20:08:10,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57246, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741876_1052, duration: 6603496
2015-02-23 20:08:10,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:10,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741877_1053 src: /127.0.0.1:57247 dest: /127.0.0.1:50010
2015-02-23 20:08:10,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57247, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741877_1053, duration: 8070298
2015-02-23 20:08:10,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:10,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741878_1054 src: /127.0.0.1:57248 dest: /127.0.0.1:50010
2015-02-23 20:08:11,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57248, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741878_1054, duration: 100691220
2015-02-23 20:08:11,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:11,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741879_1055 src: /127.0.0.1:57249 dest: /127.0.0.1:50010
2015-02-23 20:08:11,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57249, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741879_1055, duration: 5174818
2015-02-23 20:08:11,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:11,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741880_1056 src: /127.0.0.1:57250 dest: /127.0.0.1:50010
2015-02-23 20:08:11,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57250, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741880_1056, duration: 11261124
2015-02-23 20:08:11,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:11,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741881_1057 src: /127.0.0.1:57251 dest: /127.0.0.1:50010
2015-02-23 20:08:11,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57251, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741881_1057, duration: 17459902
2015-02-23 20:08:11,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:20,527 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741871_1047
2015-02-23 20:08:37,326 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741873_1049
2015-02-23 20:08:37,726 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741872_1048
2015-02-23 20:08:42,738 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741876_1052
2015-02-23 20:08:42,739 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741880_1056
2015-02-23 20:08:43,340 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741881_1057
2015-02-23 20:08:43,341 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741877_1053
2015-02-23 20:08:43,540 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741874_1050
2015-02-23 20:08:43,740 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741879_1055
2015-02-23 20:08:47,539 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741878_1054
2015-02-23 20:08:47,541 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741875_1051
2015-02-23 20:08:51,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741882_1058 src: /127.0.0.1:57254 dest: /127.0.0.1:50010
2015-02-23 20:08:52,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57254, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741882_1058, duration: 101930905
2015-02-23 20:08:52,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:52,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741883_1059 src: /127.0.0.1:57255 dest: /127.0.0.1:50010
2015-02-23 20:08:52,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57255, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741883_1059, duration: 27946994
2015-02-23 20:08:52,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:52,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741884_1060 src: /127.0.0.1:57256 dest: /127.0.0.1:50010
2015-02-23 20:08:53,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57256, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741884_1060, duration: 477787152
2015-02-23 20:08:53,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:53,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741885_1061 src: /127.0.0.1:57257 dest: /127.0.0.1:50010
2015-02-23 20:08:53,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57257, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741885_1061, duration: 23479872
2015-02-23 20:08:53,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:53,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741886_1062 src: /127.0.0.1:57258 dest: /127.0.0.1:50010
2015-02-23 20:08:53,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57258, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741886_1062, duration: 7341442
2015-02-23 20:08:53,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:53,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741887_1063 src: /127.0.0.1:57259 dest: /127.0.0.1:50010
2015-02-23 20:08:53,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57259, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741887_1063, duration: 3299512
2015-02-23 20:08:53,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:53,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741888_1064 src: /127.0.0.1:57260 dest: /127.0.0.1:50010
2015-02-23 20:08:53,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57260, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741888_1064, duration: 1047480
2015-02-23 20:08:53,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:53,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741889_1065 src: /127.0.0.1:57261 dest: /127.0.0.1:50010
2015-02-23 20:08:53,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57261, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741889_1065, duration: 93168134
2015-02-23 20:08:53,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:55,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741890_1066 src: /127.0.0.1:57262 dest: /127.0.0.1:50010
2015-02-23 20:08:55,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57262, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741890_1066, duration: 13909238
2015-02-23 20:08:55,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:55,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741891_1067 src: /127.0.0.1:57263 dest: /127.0.0.1:50010
2015-02-23 20:08:55,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57263, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741891_1067, duration: 23487552
2015-02-23 20:08:55,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:08:55,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741892_1068 src: /127.0.0.1:57264 dest: /127.0.0.1:50010
2015-02-23 20:08:55,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57264, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1962348704_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741892_1068, duration: 16760859
2015-02-23 20:08:55,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:09:02,370 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741882_1058
2015-02-23 20:09:02,769 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741883_1059
2015-02-23 20:09:08,184 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741892_1068
2015-02-23 20:09:08,184 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741888_1064
2015-02-23 20:09:08,384 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741885_1061
2015-02-23 20:09:08,583 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741887_1063
2015-02-23 20:09:08,585 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741890_1066
2015-02-23 20:09:25,384 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741884_1060
2015-02-23 20:09:25,583 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741891_1067
2015-02-23 20:09:25,783 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741886_1062
2015-02-23 20:09:29,583 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741889_1065
2015-02-23 20:20:23,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741893_1069 src: /127.0.0.1:57304 dest: /127.0.0.1:50010
2015-02-23 20:20:24,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57304, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741893_1069, duration: 146916731
2015-02-23 20:20:24,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741893_1069, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:24,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741894_1070 src: /127.0.0.1:57305 dest: /127.0.0.1:50010
2015-02-23 20:20:24,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57305, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741894_1070, duration: 17816439
2015-02-23 20:20:24,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741894_1070, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:24,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741895_1071 src: /127.0.0.1:57306 dest: /127.0.0.1:50010
2015-02-23 20:20:25,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57306, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741895_1071, duration: 710760638
2015-02-23 20:20:25,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:25,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741896_1072 src: /127.0.0.1:57307 dest: /127.0.0.1:50010
2015-02-23 20:20:25,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57307, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741896_1072, duration: 11387966
2015-02-23 20:20:25,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741896_1072, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:25,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741897_1073 src: /127.0.0.1:57308 dest: /127.0.0.1:50010
2015-02-23 20:20:25,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57308, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741897_1073, duration: 24240724
2015-02-23 20:20:25,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741897_1073, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:25,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741898_1074 src: /127.0.0.1:57309 dest: /127.0.0.1:50010
2015-02-23 20:20:25,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57309, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741898_1074, duration: 5983234
2015-02-23 20:20:25,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741898_1074, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:25,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741899_1075 src: /127.0.0.1:57310 dest: /127.0.0.1:50010
2015-02-23 20:20:25,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57310, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741899_1075, duration: 4004490
2015-02-23 20:20:25,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:25,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741900_1076 src: /127.0.0.1:57311 dest: /127.0.0.1:50010
2015-02-23 20:20:26,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57311, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741900_1076, duration: 164816981
2015-02-23 20:20:26,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:26,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741901_1077 src: /127.0.0.1:57312 dest: /127.0.0.1:50010
2015-02-23 20:20:26,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57312, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741901_1077, duration: 5020260
2015-02-23 20:20:26,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:26,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741902_1078 src: /127.0.0.1:57313 dest: /127.0.0.1:50010
2015-02-23 20:20:26,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57313, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741902_1078, duration: 9273735
2015-02-23 20:20:26,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:26,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741903_1079 src: /127.0.0.1:57314 dest: /127.0.0.1:50010
2015-02-23 20:20:26,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57314, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1000905578_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741903_1079, duration: 11733125
2015-02-23 20:20:26,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:20:29,655 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741894_1070
2015-02-23 20:20:34,654 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741893_1069
2015-02-23 20:20:40,066 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741903_1079
2015-02-23 20:20:56,666 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741895_1071
2015-02-23 20:20:56,866 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741901_1077
2015-02-23 20:20:57,065 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741898_1074
2015-02-23 20:20:59,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741904_1080 src: /127.0.0.1:57316 dest: /127.0.0.1:50010
2015-02-23 20:21:00,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57316, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741904_1080, duration: 407703872
2015-02-23 20:21:00,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:00,665 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741900_1076
2015-02-23 20:21:00,866 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741899_1075
2015-02-23 20:21:00,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741905_1081 src: /127.0.0.1:57317 dest: /127.0.0.1:50010
2015-02-23 20:21:00,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57317, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741905_1081, duration: 22644360
2015-02-23 20:21:00,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:00,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741906_1082 src: /127.0.0.1:57318 dest: /127.0.0.1:50010
2015-02-23 20:21:01,065 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741896_1072
2015-02-23 20:21:01,268 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741897_1073
2015-02-23 20:21:01,465 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741902_1078
2015-02-23 20:21:01,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57318, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741906_1082, duration: 592123551
2015-02-23 20:21:01,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741907_1083 src: /127.0.0.1:57319 dest: /127.0.0.1:50010
2015-02-23 20:21:02,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57319, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741907_1083, duration: 16155826
2015-02-23 20:21:02,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741908_1084 src: /127.0.0.1:57320 dest: /127.0.0.1:50010
2015-02-23 20:21:02,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57320, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741908_1084, duration: 15690893
2015-02-23 20:21:02,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741909_1085 src: /127.0.0.1:57321 dest: /127.0.0.1:50010
2015-02-23 20:21:02,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57321, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741909_1085, duration: 3259979
2015-02-23 20:21:02,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741909_1085, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741910_1086 src: /127.0.0.1:57322 dest: /127.0.0.1:50010
2015-02-23 20:21:02,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57322, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741910_1086, duration: 6424357
2015-02-23 20:21:02,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741910_1086, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741911_1087 src: /127.0.0.1:57323 dest: /127.0.0.1:50010
2015-02-23 20:21:02,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57323, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741911_1087, duration: 119482934
2015-02-23 20:21:02,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741911_1087, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741912_1088 src: /127.0.0.1:57324 dest: /127.0.0.1:50010
2015-02-23 20:21:02,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57324, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741912_1088, duration: 15371720
2015-02-23 20:21:02,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741912_1088, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741913_1089 src: /127.0.0.1:57325 dest: /127.0.0.1:50010
2015-02-23 20:21:02,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57325, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741913_1089, duration: 6229285
2015-02-23 20:21:02,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741913_1089, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:02,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741914_1090 src: /127.0.0.1:57326 dest: /127.0.0.1:50010
2015-02-23 20:21:02,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57326, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995480932_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741914_1090, duration: 15529701
2015-02-23 20:21:02,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741914_1090, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:21:11,080 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741904_1080
2015-02-23 20:21:11,480 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741905_1081
2015-02-23 20:21:19,896 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741911_1087
2015-02-23 20:21:20,096 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741912_1088
2015-02-23 20:21:20,495 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741907_1083
2015-02-23 20:21:37,096 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741906_1082
2015-02-23 20:21:37,696 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741914_1090
2015-02-23 20:21:37,896 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741913_1089
2015-02-23 20:21:37,896 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741910_1086
2015-02-23 20:21:38,095 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741909_1085
2015-02-23 20:21:38,295 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741908_1084
2015-02-23 20:33:21,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741915_1091 src: /127.0.0.1:57361 dest: /127.0.0.1:50010
2015-02-23 20:33:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57361, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741915_1091, duration: 263069275
2015-02-23 20:33:21,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:22,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741916_1092 src: /127.0.0.1:57362 dest: /127.0.0.1:50010
2015-02-23 20:33:22,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57362, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741916_1092, duration: 16797452
2015-02-23 20:33:22,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741916_1092, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:22,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741917_1093 src: /127.0.0.1:57363 dest: /127.0.0.1:50010
2015-02-23 20:33:23,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57363, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741917_1093, duration: 402383992
2015-02-23 20:33:23,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741917_1093, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:23,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741918_1094 src: /127.0.0.1:57364 dest: /127.0.0.1:50010
2015-02-23 20:33:23,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57364, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741918_1094, duration: 9310681
2015-02-23 20:33:23,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741918_1094, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:23,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741919_1095 src: /127.0.0.1:57365 dest: /127.0.0.1:50010
2015-02-23 20:33:23,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57365, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741919_1095, duration: 11884470
2015-02-23 20:33:23,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:23,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741920_1096 src: /127.0.0.1:57366 dest: /127.0.0.1:50010
2015-02-23 20:33:23,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57366, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741920_1096, duration: 6273425
2015-02-23 20:33:23,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741920_1096, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:24,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741921_1097 src: /127.0.0.1:57367 dest: /127.0.0.1:50010
2015-02-23 20:33:24,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57367, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741921_1097, duration: 3146350
2015-02-23 20:33:24,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741921_1097, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:24,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741922_1098 src: /127.0.0.1:57368 dest: /127.0.0.1:50010
2015-02-23 20:33:24,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57368, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741922_1098, duration: 147488940
2015-02-23 20:33:24,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741922_1098, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:25,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741923_1099 src: /127.0.0.1:57369 dest: /127.0.0.1:50010
2015-02-23 20:33:25,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57369, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741923_1099, duration: 4081175
2015-02-23 20:33:25,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741923_1099, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:25,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741924_1100 src: /127.0.0.1:57370 dest: /127.0.0.1:50010
2015-02-23 20:33:25,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57370, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741924_1100, duration: 4032844
2015-02-23 20:33:25,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741924_1100, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:25,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741925_1101 src: /127.0.0.1:57371 dest: /127.0.0.1:50010
2015-02-23 20:33:25,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57371, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741925_1101, duration: 19901204
2015-02-23 20:33:25,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741925_1101, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:28,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741926_1102 src: /127.0.0.1:57372 dest: /127.0.0.1:50010
2015-02-23 20:33:29,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57372, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741926_1102, duration: 110411664
2015-02-23 20:33:29,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741926_1102, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:29,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741927_1103 src: /127.0.0.1:57373 dest: /127.0.0.1:50010
2015-02-23 20:33:29,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57373, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741927_1103, duration: 6354922
2015-02-23 20:33:29,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741927_1103, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:29,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741928_1104 src: /127.0.0.1:57374 dest: /127.0.0.1:50010
2015-02-23 20:33:30,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57374, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741928_1104, duration: 442892912
2015-02-23 20:33:30,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741928_1104, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:30,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741929_1105 src: /127.0.0.1:57375 dest: /127.0.0.1:50010
2015-02-23 20:33:30,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57375, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741929_1105, duration: 9671203
2015-02-23 20:33:30,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741929_1105, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:30,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741930_1106 src: /127.0.0.1:57376 dest: /127.0.0.1:50010
2015-02-23 20:33:30,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57376, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741930_1106, duration: 5135710
2015-02-23 20:33:30,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741930_1106, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:30,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741931_1107 src: /127.0.0.1:57377 dest: /127.0.0.1:50010
2015-02-23 20:33:30,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57377, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741931_1107, duration: 3137128
2015-02-23 20:33:30,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741931_1107, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:30,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741932_1108 src: /127.0.0.1:57378 dest: /127.0.0.1:50010
2015-02-23 20:33:30,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57378, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741932_1108, duration: 9895949
2015-02-23 20:33:30,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741932_1108, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:30,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741933_1109 src: /127.0.0.1:57379 dest: /127.0.0.1:50010
2015-02-23 20:33:30,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57379, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741933_1109, duration: 63633719
2015-02-23 20:33:30,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:31,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741934_1110 src: /127.0.0.1:57380 dest: /127.0.0.1:50010
2015-02-23 20:33:31,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57380, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741934_1110, duration: 6631148
2015-02-23 20:33:31,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741934_1110, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:31,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741935_1111 src: /127.0.0.1:57381 dest: /127.0.0.1:50010
2015-02-23 20:33:31,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57381, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741935_1111, duration: 5003851
2015-02-23 20:33:31,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741935_1111, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:31,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741936_1112 src: /127.0.0.1:57382 dest: /127.0.0.1:50010
2015-02-23 20:33:31,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57382, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1165009421_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741936_1112, duration: 11271545
2015-02-23 20:33:31,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741936_1112, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 20:33:33,001 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741915_1091
2015-02-23 20:33:33,401 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741918_1094
2015-02-23 20:33:33,602 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741916_1092
2015-02-23 20:33:50,402 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741917_1093
2015-02-23 20:33:55,417 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741934_1110
2015-02-23 20:33:55,617 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741931_1107
2015-02-23 20:33:59,218 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741933_1109
2015-02-23 20:33:59,218 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741921_1097
2015-02-23 20:33:59,617 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741929_1105
2015-02-23 20:33:59,817 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741930_1106
2015-02-23 20:34:03,418 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741922_1098
2015-02-23 20:34:03,418 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741932_1108
2015-02-23 20:34:20,217 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741928_1104
2015-02-23 20:34:20,418 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741923_1099
2015-02-23 20:34:20,618 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741924_1100
2015-02-23 20:34:20,818 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741919_1095
2015-02-23 20:34:21,218 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741925_1101
2015-02-23 20:34:21,818 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741936_1112
2015-02-23 20:34:22,218 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741927_1103
2015-02-23 20:34:22,226 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741920_1096
2015-02-23 20:34:22,418 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741935_1111
2015-02-23 20:34:27,417 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741926_1102
2015-02-23 21:04:39,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741937_1113 src: /127.0.0.1:57605 dest: /127.0.0.1:50010
2015-02-23 21:04:39,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57605, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741937_1113, duration: 160654222
2015-02-23 21:04:39,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:39,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741938_1114 src: /127.0.0.1:57606 dest: /127.0.0.1:50010
2015-02-23 21:04:39,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57606, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741938_1114, duration: 7223188
2015-02-23 21:04:39,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741938_1114, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:39,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741939_1115 src: /127.0.0.1:57607 dest: /127.0.0.1:50010
2015-02-23 21:04:40,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57607, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741939_1115, duration: 382799098
2015-02-23 21:04:40,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741939_1115, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:40,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741940_1116 src: /127.0.0.1:57608 dest: /127.0.0.1:50010
2015-02-23 21:04:40,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57608, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741940_1116, duration: 9838747
2015-02-23 21:04:40,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741940_1116, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:40,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741941_1117 src: /127.0.0.1:57609 dest: /127.0.0.1:50010
2015-02-23 21:04:40,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57609, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741941_1117, duration: 15844964
2015-02-23 21:04:40,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741941_1117, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:40,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741942_1118 src: /127.0.0.1:57610 dest: /127.0.0.1:50010
2015-02-23 21:04:40,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57610, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741942_1118, duration: 4111909
2015-02-23 21:04:40,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741942_1118, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:40,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741943_1119 src: /127.0.0.1:57611 dest: /127.0.0.1:50010
2015-02-23 21:04:40,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57611, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741943_1119, duration: 1860222
2015-02-23 21:04:40,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741943_1119, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:40,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741944_1120 src: /127.0.0.1:57612 dest: /127.0.0.1:50010
2015-02-23 21:04:40,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57612, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741944_1120, duration: 111276084
2015-02-23 21:04:40,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741944_1120, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:41,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741945_1121 src: /127.0.0.1:57613 dest: /127.0.0.1:50010
2015-02-23 21:04:41,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57613, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741945_1121, duration: 8227934
2015-02-23 21:04:41,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:41,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741946_1122 src: /127.0.0.1:57614 dest: /127.0.0.1:50010
2015-02-23 21:04:41,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57614, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741946_1122, duration: 4381283
2015-02-23 21:04:41,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741946_1122, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:41,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741947_1123 src: /127.0.0.1:57615 dest: /127.0.0.1:50010
2015-02-23 21:04:41,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57615, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2099576145_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741947_1123, duration: 17827828
2015-02-23 21:04:41,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741947_1123, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:04:47,539 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741942_1118
2015-02-23 21:04:47,540 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741946_1122
2015-02-23 21:04:52,540 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741937_1113
2015-02-23 21:04:52,541 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741943_1119
2015-02-23 21:04:52,741 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741940_1116
2015-02-23 21:04:52,940 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741945_1121
2015-02-23 21:05:09,740 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741939_1115
2015-02-23 21:05:10,140 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741938_1114
2015-02-23 21:05:10,340 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741941_1117
2015-02-23 21:05:10,741 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741947_1123
2015-02-23 21:05:14,541 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741944_1120
2015-02-23 21:05:47,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741948_1124 src: /127.0.0.1:57618 dest: /127.0.0.1:50010
2015-02-23 21:05:47,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57618, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741948_1124, duration: 149348464
2015-02-23 21:05:47,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741948_1124, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:47,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741949_1125 src: /127.0.0.1:57619 dest: /127.0.0.1:50010
2015-02-23 21:05:47,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57619, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741949_1125, duration: 22621584
2015-02-23 21:05:47,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:48,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741950_1126 src: /127.0.0.1:57620 dest: /127.0.0.1:50010
2015-02-23 21:05:48,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57620, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741950_1126, duration: 359818316
2015-02-23 21:05:48,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741950_1126, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:48,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741951_1127 src: /127.0.0.1:57621 dest: /127.0.0.1:50010
2015-02-23 21:05:48,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57621, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741951_1127, duration: 13945281
2015-02-23 21:05:48,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741951_1127, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:48,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741952_1128 src: /127.0.0.1:57622 dest: /127.0.0.1:50010
2015-02-23 21:05:48,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57622, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741952_1128, duration: 22136896
2015-02-23 21:05:48,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741952_1128, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:48,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741953_1129 src: /127.0.0.1:57623 dest: /127.0.0.1:50010
2015-02-23 21:05:48,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57623, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741953_1129, duration: 8162906
2015-02-23 21:05:48,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741953_1129, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:48,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741954_1130 src: /127.0.0.1:57624 dest: /127.0.0.1:50010
2015-02-23 21:05:48,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57624, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741954_1130, duration: 4927719
2015-02-23 21:05:48,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741954_1130, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:48,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741955_1131 src: /127.0.0.1:57625 dest: /127.0.0.1:50010
2015-02-23 21:05:48,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57625, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741955_1131, duration: 103838067
2015-02-23 21:05:48,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741955_1131, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:49,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741956_1132 src: /127.0.0.1:57626 dest: /127.0.0.1:50010
2015-02-23 21:05:49,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57626, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741956_1132, duration: 8710325
2015-02-23 21:05:49,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:49,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741957_1133 src: /127.0.0.1:57627 dest: /127.0.0.1:50010
2015-02-23 21:05:49,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57627, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741957_1133, duration: 6480570
2015-02-23 21:05:49,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741957_1133, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:49,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741958_1134 src: /127.0.0.1:57628 dest: /127.0.0.1:50010
2015-02-23 21:05:49,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57628, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741958_1134, duration: 18505565
2015-02-23 21:05:49,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:05:54,571 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741951_1127
2015-02-23 21:06:11,371 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741950_1126
2015-02-23 21:06:11,770 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741949_1125
2015-02-23 21:06:11,970 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741957_1133
2015-02-23 21:06:11,971 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741954_1130
2015-02-23 21:06:11,972 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741953_1129
2015-02-23 21:06:12,571 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741958_1134
2015-02-23 21:06:12,770 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741952_1128
2015-02-23 21:06:17,770 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741948_1124
2015-02-23 21:06:17,970 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741956_1132
2015-02-23 21:06:21,570 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741955_1131
2015-02-23 21:06:35,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741959_1135 src: /127.0.0.1:57631 dest: /127.0.0.1:50010
2015-02-23 21:06:35,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57631, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741959_1135, duration: 117563907
2015-02-23 21:06:35,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741959_1135, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:35,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741960_1136 src: /127.0.0.1:57632 dest: /127.0.0.1:50010
2015-02-23 21:06:35,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57632, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741960_1136, duration: 15821843
2015-02-23 21:06:35,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741960_1136, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:35,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741961_1137 src: /127.0.0.1:57633 dest: /127.0.0.1:50010
2015-02-23 21:06:35,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57633, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741961_1137, duration: 347165938
2015-02-23 21:06:35,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741961_1137, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741962_1138 src: /127.0.0.1:57634 dest: /127.0.0.1:50010
2015-02-23 21:06:36,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57634, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741962_1138, duration: 8875641
2015-02-23 21:06:36,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741962_1138, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741963_1139 src: /127.0.0.1:57635 dest: /127.0.0.1:50010
2015-02-23 21:06:36,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57635, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741963_1139, duration: 3911388
2015-02-23 21:06:36,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741963_1139, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741964_1140 src: /127.0.0.1:57636 dest: /127.0.0.1:50010
2015-02-23 21:06:36,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57636, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741964_1140, duration: 9120434
2015-02-23 21:06:36,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741964_1140, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741965_1141 src: /127.0.0.1:57637 dest: /127.0.0.1:50010
2015-02-23 21:06:36,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57637, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741965_1141, duration: 5311433
2015-02-23 21:06:36,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741965_1141, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741966_1142 src: /127.0.0.1:57638 dest: /127.0.0.1:50010
2015-02-23 21:06:36,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57638, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741966_1142, duration: 73063551
2015-02-23 21:06:36,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741966_1142, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,687 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741967_1143 src: /127.0.0.1:57639 dest: /127.0.0.1:50010
2015-02-23 21:06:36,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57639, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741967_1143, duration: 4332048
2015-02-23 21:06:36,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741967_1143, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741968_1144 src: /127.0.0.1:57640 dest: /127.0.0.1:50010
2015-02-23 21:06:36,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57640, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741968_1144, duration: 4263459
2015-02-23 21:06:36,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741968_1144, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:36,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741969_1145 src: /127.0.0.1:57641 dest: /127.0.0.1:50010
2015-02-23 21:06:36,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57641, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-344430898_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741969_1145, duration: 22599033
2015-02-23 21:06:36,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:06:44,998 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741966_1142
2015-02-23 21:06:45,398 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741960_1136
2015-02-23 21:07:02,197 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741961_1137
2015-02-23 21:07:07,197 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741959_1135
2015-02-23 21:07:07,198 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741965_1141
2015-02-23 21:07:07,397 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741963_1139
2015-02-23 21:07:07,598 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741962_1138
2015-02-23 21:07:07,797 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741964_1140
2015-02-23 21:07:12,817 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741967_1143
2015-02-23 21:07:12,818 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741968_1144
2015-02-23 21:07:13,418 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741969_1145
2015-02-23 21:10:58,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741970_1146 src: /127.0.0.1:57654 dest: /127.0.0.1:50010
2015-02-23 21:10:58,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57654, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741970_1146, duration: 118036867
2015-02-23 21:10:58,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:10:59,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741971_1147 src: /127.0.0.1:57655 dest: /127.0.0.1:50010
2015-02-23 21:10:59,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57655, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741971_1147, duration: 13355749
2015-02-23 21:10:59,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741971_1147, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:10:59,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741972_1148 src: /127.0.0.1:57656 dest: /127.0.0.1:50010
2015-02-23 21:10:59,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57656, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741972_1148, duration: 386710422
2015-02-23 21:10:59,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741972_1148, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:10:59,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741973_1149 src: /127.0.0.1:57657 dest: /127.0.0.1:50010
2015-02-23 21:10:59,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57657, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741973_1149, duration: 13701600
2015-02-23 21:10:59,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741974_1150 src: /127.0.0.1:57658 dest: /127.0.0.1:50010
2015-02-23 21:11:00,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57658, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741974_1150, duration: 7305470
2015-02-23 21:11:00,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741975_1151 src: /127.0.0.1:57659 dest: /127.0.0.1:50010
2015-02-23 21:11:00,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57659, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741975_1151, duration: 3330034
2015-02-23 21:11:00,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741976_1152 src: /127.0.0.1:57660 dest: /127.0.0.1:50010
2015-02-23 21:11:00,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57660, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741976_1152, duration: 4078244
2015-02-23 21:11:00,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741977_1153 src: /127.0.0.1:57661 dest: /127.0.0.1:50010
2015-02-23 21:11:00,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57661, dest: /127.0.0.1:50010, bytes: 3834621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741977_1153, duration: 70230856
2015-02-23 21:11:00,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741978_1154 src: /127.0.0.1:57662 dest: /127.0.0.1:50010
2015-02-23 21:11:00,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57662, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741978_1154, duration: 5534218
2015-02-23 21:11:00,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741978_1154, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741979_1155 src: /127.0.0.1:57663 dest: /127.0.0.1:50010
2015-02-23 21:11:00,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57663, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741979_1155, duration: 7210695
2015-02-23 21:11:00,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:00,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741980_1156 src: /127.0.0.1:57664 dest: /127.0.0.1:50010
2015-02-23 21:11:00,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57664, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_799692185_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741980_1156, duration: 20126585
2015-02-23 21:11:00,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:11:13,263 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741970_1146
2015-02-23 21:11:13,464 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741973_1149
2015-02-23 21:11:13,664 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741978_1154
2015-02-23 21:11:14,263 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741980_1156
2015-02-23 21:11:14,264 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741976_1152
2015-02-23 21:11:31,063 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741972_1148
2015-02-23 21:11:31,064 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741975_1151
2015-02-23 21:11:31,264 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741974_1150
2015-02-23 21:11:35,063 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741977_1153
2015-02-23 21:11:35,064 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741979_1155
2015-02-23 21:11:35,463 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741971_1147
2015-02-23 21:14:42,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741981_1157 src: /127.0.0.1:57670 dest: /127.0.0.1:50010
2015-02-23 21:14:42,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57670, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741981_1157, duration: 135833266
2015-02-23 21:14:42,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:43,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741982_1158 src: /127.0.0.1:57671 dest: /127.0.0.1:50010
2015-02-23 21:14:43,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57671, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741982_1158, duration: 10255986
2015-02-23 21:14:43,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741982_1158, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:43,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741983_1159 src: /127.0.0.1:57672 dest: /127.0.0.1:50010
2015-02-23 21:14:43,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57672, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741983_1159, duration: 360611779
2015-02-23 21:14:43,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:44,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741984_1160 src: /127.0.0.1:57673 dest: /127.0.0.1:50010
2015-02-23 21:14:44,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57673, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741984_1160, duration: 22746680
2015-02-23 21:14:44,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:44,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741985_1161 src: /127.0.0.1:57674 dest: /127.0.0.1:50010
2015-02-23 21:14:44,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57674, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741985_1161, duration: 14929966
2015-02-23 21:14:44,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:44,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741986_1162 src: /127.0.0.1:57675 dest: /127.0.0.1:50010
2015-02-23 21:14:44,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57675, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741986_1162, duration: 10591082
2015-02-23 21:14:44,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:44,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741987_1163 src: /127.0.0.1:57676 dest: /127.0.0.1:50010
2015-02-23 21:14:44,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57676, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741987_1163, duration: 80296033
2015-02-23 21:14:44,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741987_1163, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:44,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741988_1164 src: /127.0.0.1:57677 dest: /127.0.0.1:50010
2015-02-23 21:14:44,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57677, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741988_1164, duration: 73502295
2015-02-23 21:14:44,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741988_1164, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:45,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741989_1165 src: /127.0.0.1:57678 dest: /127.0.0.1:50010
2015-02-23 21:14:45,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57678, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741989_1165, duration: 3331500
2015-02-23 21:14:45,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741989_1165, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:45,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741990_1166 src: /127.0.0.1:57679 dest: /127.0.0.1:50010
2015-02-23 21:14:45,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57679, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741990_1166, duration: 15930797
2015-02-23 21:14:45,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:45,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741991_1167 src: /127.0.0.1:57680 dest: /127.0.0.1:50010
2015-02-23 21:14:45,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57680, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_642988846_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741991_1167, duration: 27739983
2015-02-23 21:14:45,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:14:55,311 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741981_1157
2015-02-23 21:14:55,511 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741982_1158
2015-02-23 21:14:55,711 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741986_1162
2015-02-23 21:14:55,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741989_1165
2015-02-23 21:14:56,111 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741984_1160
2015-02-23 21:14:56,112 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741987_1163
2015-02-23 21:15:00,111 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741988_1164
2015-02-23 21:15:00,311 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741985_1161
2015-02-23 21:15:17,111 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741983_1159
2015-02-23 21:15:22,134 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741990_1166
2015-02-23 21:15:22,535 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741991_1167
2015-02-23 21:21:16,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741992_1168 src: /127.0.0.1:57784 dest: /127.0.0.1:50010
2015-02-23 21:21:16,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57784, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741992_1168, duration: 147253921
2015-02-23 21:21:16,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741992_1168, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:17,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741993_1169 src: /127.0.0.1:57785 dest: /127.0.0.1:50010
2015-02-23 21:21:17,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57785, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741993_1169, duration: 8578956
2015-02-23 21:21:17,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741993_1169, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:17,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741994_1170 src: /127.0.0.1:57786 dest: /127.0.0.1:50010
2015-02-23 21:21:18,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57786, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741994_1170, duration: 393950934
2015-02-23 21:21:18,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741995_1171 src: /127.0.0.1:57788 dest: /127.0.0.1:50010
2015-02-23 21:21:18,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57788, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741995_1171, duration: 18646509
2015-02-23 21:21:18,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741996_1172 src: /127.0.0.1:57789 dest: /127.0.0.1:50010
2015-02-23 21:21:18,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57789, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741996_1172, duration: 12429793
2015-02-23 21:21:18,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741996_1172, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741997_1173 src: /127.0.0.1:57790 dest: /127.0.0.1:50010
2015-02-23 21:21:18,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57790, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741997_1173, duration: 4837209
2015-02-23 21:21:18,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741997_1173, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741998_1174 src: /127.0.0.1:57791 dest: /127.0.0.1:50010
2015-02-23 21:21:18,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57791, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741998_1174, duration: 14431512
2015-02-23 21:21:18,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741998_1174, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073741999_1175 src: /127.0.0.1:57792 dest: /127.0.0.1:50010
2015-02-23 21:21:18,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57792, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073741999_1175, duration: 114228282
2015-02-23 21:21:18,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073741999_1175, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742000_1176 src: /127.0.0.1:57793 dest: /127.0.0.1:50010
2015-02-23 21:21:18,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57793, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742000_1176, duration: 6305694
2015-02-23 21:21:18,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742000_1176, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742001_1177 src: /127.0.0.1:57794 dest: /127.0.0.1:50010
2015-02-23 21:21:18,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57794, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742001_1177, duration: 6056077
2015-02-23 21:21:18,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742001_1177, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:18,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742002_1178 src: /127.0.0.1:57795 dest: /127.0.0.1:50010
2015-02-23 21:21:19,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57795, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2055438382_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742002_1178, duration: 17097149
2015-02-23 21:21:19,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742002_1178, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:21:22,785 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741993_1169
2015-02-23 21:21:27,784 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741992_1168
2015-02-23 21:21:32,804 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742001_1177
2015-02-23 21:21:33,205 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742002_1178
2015-02-23 21:21:50,005 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741994_1170
2015-02-23 21:21:50,204 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741996_1172
2015-02-23 21:21:50,404 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741997_1173
2015-02-23 21:21:54,205 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741999_1175
2015-02-23 21:21:54,405 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742000_1176
2015-02-23 21:21:54,604 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741998_1174
2015-02-23 21:21:54,804 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073741995_1171
2015-02-23 21:21:59,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742003_1179 src: /127.0.0.1:57797 dest: /127.0.0.1:50010
2015-02-23 21:21:59,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57797, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742003_1179, duration: 133956702
2015-02-23 21:21:59,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742003_1179, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:00,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742004_1180 src: /127.0.0.1:57798 dest: /127.0.0.1:50010
2015-02-23 21:22:00,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57798, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742004_1180, duration: 18622621
2015-02-23 21:22:00,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742004_1180, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:00,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742005_1181 src: /127.0.0.1:57799 dest: /127.0.0.1:50010
2015-02-23 21:22:01,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57799, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742005_1181, duration: 428812424
2015-02-23 21:22:01,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742005_1181, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:01,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742006_1182 src: /127.0.0.1:57800 dest: /127.0.0.1:50010
2015-02-23 21:22:01,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57800, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742006_1182, duration: 12972880
2015-02-23 21:22:01,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742006_1182, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:01,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742007_1183 src: /127.0.0.1:57801 dest: /127.0.0.1:50010
2015-02-23 21:22:01,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57801, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742007_1183, duration: 16050716
2015-02-23 21:22:01,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742007_1183, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:01,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742008_1184 src: /127.0.0.1:57802 dest: /127.0.0.1:50010
2015-02-23 21:22:01,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57802, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742008_1184, duration: 2990255
2015-02-23 21:22:01,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742008_1184, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:01,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742009_1185 src: /127.0.0.1:57803 dest: /127.0.0.1:50010
2015-02-23 21:22:01,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57803, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742009_1185, duration: 4414178
2015-02-23 21:22:01,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742009_1185, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:01,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742010_1186 src: /127.0.0.1:57804 dest: /127.0.0.1:50010
2015-02-23 21:22:01,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57804, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742010_1186, duration: 118386629
2015-02-23 21:22:01,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742010_1186, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:02,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742011_1187 src: /127.0.0.1:57805 dest: /127.0.0.1:50010
2015-02-23 21:22:02,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57805, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742011_1187, duration: 4692217
2015-02-23 21:22:02,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742011_1187, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:02,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742012_1188 src: /127.0.0.1:57806 dest: /127.0.0.1:50010
2015-02-23 21:22:02,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57806, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742012_1188, duration: 5496231
2015-02-23 21:22:02,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742012_1188, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:02,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742013_1189 src: /127.0.0.1:57807 dest: /127.0.0.1:50010
2015-02-23 21:22:02,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57807, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1202593897_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742013_1189, duration: 8624282
2015-02-23 21:22:02,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742013_1189, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:22:09,843 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742006_1182
2015-02-23 21:22:10,043 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742008_1184
2015-02-23 21:22:10,245 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742004_1180
2015-02-23 21:22:27,044 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742005_1181
2015-02-23 21:22:27,644 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742013_1189
2015-02-23 21:22:32,644 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742003_1179
2015-02-23 21:22:32,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742012_1188
2015-02-23 21:22:32,844 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742011_1187
2015-02-23 21:22:36,844 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742010_1186
2015-02-23 21:22:36,845 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742009_1185
2015-02-23 21:22:37,044 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742007_1183
2015-02-23 21:23:30,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742014_1190 src: /127.0.0.1:57811 dest: /127.0.0.1:50010
2015-02-23 21:23:30,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57811, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742014_1190, duration: 123135980
2015-02-23 21:23:30,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742014_1190, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:31,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742015_1191 src: /127.0.0.1:57812 dest: /127.0.0.1:50010
2015-02-23 21:23:31,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57812, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742015_1191, duration: 23712792
2015-02-23 21:23:31,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742015_1191, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:31,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742016_1192 src: /127.0.0.1:57813 dest: /127.0.0.1:50010
2015-02-23 21:23:31,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57813, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742016_1192, duration: 312489391
2015-02-23 21:23:31,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742016_1192, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:31,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742017_1193 src: /127.0.0.1:57814 dest: /127.0.0.1:50010
2015-02-23 21:23:31,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57814, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742017_1193, duration: 17410453
2015-02-23 21:23:31,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742018_1194 src: /127.0.0.1:57815 dest: /127.0.0.1:50010
2015-02-23 21:23:32,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57815, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742018_1194, duration: 9612951
2015-02-23 21:23:32,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742019_1195 src: /127.0.0.1:57816 dest: /127.0.0.1:50010
2015-02-23 21:23:32,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57816, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742019_1195, duration: 3699213
2015-02-23 21:23:32,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742019_1195, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742020_1196 src: /127.0.0.1:57817 dest: /127.0.0.1:50010
2015-02-23 21:23:32,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57817, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742020_1196, duration: 13834437
2015-02-23 21:23:32,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742021_1197 src: /127.0.0.1:57818 dest: /127.0.0.1:50010
2015-02-23 21:23:32,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57818, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742021_1197, duration: 89390138
2015-02-23 21:23:32,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742021_1197, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742022_1198 src: /127.0.0.1:57819 dest: /127.0.0.1:50010
2015-02-23 21:23:32,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57819, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742022_1198, duration: 9512592
2015-02-23 21:23:32,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742022_1198, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742023_1199 src: /127.0.0.1:57820 dest: /127.0.0.1:50010
2015-02-23 21:23:32,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57820, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742023_1199, duration: 56291653
2015-02-23 21:23:32,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742023_1199, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:32,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742024_1200 src: /127.0.0.1:57821 dest: /127.0.0.1:50010
2015-02-23 21:23:32,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57821, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-716385647_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742024_1200, duration: 17842562
2015-02-23 21:23:32,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:23:37,273 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742017_1193
2015-02-23 21:23:53,873 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742016_1192
2015-02-23 21:23:58,873 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742014_1190
2015-02-23 21:23:59,272 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742015_1191
2015-02-23 21:23:59,472 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742018_1194
2015-02-23 21:24:04,490 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742020_1196
2015-02-23 21:24:04,491 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742023_1199
2015-02-23 21:24:04,892 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742024_1200
2015-02-23 21:24:05,092 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742022_1198
2015-02-23 21:24:09,092 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742021_1197
2015-02-23 21:24:09,292 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742019_1195
2015-02-23 21:29:19,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742025_1201 src: /127.0.0.1:57901 dest: /127.0.0.1:50010
2015-02-23 21:29:19,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57901, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742025_1201, duration: 120688186
2015-02-23 21:29:19,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:20,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742026_1202 src: /127.0.0.1:57902 dest: /127.0.0.1:50010
2015-02-23 21:29:20,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57902, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742026_1202, duration: 25692937
2015-02-23 21:29:20,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742026_1202, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:20,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742027_1203 src: /127.0.0.1:57903 dest: /127.0.0.1:50010
2015-02-23 21:29:21,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57903, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742027_1203, duration: 367083551
2015-02-23 21:29:21,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742027_1203, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:21,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742028_1204 src: /127.0.0.1:57904 dest: /127.0.0.1:50010
2015-02-23 21:29:21,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57904, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742028_1204, duration: 4235183
2015-02-23 21:29:21,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:21,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742029_1205 src: /127.0.0.1:57905 dest: /127.0.0.1:50010
2015-02-23 21:29:21,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57905, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742029_1205, duration: 12989011
2015-02-23 21:29:21,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742029_1205, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:21,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742030_1206 src: /127.0.0.1:57906 dest: /127.0.0.1:50010
2015-02-23 21:29:21,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57906, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742030_1206, duration: 18937947
2015-02-23 21:29:21,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742030_1206, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:21,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742031_1207 src: /127.0.0.1:57907 dest: /127.0.0.1:50010
2015-02-23 21:29:21,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57907, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742031_1207, duration: 2145661
2015-02-23 21:29:21,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742031_1207, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:21,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742032_1208 src: /127.0.0.1:57908 dest: /127.0.0.1:50010
2015-02-23 21:29:21,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57908, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742032_1208, duration: 72656656
2015-02-23 21:29:21,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742032_1208, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:22,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742033_1209 src: /127.0.0.1:57909 dest: /127.0.0.1:50010
2015-02-23 21:29:22,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57909, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742033_1209, duration: 10057350
2015-02-23 21:29:22,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742033_1209, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:22,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742034_1210 src: /127.0.0.1:57910 dest: /127.0.0.1:50010
2015-02-23 21:29:22,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57910, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742034_1210, duration: 3843855
2015-02-23 21:29:22,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742034_1210, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:22,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742035_1211 src: /127.0.0.1:57911 dest: /127.0.0.1:50010
2015-02-23 21:29:22,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57911, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-963075281_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742035_1211, duration: 13744279
2015-02-23 21:29:22,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742035_1211, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:29:33,931 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742025_1201
2015-02-23 21:29:34,130 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742034_1210
2015-02-23 21:29:50,930 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742027_1203
2015-02-23 21:29:50,931 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742030_1206
2015-02-23 21:29:51,531 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742035_1211
2015-02-23 21:29:51,731 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742033_1209
2015-02-23 21:29:51,731 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742031_1207
2015-02-23 21:29:55,730 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742032_1208
2015-02-23 21:29:55,930 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742029_1205
2015-02-23 21:29:56,330 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742026_1202
2015-02-23 21:29:56,530 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742028_1204
2015-02-23 21:31:07,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742036_1212 src: /127.0.0.1:57916 dest: /127.0.0.1:50010
2015-02-23 21:31:07,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57916, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742036_1212, duration: 157437762
2015-02-23 21:31:07,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742036_1212, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:08,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742037_1213 src: /127.0.0.1:57917 dest: /127.0.0.1:50010
2015-02-23 21:31:08,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57917, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742037_1213, duration: 9591587
2015-02-23 21:31:08,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742037_1213, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:08,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742038_1214 src: /127.0.0.1:57918 dest: /127.0.0.1:50010
2015-02-23 21:31:08,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57918, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742038_1214, duration: 335787954
2015-02-23 21:31:08,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742039_1215 src: /127.0.0.1:57919 dest: /127.0.0.1:50010
2015-02-23 21:31:09,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57919, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742039_1215, duration: 13665142
2015-02-23 21:31:09,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742039_1215, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742040_1216 src: /127.0.0.1:57920 dest: /127.0.0.1:50010
2015-02-23 21:31:09,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57920, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742040_1216, duration: 6756523
2015-02-23 21:31:09,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742040_1216, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742041_1217 src: /127.0.0.1:57921 dest: /127.0.0.1:50010
2015-02-23 21:31:09,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57921, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742041_1217, duration: 18097760
2015-02-23 21:31:09,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742041_1217, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742042_1218 src: /127.0.0.1:57922 dest: /127.0.0.1:50010
2015-02-23 21:31:09,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57922, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742042_1218, duration: 3360762
2015-02-23 21:31:09,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742042_1218, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742043_1219 src: /127.0.0.1:57923 dest: /127.0.0.1:50010
2015-02-23 21:31:09,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57923, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742043_1219, duration: 95749115
2015-02-23 21:31:09,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742043_1219, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742044_1220 src: /127.0.0.1:57924 dest: /127.0.0.1:50010
2015-02-23 21:31:09,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57924, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742044_1220, duration: 11138841
2015-02-23 21:31:09,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742044_1220, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742045_1221 src: /127.0.0.1:57925 dest: /127.0.0.1:50010
2015-02-23 21:31:09,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57925, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742045_1221, duration: 7515479
2015-02-23 21:31:09,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742045_1221, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:09,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742046_1222 src: /127.0.0.1:57926 dest: /127.0.0.1:50010
2015-02-23 21:31:09,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57926, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1425428707_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742046_1222, duration: 8245324
2015-02-23 21:31:09,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742046_1222, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:31:21,364 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742036_1212
2015-02-23 21:31:21,365 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742041_1217
2015-02-23 21:31:38,164 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742038_1214
2015-02-23 21:31:38,165 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742042_1218
2015-02-23 21:31:38,365 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742044_1220
2015-02-23 21:31:42,364 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742043_1219
2015-02-23 21:31:42,565 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742040_1216
2015-02-23 21:31:42,764 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742045_1221
2015-02-23 21:31:42,965 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742037_1213
2015-02-23 21:31:43,364 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742039_1215
2015-02-23 21:31:43,765 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742046_1222
2015-02-23 21:37:45,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742047_1223 src: /127.0.0.1:58096 dest: /127.0.0.1:50010
2015-02-23 21:37:45,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58096, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742047_1223, duration: 133017823
2015-02-23 21:37:45,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742047_1223, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:45,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742048_1224 src: /127.0.0.1:58097 dest: /127.0.0.1:50010
2015-02-23 21:37:45,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58097, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742048_1224, duration: 21321773
2015-02-23 21:37:45,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742048_1224, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:45,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742049_1225 src: /127.0.0.1:58098 dest: /127.0.0.1:50010
2015-02-23 21:37:46,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58098, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742049_1225, duration: 429822049
2015-02-23 21:37:46,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742049_1225, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:46,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742050_1226 src: /127.0.0.1:58099 dest: /127.0.0.1:50010
2015-02-23 21:37:46,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58099, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742050_1226, duration: 8517700
2015-02-23 21:37:46,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742050_1226, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742051_1227 src: /127.0.0.1:58100 dest: /127.0.0.1:50010
2015-02-23 21:37:47,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58100, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742051_1227, duration: 9771359
2015-02-23 21:37:47,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742051_1227, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742052_1228 src: /127.0.0.1:58101 dest: /127.0.0.1:50010
2015-02-23 21:37:47,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58101, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742052_1228, duration: 2778633
2015-02-23 21:37:47,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742052_1228, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742053_1229 src: /127.0.0.1:58102 dest: /127.0.0.1:50010
2015-02-23 21:37:47,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58102, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742053_1229, duration: 6674311
2015-02-23 21:37:47,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742053_1229, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742054_1230 src: /127.0.0.1:58103 dest: /127.0.0.1:50010
2015-02-23 21:37:47,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58103, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742054_1230, duration: 89549239
2015-02-23 21:37:47,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742054_1230, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742055_1231 src: /127.0.0.1:58104 dest: /127.0.0.1:50010
2015-02-23 21:37:47,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58104, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742055_1231, duration: 12838224
2015-02-23 21:37:47,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742055_1231, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742056_1232 src: /127.0.0.1:58105 dest: /127.0.0.1:50010
2015-02-23 21:37:47,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58105, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742056_1232, duration: 3949106
2015-02-23 21:37:47,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742056_1232, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:47,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742057_1233 src: /127.0.0.1:58106 dest: /127.0.0.1:50010
2015-02-23 21:37:47,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58106, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_136671150_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742057_1233, duration: 11363666
2015-02-23 21:37:47,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742057_1233, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:37:53,827 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742056_1232
2015-02-23 21:37:54,027 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742052_1228
2015-02-23 21:37:54,028 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742055_1231
2015-02-23 21:37:59,027 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742047_1223
2015-02-23 21:38:03,027 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742054_1230
2015-02-23 21:38:03,227 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742051_1227
2015-02-23 21:38:03,427 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742050_1226
2015-02-23 21:38:20,227 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742049_1225
2015-02-23 21:38:20,828 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742057_1233
2015-02-23 21:38:21,027 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742048_1224
2015-02-23 21:38:21,227 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742053_1229
2015-02-23 21:41:22,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742058_1234 src: /127.0.0.1:58168 dest: /127.0.0.1:50010
2015-02-23 21:41:22,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58168, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742058_1234, duration: 178149401
2015-02-23 21:41:22,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742058_1234, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:22,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742059_1235 src: /127.0.0.1:58169 dest: /127.0.0.1:50010
2015-02-23 21:41:22,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58169, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742059_1235, duration: 13418534
2015-02-23 21:41:22,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742059_1235, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:22,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742060_1236 src: /127.0.0.1:58170 dest: /127.0.0.1:50010
2015-02-23 21:41:22,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58170, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742060_1236, duration: 452334548
2015-02-23 21:41:22,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742060_1236, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742061_1237 src: /127.0.0.1:58171 dest: /127.0.0.1:50010
2015-02-23 21:41:23,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58171, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742061_1237, duration: 18320277
2015-02-23 21:41:23,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742061_1237, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742062_1238 src: /127.0.0.1:58172 dest: /127.0.0.1:50010
2015-02-23 21:41:23,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58172, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742062_1238, duration: 12942146
2015-02-23 21:41:23,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742062_1238, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742063_1239 src: /127.0.0.1:58173 dest: /127.0.0.1:50010
2015-02-23 21:41:23,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58173, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742063_1239, duration: 4161285
2015-02-23 21:41:23,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742063_1239, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742064_1240 src: /127.0.0.1:58174 dest: /127.0.0.1:50010
2015-02-23 21:41:23,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58174, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742064_1240, duration: 3153053
2015-02-23 21:41:23,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742064_1240, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742065_1241 src: /127.0.0.1:58175 dest: /127.0.0.1:50010
2015-02-23 21:41:23,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58175, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742065_1241, duration: 229620820
2015-02-23 21:41:23,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742065_1241, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742066_1242 src: /127.0.0.1:58176 dest: /127.0.0.1:50010
2015-02-23 21:41:23,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58176, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742066_1242, duration: 13752376
2015-02-23 21:41:23,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742066_1242, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:23,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742067_1243 src: /127.0.0.1:58177 dest: /127.0.0.1:50010
2015-02-23 21:41:24,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58177, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742067_1243, duration: 7028269
2015-02-23 21:41:24,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742067_1243, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:24,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742068_1244 src: /127.0.0.1:58178 dest: /127.0.0.1:50010
2015-02-23 21:41:24,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58178, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742068_1244, duration: 10516842
2015-02-23 21:41:24,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742068_1244, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:29,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742069_1245 src: /127.0.0.1:58183 dest: /127.0.0.1:50010
2015-02-23 21:41:30,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58183, dest: /127.0.0.1:50010, bytes: 1500000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-691765503_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742069_1245, duration: 359980554
2015-02-23 21:41:30,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742069_1245, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:41:47,717 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742060_1236
2015-02-23 21:41:47,916 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742066_1242
2015-02-23 21:41:48,116 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742067_1243
2015-02-23 21:41:48,315 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742063_1239
2015-02-23 21:41:52,116 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742065_1241
2015-02-23 21:41:52,316 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742062_1238
2015-02-23 21:41:52,716 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742059_1235
2015-02-23 21:41:53,316 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742068_1244
2015-02-23 21:41:53,515 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742061_1237
2015-02-23 21:41:53,516 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742064_1240
2015-02-23 21:41:58,515 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742058_1234
2015-02-23 21:42:04,735 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742069_1245
2015-02-23 21:45:47,192 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-55626325-127.0.0.1-1424690419215 Total blocks: 244, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-02-23 21:54:15,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742070_1246 src: /127.0.0.1:58336 dest: /127.0.0.1:50010
2015-02-23 21:54:15,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58336, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742070_1246, duration: 129315323
2015-02-23 21:54:15,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742070_1246, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:15,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742071_1247 src: /127.0.0.1:58337 dest: /127.0.0.1:50010
2015-02-23 21:54:15,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58337, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742071_1247, duration: 26574605
2015-02-23 21:54:15,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742071_1247, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:15,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742072_1248 src: /127.0.0.1:58338 dest: /127.0.0.1:50010
2015-02-23 21:54:16,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58338, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742072_1248, duration: 443322925
2015-02-23 21:54:16,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742072_1248, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:16,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742073_1249 src: /127.0.0.1:58339 dest: /127.0.0.1:50010
2015-02-23 21:54:16,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58339, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742073_1249, duration: 13630501
2015-02-23 21:54:16,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742073_1249, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:16,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742074_1250 src: /127.0.0.1:58340 dest: /127.0.0.1:50010
2015-02-23 21:54:16,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58340, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742074_1250, duration: 7804831
2015-02-23 21:54:16,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742074_1250, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:16,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742075_1251 src: /127.0.0.1:58341 dest: /127.0.0.1:50010
2015-02-23 21:54:16,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58341, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742075_1251, duration: 3804463
2015-02-23 21:54:16,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742075_1251, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:16,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742076_1252 src: /127.0.0.1:58342 dest: /127.0.0.1:50010
2015-02-23 21:54:16,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58342, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742076_1252, duration: 6679341
2015-02-23 21:54:16,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742076_1252, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:16,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742077_1253 src: /127.0.0.1:58343 dest: /127.0.0.1:50010
2015-02-23 21:54:16,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58343, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742077_1253, duration: 74424965
2015-02-23 21:54:16,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742077_1253, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:17,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742078_1254 src: /127.0.0.1:58344 dest: /127.0.0.1:50010
2015-02-23 21:54:17,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58344, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742078_1254, duration: 5052044
2015-02-23 21:54:17,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742078_1254, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:17,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742079_1255 src: /127.0.0.1:58345 dest: /127.0.0.1:50010
2015-02-23 21:54:17,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58345, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742079_1255, duration: 5564816
2015-02-23 21:54:17,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742079_1255, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:17,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742080_1256 src: /127.0.0.1:58346 dest: /127.0.0.1:50010
2015-02-23 21:54:17,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58346, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742080_1256, duration: 11423099
2015-02-23 21:54:17,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742080_1256, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:22,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742081_1257 src: /127.0.0.1:58353 dest: /127.0.0.1:50010
2015-02-23 21:54:23,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58353, dest: /127.0.0.1:50010, bytes: 1600000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_589470184_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742081_1257, duration: 985876697
2015-02-23 21:54:23,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742081_1257, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 21:54:29,612 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742070_1246
2015-02-23 21:54:29,812 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742078_1254
2015-02-23 21:54:30,013 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742073_1249
2015-02-23 21:54:46,812 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742072_1248
2015-02-23 21:54:46,813 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742076_1252
2015-02-23 21:54:50,812 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742077_1253
2015-02-23 21:54:51,012 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742079_1255
2015-02-23 21:54:51,212 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742074_1250
2015-02-23 21:54:51,613 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742080_1256
2015-02-23 21:54:52,013 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742071_1247
2015-02-23 21:54:52,212 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742075_1251
2015-02-23 21:54:58,438 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742081_1257
2015-02-23 22:44:54,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742082_1258 src: /127.0.0.1:58678 dest: /127.0.0.1:50010
2015-02-23 22:44:54,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58678, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742082_1258, duration: 127285739
2015-02-23 22:44:54,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742082_1258, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:54,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742083_1259 src: /127.0.0.1:58679 dest: /127.0.0.1:50010
2015-02-23 22:44:54,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58679, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742083_1259, duration: 13619332
2015-02-23 22:44:54,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742083_1259, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:54,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742084_1260 src: /127.0.0.1:58680 dest: /127.0.0.1:50010
2015-02-23 22:44:55,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58680, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742084_1260, duration: 505730234
2015-02-23 22:44:55,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742084_1260, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:55,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742085_1261 src: /127.0.0.1:58681 dest: /127.0.0.1:50010
2015-02-23 22:44:55,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58681, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742085_1261, duration: 14029295
2015-02-23 22:44:55,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742085_1261, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:55,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742086_1262 src: /127.0.0.1:58682 dest: /127.0.0.1:50010
2015-02-23 22:44:55,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58682, dest: /127.0.0.1:50010, bytes: 201124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742086_1262, duration: 11507257
2015-02-23 22:44:55,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742086_1262, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:56,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742087_1263 src: /127.0.0.1:58683 dest: /127.0.0.1:50010
2015-02-23 22:44:56,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58683, dest: /127.0.0.1:50010, bytes: 107720, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742087_1263, duration: 3488854
2015-02-23 22:44:56,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742087_1263, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:56,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742088_1264 src: /127.0.0.1:58684 dest: /127.0.0.1:50010
2015-02-23 22:44:56,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58684, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742088_1264, duration: 3164440
2015-02-23 22:44:56,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742088_1264, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:56,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742089_1265 src: /127.0.0.1:58685 dest: /127.0.0.1:50010
2015-02-23 22:44:56,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58685, dest: /127.0.0.1:50010, bytes: 4104392, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742089_1265, duration: 95589603
2015-02-23 22:44:56,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742089_1265, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:56,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742090_1266 src: /127.0.0.1:58686 dest: /127.0.0.1:50010
2015-02-23 22:44:56,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58686, dest: /127.0.0.1:50010, bytes: 176285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742090_1266, duration: 3988564
2015-02-23 22:44:56,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742090_1266, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:56,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742091_1267 src: /127.0.0.1:58687 dest: /127.0.0.1:50010
2015-02-23 22:44:56,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58687, dest: /127.0.0.1:50010, bytes: 164368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742091_1267, duration: 12228862
2015-02-23 22:44:56,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742091_1267, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:44:56,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742092_1268 src: /127.0.0.1:58688 dest: /127.0.0.1:50010
2015-02-23 22:44:56,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58688, dest: /127.0.0.1:50010, bytes: 570478, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742092_1268, duration: 16249552
2015-02-23 22:44:56,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742092_1268, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:45:01,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-55626325-127.0.0.1-1424690419215:blk_1073742093_1269 src: /127.0.0.1:58694 dest: /127.0.0.1:50010
2015-02-23 22:45:02,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58694, dest: /127.0.0.1:50010, bytes: 1600000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1896555072_1, offset: 0, srvID: 4ed15a45-c6a4-47d9-96c9-67e1a61ded84, blockid: BP-55626325-127.0.0.1-1424690419215:blk_1073742093_1269, duration: 807884579
2015-02-23 22:45:02,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-55626325-127.0.0.1-1424690419215:blk_1073742093_1269, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-23 22:45:03,599 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742087_1263
2015-02-23 22:45:07,599 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742089_1265
2015-02-23 22:45:24,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742084_1260
2015-02-23 22:45:24,400 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742090_1266
2015-02-23 22:45:25,000 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742092_1268
2015-02-23 22:45:25,200 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742085_1261
2015-02-23 22:45:25,599 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742083_1259
2015-02-23 22:45:25,799 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742086_1262
2015-02-23 22:45:25,800 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742088_1264
2015-02-23 22:45:30,800 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742082_1258
2015-02-23 22:45:31,000 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742091_1267
2015-02-23 22:45:37,221 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-55626325-127.0.0.1-1424690419215:blk_1073742093_1269
2015-02-23 23:04:02,002 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "shrinivaasanka-Inspiron-1545/127.0.0.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-02-23 23:04:05,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-23 23:04:06,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-23 23:04:06,960 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-23 23:04:06,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 10:28:32,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 10:28:32,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 10:28:33,725 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 10:28:34,416 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 10:28:34,652 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 10:28:34,652 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 10:28:34,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 10:28:34,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 10:28:34,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 10:28:34,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 10:28:34,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 10:28:34,856 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 10:28:34,863 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 10:28:34,879 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 10:28:34,883 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 10:28:34,883 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 10:28:34,883 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 10:28:34,907 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 10:28:34,913 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 10:28:34,913 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 10:28:35,457 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 10:28:35,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 10:28:35,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 10:28:35,653 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 10:28:35,679 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 10:28:35,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 10:28:35,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 10:28:36,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 10:28:36,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 10:28:36,154 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 10:28:36,196 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 10:28:37,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:38,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:39,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:40,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:41,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:42,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:43,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:44,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:45,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:46,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:46,531 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-24 10:28:52,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:53,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:54,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:55,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:56,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:57,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:58,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:28:59,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:00,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:01,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:01,542 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-24 10:29:07,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:08,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:09,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:10,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:11,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:12,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:13,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:14,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:15,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:16,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 10:29:16,552 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-02-24 10:29:17,070 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-24 10:29:17,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 10:30:01,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 10:30:01,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 10:30:02,008 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 10:30:02,634 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 10:30:02,796 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 10:30:02,796 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 10:30:02,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 10:30:02,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 10:30:02,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 10:30:02,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 10:30:02,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 10:30:03,050 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 10:30:03,057 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 10:30:03,077 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 10:30:03,081 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 10:30:03,081 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 10:30:03,081 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 10:30:03,106 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 10:30:03,110 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 10:30:03,111 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 10:30:03,575 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 10:30:03,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 10:30:03,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 10:30:03,672 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 10:30:03,727 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 10:30:03,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 10:30:03,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 10:30:03,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 10:30:04,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 10:30:04,027 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 10:30:04,037 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 10:30:04,919 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 10:30:04,980 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 7242@shrinivaasanka-Inspiron-1545
2015-02-24 10:30:04,982 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-762311704-127.0.0.1-1424753988129
2015-02-24 10:30:04,982 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-24 10:30:05,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-762311704-127.0.0.1-1424753988129
2015-02-24 10:30:05,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-24 10:30:05,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-762311704-127.0.0.1-1424753988129 is not formatted.
2015-02-24 10:30:05,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-24 10:30:05,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-762311704-127.0.0.1-1424753988129 directory /tmp/hadoop-root/dfs/data/current/BP-762311704-127.0.0.1-1424753988129/current
2015-02-24 10:30:05,672 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-24 10:30:05,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=495027567;bpid=BP-762311704-127.0.0.1-1424753988129;lv=-56;nsInfo=lv=-60;cid=CID-975390f8-7f1e-4999-bafb-d1e845945f41;nsid=495027567;c=0;bpid=BP-762311704-127.0.0.1-1424753988129;dnuuid=null
2015-02-24 10:30:05,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 2afc0a9f-94d4-407b-a341-a253ffd8deb5
2015-02-24 10:30:05,910 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-24 10:30:05,924 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-24 10:30:05,957 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-24 10:30:05,979 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424762862971 with interval 21600000
2015-02-24 10:30:05,980 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-762311704-127.0.0.1-1424753988129
2015-02-24 10:30:05,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-762311704-127.0.0.1-1424753988129 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-24 10:30:06,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-762311704-127.0.0.1-1424753988129 on /tmp/hadoop-root/dfs/data/current: 58ms
2015-02-24 10:30:06,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-762311704-127.0.0.1-1424753988129: 70ms
2015-02-24 10:30:06,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-762311704-127.0.0.1-1424753988129 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-24 10:30:06,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-762311704-127.0.0.1-1424753988129 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-24 10:30:06,059 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2015-02-24 10:30:06,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-762311704-127.0.0.1-1424753988129 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-24 10:30:06,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-762311704-127.0.0.1-1424753988129 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-24 10:30:06,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-24 10:30:06,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-762311704-127.0.0.1-1424753988129 (Datanode Uuid 2afc0a9f-94d4-407b-a341-a253ffd8deb5) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-24 10:30:06,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-762311704-127.0.0.1-1424753988129 (Datanode Uuid 2afc0a9f-94d4-407b-a341-a253ffd8deb5) service to localhost/127.0.0.1:9000
2015-02-24 10:30:06,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 10 msec to generate and 145 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@18786da
2015-02-24 10:30:06,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-762311704-127.0.0.1-1424753988129
2015-02-24 10:30:06,807 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-24 10:30:06,807 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-24 10:30:06,809 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-24 10:30:06,809 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-24 10:30:06,811 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-762311704-127.0.0.1-1424753988129
2015-02-24 10:30:06,818 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-762311704-127.0.0.1-1424753988129 to blockPoolScannerMap, new size=1
2015-02-24 11:01:02,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-762311704-127.0.0.1-1424753988129:blk_1073741825_1001 src: /127.0.0.1:54417 dest: /127.0.0.1:50010
2015-02-24 11:01:02,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54417, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_348089801_1, offset: 0, srvID: 2afc0a9f-94d4-407b-a341-a253ffd8deb5, blockid: BP-762311704-127.0.0.1-1424753988129:blk_1073741825_1001, duration: 167772102
2015-02-24 11:01:02,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-762311704-127.0.0.1-1424753988129:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-24 11:01:10,164 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-762311704-127.0.0.1-1424753988129:blk_1073741825_1001
2015-02-24 11:01:48,407 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /tmp/hadoop-root/dfs/data/current/BP-762311704-127.0.0.1-1424753988129/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-02-24 11:01:48,411 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-762311704-127.0.0.1-1424753988129 blk_1073741825_1001 file /tmp/hadoop-root/dfs/data/current/BP-762311704-127.0.0.1-1424753988129/current/finalized/subdir0/subdir0/blk_1073741825
2015-02-24 11:02:16,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-762311704-127.0.0.1-1424753988129:blk_1073741826_1002 src: /127.0.0.1:54429 dest: /127.0.0.1:50010
2015-02-24 11:02:16,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54429, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1995999842_1, offset: 0, srvID: 2afc0a9f-94d4-407b-a341-a253ffd8deb5, blockid: BP-762311704-127.0.0.1-1424753988129:blk_1073741826_1002, duration: 103578537
2015-02-24 11:02:16,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-762311704-127.0.0.1-1424753988129:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-24 11:02:25,447 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-762311704-127.0.0.1-1424753988129:blk_1073741826_1002
2015-02-24 12:57:42,109 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-762311704-127.0.0.1-1424753988129 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-02-24 14:19:19,030 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "shrinivaasanka-Inspiron-1545/127.0.0.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-02-24 14:19:23,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-02-24 14:19:23,482 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-24 14:19:23,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 14:19:56,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 14:19:56,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 14:19:57,639 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 14:19:58,658 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 14:19:58,858 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 14:19:58,858 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 14:19:58,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 14:19:58,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 14:19:58,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 14:19:58,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 14:19:58,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 14:19:59,388 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 14:19:59,394 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 14:19:59,411 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 14:19:59,414 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 14:19:59,414 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 14:19:59,414 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 14:19:59,440 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 14:19:59,446 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 14:19:59,446 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 14:20:00,357 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 14:20:00,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 14:20:00,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 14:20:00,870 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 14:20:00,917 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 14:20:01,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 14:20:01,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 14:20:01,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 14:20:01,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 14:20:01,269 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 14:20:01,298 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 14:20:02,397 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 14:20:02,446 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 15777@shrinivaasanka-Inspiron-1545
2015-02-24 14:20:02,466 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-root/dfs/data: namenode clusterID = CID-1b61dd26-892c-49af-a866-e787fd49888e; datanode clusterID = CID-975390f8-7f1e-4999-bafb-d1e845945f41
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-02-24 14:20:02,478 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-02-24 14:20:02,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-02-24 14:20:04,587 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-02-24 14:20:04,589 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-02-24 14:20:04,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 14:23:32,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 14:23:32,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 14:23:33,799 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 14:23:34,302 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 14:23:34,533 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 14:23:34,533 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 14:23:34,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 14:23:34,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 14:23:34,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 14:23:34,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 14:23:34,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 14:23:35,044 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 14:23:35,050 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 14:23:35,067 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 14:23:35,070 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 14:23:35,070 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 14:23:35,070 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 14:23:35,095 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 14:23:35,101 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 14:23:35,101 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 14:23:35,714 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 14:23:35,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 14:23:35,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 14:23:36,135 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 14:23:36,177 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 14:23:36,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 14:23:36,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 14:23:36,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 14:23:36,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 14:23:36,393 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 14:23:36,406 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 14:23:37,091 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 14:23:37,146 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 17398@shrinivaasanka-Inspiron-1545
2015-02-24 14:23:37,150 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-root/dfs/data: namenode clusterID = CID-b56c2e4c-25ab-4591-889d-ef8b803f39f1; datanode clusterID = CID-975390f8-7f1e-4999-bafb-d1e845945f41
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-02-24 14:23:37,197 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-02-24 14:23:37,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-02-24 14:23:39,202 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-02-24 14:23:39,204 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-02-24 14:23:39,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 14:30:55,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 14:30:55,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 14:30:56,513 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 14:30:57,279 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 14:30:57,452 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 14:30:57,452 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 14:30:57,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 14:30:57,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 14:30:57,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 14:30:57,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 14:30:57,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 14:30:57,905 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 14:30:57,911 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 14:30:57,927 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 14:30:57,930 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 14:30:57,930 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 14:30:57,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 14:30:57,956 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 14:30:57,962 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 14:30:57,962 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 14:30:58,455 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 14:30:58,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 14:30:58,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 14:30:58,826 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 14:30:58,865 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 14:30:58,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 14:30:59,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 14:30:59,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 14:30:59,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 14:30:59,080 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 14:30:59,121 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 14:30:59,919 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 14:30:59,988 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 19218@shrinivaasanka-Inspiron-1545
2015-02-24 14:30:59,992 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-root/dfs/data: namenode clusterID = CID-f3b9f4ac-c883-45fe-9424-5b87f022e401; datanode clusterID = CID-975390f8-7f1e-4999-bafb-d1e845945f41
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-02-24 14:31:00,005 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-02-24 14:31:00,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-02-24 14:31:02,008 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-02-24 14:31:02,010 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-02-24 14:31:02,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 14:37:58,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 14:37:58,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 14:37:59,633 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 14:38:00,145 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 14:38:00,332 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 14:38:00,332 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 14:38:00,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 14:38:00,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 14:38:00,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 14:38:00,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 14:38:00,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 14:38:01,515 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 14:38:01,523 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 14:38:01,542 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 14:38:01,545 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 14:38:01,545 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 14:38:01,546 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 14:38:01,594 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 14:38:01,601 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 14:38:01,601 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 14:38:02,593 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 14:38:02,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 14:38:02,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 14:38:03,123 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 14:38:03,169 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 14:38:03,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 14:38:03,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 14:38:03,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 14:38:03,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 14:38:03,502 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 14:38:03,505 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 14:38:04,682 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 14:38:04,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 20916@shrinivaasanka-Inspiron-1545
2015-02-24 14:38:04,747 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /tmp/hadoop-root/dfs/data: namenode clusterID = CID-f3e868ae-34b6-46b2-99b3-a9bbc3a2a91a; datanode clusterID = CID-975390f8-7f1e-4999-bafb-d1e845945f41
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-02-24 14:38:04,779 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-02-24 14:38:04,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-02-24 14:38:06,782 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-02-24 14:38:06,784 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-02-24 14:38:06,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 14:42:51,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 14:42:51,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 14:42:52,752 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 14:42:53,416 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 14:42:53,608 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 14:42:53,608 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 14:42:53,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 14:42:53,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 14:42:53,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 14:42:53,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 14:42:53,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 14:42:53,797 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 14:42:53,804 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 14:42:53,824 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 14:42:53,828 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 14:42:53,828 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 14:42:53,828 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 14:42:53,856 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 14:42:53,861 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 14:42:53,861 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 14:42:54,371 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 14:42:54,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 14:42:54,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 14:42:54,464 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 14:42:54,494 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 14:42:54,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 14:42:54,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 14:42:54,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 14:42:54,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 14:42:54,666 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 14:42:54,668 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 14:42:55,388 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 14:42:55,449 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 22503@shrinivaasanka-Inspiron-1545
2015-02-24 14:42:55,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-254673872-127.0.0.1-1424769152623
2015-02-24 14:42:55,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-24 14:42:55,790 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-254673872-127.0.0.1-1424769152623
2015-02-24 14:42:55,790 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-24 14:42:55,791 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-254673872-127.0.0.1-1424769152623 is not formatted.
2015-02-24 14:42:55,791 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-24 14:42:55,791 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-254673872-127.0.0.1-1424769152623 directory /tmp/hadoop-root/dfs/data/current/BP-254673872-127.0.0.1-1424769152623/current
2015-02-24 14:42:55,818 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-24 14:42:55,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1994899122;bpid=BP-254673872-127.0.0.1-1424769152623;lv=-56;nsInfo=lv=-60;cid=CID-cfc07245-a25f-489b-b551-3d0148b01ecf;nsid=1994899122;c=0;bpid=BP-254673872-127.0.0.1-1424769152623;dnuuid=null
2015-02-24 14:42:55,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 27fe5016-c358-4664-a06e-700ef0a890d8
2015-02-24 14:42:56,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-24 14:42:56,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-24 14:42:56,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-24 14:42:56,108 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424790368108 with interval 21600000
2015-02-24 14:42:56,117 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-254673872-127.0.0.1-1424769152623
2015-02-24 14:42:56,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-254673872-127.0.0.1-1424769152623 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-24 14:42:56,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-254673872-127.0.0.1-1424769152623 on /tmp/hadoop-root/dfs/data/current: 132ms
2015-02-24 14:42:56,252 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-254673872-127.0.0.1-1424769152623: 135ms
2015-02-24 14:42:56,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-254673872-127.0.0.1-1424769152623 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-24 14:42:56,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-254673872-127.0.0.1-1424769152623 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-24 14:42:56,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 16ms
2015-02-24 14:42:56,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-254673872-127.0.0.1-1424769152623 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-24 14:42:56,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-254673872-127.0.0.1-1424769152623 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-24 14:42:56,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-24 14:42:56,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-254673872-127.0.0.1-1424769152623 (Datanode Uuid 27fe5016-c358-4664-a06e-700ef0a890d8) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-24 14:42:56,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-254673872-127.0.0.1-1424769152623 (Datanode Uuid 27fe5016-c358-4664-a06e-700ef0a890d8) service to localhost/127.0.0.1:9000
2015-02-24 14:42:56,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 219 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@1ba2255
2015-02-24 14:42:56,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-254673872-127.0.0.1-1424769152623
2015-02-24 14:42:56,994 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-24 14:42:56,994 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-24 14:42:56,996 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-24 14:42:56,996 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-24 14:42:57,005 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-254673872-127.0.0.1-1424769152623
2015-02-24 14:42:57,013 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-254673872-127.0.0.1-1424769152623 to blockPoolScannerMap, new size=1
2015-02-24 16:31:33,490 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-02-24 16:31:33,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at shrinivaasanka-Inspiron-1545/127.0.0.1
************************************************************/
2015-02-24 17:23:39,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = shrinivaasanka-Inspiron-1545/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_75
************************************************************/
2015-02-24 17:23:39,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-02-24 17:23:40,636 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-24 17:23:41,250 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-02-24 17:23:41,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-02-24 17:23:41,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-02-24 17:23:41,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-02-24 17:23:41,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-02-24 17:23:41,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-02-24 17:23:41,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-02-24 17:23:41,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-02-24 17:23:41,874 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-02-24 17:23:41,880 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-02-24 17:23:41,896 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-02-24 17:23:41,899 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-02-24 17:23:41,899 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-02-24 17:23:41,900 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-02-24 17:23:41,927 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-02-24 17:23:41,932 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-02-24 17:23:41,932 INFO org.mortbay.log: jetty-6.1.26
2015-02-24 17:23:42,641 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-02-24 17:23:42,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = root
2015-02-24 17:23:42,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-02-24 17:23:43,178 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-02-24 17:23:43,243 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-02-24 17:23:43,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-02-24 17:23:43,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-02-24 17:23:43,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-02-24 17:23:43,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-02-24 17:23:43,553 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-02-24 17:23:43,555 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-02-24 17:23:44,553 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-02-24 17:23:44,616 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/data/in_use.lock acquired by nodename 4458@shrinivaasanka-Inspiron-1545
2015-02-24 17:23:44,618 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data is not formatted for BP-1515673266-127.0.0.1-1424778785922
2015-02-24 17:23:44,618 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-24 17:23:45,058 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1515673266-127.0.0.1-1424778785922
2015-02-24 17:23:45,058 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-02-24 17:23:45,059 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-root/dfs/data/current/BP-1515673266-127.0.0.1-1424778785922 is not formatted.
2015-02-24 17:23:45,060 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-02-24 17:23:45,060 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1515673266-127.0.0.1-1424778785922 directory /tmp/hadoop-root/dfs/data/current/BP-1515673266-127.0.0.1-1424778785922/current
2015-02-24 17:23:45,108 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-02-24 17:23:45,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1286159446;bpid=BP-1515673266-127.0.0.1-1424778785922;lv=-56;nsInfo=lv=-60;cid=CID-9f622f53-f7d8-4532-816d-333a19329992;nsid=1286159446;c=0;bpid=BP-1515673266-127.0.0.1-1424778785922;dnuuid=null
2015-02-24 17:23:45,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID cfcc04c8-e116-4908-9eb1-6915ef829199
2015-02-24 17:23:45,450 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-root/dfs/data/current
2015-02-24 17:23:45,452 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-root/dfs/data/current, StorageType: DISK
2015-02-24 17:23:45,502 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-02-24 17:23:45,514 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1424786611514 with interval 21600000
2015-02-24 17:23:45,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1515673266-127.0.0.1-1424778785922
2015-02-24 17:23:45,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1515673266-127.0.0.1-1424778785922 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-24 17:23:45,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1515673266-127.0.0.1-1424778785922 on /tmp/hadoop-root/dfs/data/current: 68ms
2015-02-24 17:23:45,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1515673266-127.0.0.1-1424778785922: 79ms
2015-02-24 17:23:45,601 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1515673266-127.0.0.1-1424778785922 on volume /tmp/hadoop-root/dfs/data/current...
2015-02-24 17:23:45,601 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1515673266-127.0.0.1-1424778785922 on volume /tmp/hadoop-root/dfs/data/current: 0ms
2015-02-24 17:23:45,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2015-02-24 17:23:45,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1515673266-127.0.0.1-1424778785922 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-02-24 17:23:45,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1515673266-127.0.0.1-1424778785922 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-02-24 17:23:45,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-02-24 17:23:46,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1515673266-127.0.0.1-1424778785922 (Datanode Uuid cfcc04c8-e116-4908-9eb1-6915ef829199) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-02-24 17:23:46,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1515673266-127.0.0.1-1424778785922 (Datanode Uuid cfcc04c8-e116-4908-9eb1-6915ef829199) service to localhost/127.0.0.1:9000
2015-02-24 17:23:46,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 9 msec to generate and 326 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@ecdc51
2015-02-24 17:23:46,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1515673266-127.0.0.1-1424778785922
2015-02-24 17:23:46,628 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-02-24 17:23:46,628 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-02-24 17:23:46,630 INFO org.apache.hadoop.util.GSet: 0.5% max memory 889 MB = 4.4 MB
2015-02-24 17:23:46,630 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-02-24 17:23:46,632 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1515673266-127.0.0.1-1424778785922
2015-02-24 17:23:46,666 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1515673266-127.0.0.1-1424778785922 to blockPoolScannerMap, new size=1
2015-02-24 18:57:16,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@aa4867
2015-02-24 18:57:16,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1515673266-127.0.0.1-1424778785922
2015-02-24 19:33:31,523 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1515673266-127.0.0.1-1424778785922 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-02-25 07:17:09,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@910bc
2015-02-25 07:17:12,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1515673266-127.0.0.1-1424778785922
2015-02-25 07:17:12,509 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1421ms
No GCs detected
2015-02-25 09:47:16,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741825_1001 src: /127.0.0.1:57522 dest: /127.0.0.1:50010
2015-02-25 09:47:17,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57522, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1648481623_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741825_1001, duration: 372818490
2015-02-25 09:47:17,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 09:47:23,084 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741825_1001
2015-02-25 10:50:46,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741826_1002 src: /127.0.0.1:57826 dest: /127.0.0.1:50010
2015-02-25 10:50:46,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57826, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-315551431_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741826_1002, duration: 78228312
2015-02-25 10:50:46,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 10:50:53,554 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741826_1002
2015-02-25 11:19:23,005 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1515673266-127.0.0.1-1424778785922 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-02-25 12:57:17,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 2 blocks total. Took 19 msec to generate and 16 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@70d828
2015-02-25 12:57:17,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1515673266-127.0.0.1-1424778785922
2015-02-25 14:01:00,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741827_1003 src: /127.0.0.1:59961 dest: /127.0.0.1:50010
2015-02-25 14:01:01,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59961, dest: /127.0.0.1:50010, bytes: 3302431, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741827_1003, duration: 186413164
2015-02-25 14:01:01,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:01,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741828_1004 src: /127.0.0.1:59962 dest: /127.0.0.1:50010
2015-02-25 14:01:01,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59962, dest: /127.0.0.1:50010, bytes: 347531, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741828_1004, duration: 47066671
2015-02-25 14:01:01,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:01,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741829_1005 src: /127.0.0.1:59963 dest: /127.0.0.1:50010
2015-02-25 14:01:01,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59963, dest: /127.0.0.1:50010, bytes: 50815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741829_1005, duration: 38975485
2015-02-25 14:01:01,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:01,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741830_1006 src: /127.0.0.1:59964 dest: /127.0.0.1:50010
2015-02-25 14:01:01,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59964, dest: /127.0.0.1:50010, bytes: 5125725, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741830_1006, duration: 331568001
2015-02-25 14:01:01,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:02,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741831_1007 src: /127.0.0.1:59967 dest: /127.0.0.1:50010
2015-02-25 14:01:03,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59967, dest: /127.0.0.1:50010, bytes: 17429823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741831_1007, duration: 1295979328
2015-02-25 14:01:03,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:03,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741832_1008 src: /127.0.0.1:59968 dest: /127.0.0.1:50010
2015-02-25 14:01:03,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59968, dest: /127.0.0.1:50010, bytes: 275186, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741832_1008, duration: 10667832
2015-02-25 14:01:03,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:03,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741833_1009 src: /127.0.0.1:59969 dest: /127.0.0.1:50010
2015-02-25 14:01:03,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59969, dest: /127.0.0.1:50010, bytes: 4960, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741833_1009, duration: 2372297
2015-02-25 14:01:03,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741834_1010 src: /127.0.0.1:59970 dest: /127.0.0.1:50010
2015-02-25 14:01:04,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59970, dest: /127.0.0.1:50010, bytes: 253115, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741834_1010, duration: 56880761
2015-02-25 14:01:04,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741835_1011 src: /127.0.0.1:59971 dest: /127.0.0.1:50010
2015-02-25 14:01:04,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59971, dest: /127.0.0.1:50010, bytes: 32119, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741835_1011, duration: 3286172
2015-02-25 14:01:04,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741836_1012 src: /127.0.0.1:59972 dest: /127.0.0.1:50010
2015-02-25 14:01:04,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59972, dest: /127.0.0.1:50010, bytes: 339666, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741836_1012, duration: 10961026
2015-02-25 14:01:04,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741837_1013 src: /127.0.0.1:59973 dest: /127.0.0.1:50010
2015-02-25 14:01:04,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59973, dest: /127.0.0.1:50010, bytes: 1890075, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741837_1013, duration: 50534159
2015-02-25 14:01:04,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741838_1014 src: /127.0.0.1:59974 dest: /127.0.0.1:50010
2015-02-25 14:01:04,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59974, dest: /127.0.0.1:50010, bytes: 1809447, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741838_1014, duration: 96633097
2015-02-25 14:01:04,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741839_1015 src: /127.0.0.1:59975 dest: /127.0.0.1:50010
2015-02-25 14:01:04,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59975, dest: /127.0.0.1:50010, bytes: 583719, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741839_1015, duration: 16255140
2015-02-25 14:01:04,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:04,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741840_1016 src: /127.0.0.1:59976 dest: /127.0.0.1:50010
2015-02-25 14:01:05,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59976, dest: /127.0.0.1:50010, bytes: 3103132, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1525501468_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741840_1016, duration: 161953482
2015-02-25 14:01:05,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:01:09,152 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741833_1009
2015-02-25 14:01:25,754 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741831_1007
2015-02-25 14:01:25,956 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741834_1010
2015-02-25 14:01:30,955 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741830_1006
2015-02-25 14:01:34,154 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741827_1003
2015-02-25 14:01:34,355 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741832_1008
2015-02-25 14:01:34,755 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741828_1004
2015-02-25 14:01:34,756 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741829_1005
2015-02-25 14:01:42,622 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741840_1016
2015-02-25 14:01:42,624 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741835_1011
2015-02-25 14:01:43,222 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741839_1015
2015-02-25 14:01:45,022 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741837_1013
2015-02-25 14:01:46,821 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741838_1014
2015-02-25 14:01:47,022 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741836_1012
2015-02-25 14:07:05,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1515673266-127.0.0.1-1424778785922:blk_1073741841_1017 src: /127.0.0.1:60015 dest: /127.0.0.1:50010
2015-02-25 14:07:06,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60015, dest: /127.0.0.1:50010, bytes: 1000000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1302350942_1, offset: 0, srvID: cfcc04c8-e116-4908-9eb1-6915ef829199, blockid: BP-1515673266-127.0.0.1-1424778785922:blk_1073741841_1017, duration: 930671913
2015-02-25 14:07:06,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1515673266-127.0.0.1-1424778785922:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-02-25 14:07:12,879 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1515673266-127.0.0.1-1424778785922:blk_1073741841_1017
