========================================================================================
Process id: 1
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2930, involuntary=5795), 1, 0.06552748946440834, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021507132257681392, 0.026487221456976427, 0.021376077278752577]
###############
Output Layer:
###############
[0.0339522957146387, 0.3113594360267407, 0.04212280834437972]
Error before Backpropagation:
0.0610708031581
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0599462312274
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021454799393089186, 0.02614511488036135, 0.021311699803515726]
###############
Output Layer:
###############
[0.03382606425725502, 0.307521700635627, 0.042018485990912985]
Weights updated in this iteration:
[0.009351519260508642, 0.022201360146406463, 0.05535547081614291, 0.038760799117019115, 0.09377918993370112, 0.08378663093763275, 0.03220226891698958, 0.020017550103581846, 0.11920712993839504, 0.23003953739503905, 0.34004869248608716, 0.4500392964715904, -0.0013897028020126634, 10.998288504230352, 0.5586187654339778, 0.770145680475924, 0.21017941355367722, 0.8801447927633516, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0588900637105
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.02140485506218302, 0.02581968206132294, 0.02125018538247357]
###############
Output Layer:
###############
[0.03370599449944742, 0.303870699466276, 0.04191929212164647]
Weights updated in this iteration:
[0.008726063533111362, 0.021439171158023324, 0.0547341867641012, 0.0346853852140271, 0.08881283426071031, 0.07973839930030424, 0.031431920285617816, 0.019078792621112377, 0.11844191939155896, 0.23007892559175613, 0.34009669148664784, 0.4500784219562364, -0.0027490332404115896, 10.99663200538265, 0.5572685014810894, 0.7702907524356708, 0.21035620024760088, 0.880288897119649, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0578960697663
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021357112862900974, 0.025509608505430718, 0.021191314299400868]
###############
Output Layer:
###############
[0.033591600835826754, 0.300391704213912, 0.04182482299681475]
Weights updated in this iteration:
[0.008122058867551131, 0.020710588395700693, 0.0541345465853779, 0.03076252708762768, 0.08408087278276233, 0.0758438874162967, 0.030687119891475306, 0.018180374513629693, 0.11770250086232863, 0.23011817118276467, 0.3401440316194918, 0.45011738396191003, -0.004079632622863972, 10.995026965175366, 0.5559475168976958, 0.7704352452318071, 0.210530495193563, 0.8804323458230112, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0569587547138
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021311406272902973, 0.025213718490371403, 0.021134889964068582]
###############
Output Layer:
###############
[0.033482448977702126, 0.2970715426740174, 0.04173471766050606]
Weights updated in this iteration:
[0.007538084155438216, 0.020013070600168072, 0.053555105359088195, 0.026982059405201462, 0.0795653630710706, 0.07209276808798222, 0.02996620885909954, 0.017319295662700507, 0.11698718637356559, 0.23015728027308932, 0.3401907447491919, 0.4501561894423302, -0.005383008364166985, 10.993470172124285, 0.5546542594634409, 0.7705791858831146, 0.21070242244264586, 0.8805751690408262, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0560732550369
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021267586018070907, 0.02493095709546158, 0.021080735885315584]
###############
Output Layer:
###############
[0.03337814929519717, 0.2938983970444223, 0.041648652359976945]
Weights updated in this iteration:
[0.006972852367190753, 0.019344339733317614, 0.05299455522476913, 0.023334756816370716, 0.07525020622283814, 0.06847567506777112, 0.029267682415295086, 0.01649286267716076, 0.11629444562536023, 0.23019625852911868, 0.34023686027822764, 0.4501948448525656, -0.006660548134120311, 10.991958703209123, 0.5533873011915686, 0.7707225993256448, 0.2108720961901869, 0.880717394630655, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0552352506993
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.02122551785066648, 0.024660374943047403, 0.02102869311057147]
###############
Output Layer:
###############
[0.03327835116678135, 0.2908616326978218, 0.04156633581582268]
Weights updated in this iteration:
[0.006425194525067273, 0.0187023470711804, 0.052451708927332405, 0.019812225485645495, 0.07112091400325526, 0.06498409155812074, 0.02859017171128747, 0.015698649818260003, 0.11562288731128909, 0.23023511122148937, 0.3402824054005426, 0.45023335619774624, -0.007913532214657695, 10.990489890889444, 0.5521453254239441, 0.7708655086304258, 0.21103962182017663, 0.8808590483805016, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0544408915652
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021185080664701376, 0.024401115190940736, 0.020978618050125682]
###############
Output Layer:
###############
[0.03318273816441695, 0.28795165224023594, 0.04148750519646103]
Weights updated in this iteration:
[0.0058940459565674395, 0.018085244443022026, 0.05192548571234726, 0.016406809334286603, 0.06716441034004864, 0.06161025391304953, 0.027932428200246107, 0.01493446579633155, 0.11497124307172467, 0.2302738432627809, 0.34032740532314887, 0.45027172907588264, -0.00914314432450189, 10.98906129461789, 0.5509271155369169, 0.7710079351930126, 0.21120509681400015, 0.8810001542181307, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0536867353752
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021146164890277697, 0.024152402401026868, 0.0209303806203089]
###############
Output Layer:
###############
[0.033091023934722935, 0.2851597706396736, 0.04141192267820151]
Weights updated in this iteration:
[0.005378434456833954, 0.017491359727740588, 0.051414899187501764, 0.013111508739359623, 0.06336886144985411, 0.058347068200638674, 0.02729331015879739, 0.014198325434308604, 0.11433835365805171, 0.23031245924075727, 0.3403718834604629, 0.4503099687156282, -0.010350481128762894, 10.987670676149106, 0.5497315450278721, 0.7711498988991583, 0.2113686115440793, 0.8811407343941209, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.052969695231
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.02110867111982907, 0.023913532977909407, 0.02088386265226745]
###############
Output Layer:
###############
[0.03300294866225627, 0.2824781079832591, 0.04133937249428098]
Weights updated in this iteration:
[0.0048774700560489355, 0.01691917588924645, 0.05091904683602047, 0.009919909815430197, 0.05972352992456799, 0.05518803768083391, 0.026671771014913512, 0.013488425384395769, 0.113723156958347, 0.23035096344777362, 0.3404158616052857, 0.45034808000968635, -0.011536560616885396, 10.986315978064352, 0.548557568788627, 0.7712914182701445, 0.21153024996945496, 0.8812808096425292, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0522869949471
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021072508928027468, 0.0236838669265886, 0.020838956522932113]
###############
Output Layer:
###############
[0.03291827602159932, 0.2798994970416644, 0.04126965839389268]
Weights updated in this iteration:
[0.0043903361418465075, 0.016367312967331155, 0.050437100923854145, 0.006826122714440267, 0.056218648947164586, 0.05212719957664189, 0.026066849203901087, 0.012803123235597146, 0.11312467759672215, 0.23038935990686812, 0.34045936007970784, 0.450386067544454, -0.012702329504014426, 10.984995305030568, 0.5474042154020077, 0.7714325105907138, 0.2116900902474957, 0.8814203993223974, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06552748946440834, 0.0]
###############
Hidden middle Layer:
###############
[0.021072508928027468, 0.0236838669265886, 0.020838956522932113]
###############
Output Layer:
###############
[0.03291827602159932, 0.2798994970416644, 0.04126965839389268]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0043903361418465075, 0.016367312967331155, 0.050437100923854145, 0.006826122714440267, 0.056218648947164586, 0.05212719957664189, 0.026066849203901087, 0.012803123235597146, 0.11312467759672215, 0.23038935990686812, 0.34045936007970784, 0.450386067544454, -0.012702329504014426, 10.984995305030568, 0.5474042154020077, 0.7714325105907138, 0.2116900902474957, 0.8814203993223974, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0801033008768
0.569913930495
0.0801033008768
self.sum1_output_product1_level3 =  0.0456519870484
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0456519870484
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0435273791847
self.product1_product2_sum1_level2 =  0.106056711213
0.569913930495
0.106056711213
self.sum1_output_product1_level3 =  0.060443197143
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.060443197143
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0576302179003
self.product1_product2_sum1_level2 =  0.114518414443
0.569913930495
0.114518414443
self.sum1_output_product1_level3 =  0.0652656396892
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0652656396892
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0622282277324
self.product1_product2_sum1_level2 =  0.117277220342
0.569913930495
0.117277220342
self.sum1_output_product1_level3 =  0.0668379216027
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0668379216027
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0637273368722
self.product1_product2_sum1_level2 =  0.118176685826
0.569913930495
0.118176685826
self.sum1_output_product1_level3 =  0.067350539512
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.067350539512
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0642160979439
self.product1_product2_sum1_level2 =  0.118469942469
0.569913930495
0.118469942469
self.sum1_output_product1_level3 =  0.067517670558
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.067517670558
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0643754508415
self.product1_product2_sum1_level2 =  0.118565554207
0.569913930495
0.118565554207
self.sum1_output_product1_level3 =  0.0675721610197
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0675721610197
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0644274053596
self.product1_product2_sum1_level2 =  0.118596726918
0.569913930495
0.118596726918
self.sum1_output_product1_level3 =  0.0675899267819
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0675899267819
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.0644443443171
self.product1_product2_sum1_level2 =  0.118606890293
0.569913930495
0.118606890293
self.sum1_output_product1_level3 =  0.0675957190306
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0675957190306
####################################################################################################################
self.cell_input_product1_level1 =  0.266467612342
self.forget_feedback_product2_level1 =  0.064449866999
self.product1_product2_sum1_level2 =  0.118610203902
0.569913930495
0.118610203902
self.sum1_output_product1_level3 =  0.0675976075027
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0675976075027
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0675976075027
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05917384852501717,
   0.05917384852501717,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06048439831430534,
   0.06048439831430534,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05524219915715267,
   0.05524219915715267,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.061794948103593504,
   0.061794948103593504,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05589747405179675,
   0.05589747405179675,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05917384852501717, 0.05, 0.05, 0.05, 0.05],
  [0.05917384852501717, 0.05, 0.05, 0.05, 0.05],
  [0.05917384852501717, 0.05, 0.05, 0.05, 0.05],
  [0.05917384852501717, 0.05, 0.05, 0.05, 0.05],
  [0.05917384852501717, 0.05, 0.05, 0.05, 0.05]],
 [[0.06048439831430534, 0.05, 0.05, 0.05, 0.05],
  [0.06048439831430534, 0.05, 0.05, 0.05, 0.05],
  [0.06048439831430534, 0.05, 0.05, 0.05, 0.05],
  [0.06048439831430534, 0.05, 0.05, 0.05, 0.05],
  [0.06048439831430534, 0.05, 0.05, 0.05, 0.05]],
 [[0.061794948103593504, 0.05, 0.05, 0.05, 0.05],
  [0.061794948103593504, 0.05, 0.05, 0.05, 0.05],
  [0.061794948103593504, 0.05, 0.05, 0.05, 0.05],
  [0.061794948103593504, 0.05, 0.05, 0.05, 0.05],
  [0.061794948103593504, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06254641214682838, 0.06261055281730941, 0.06267470332738362]
Scheduled Classes by Deep Learning for process id  1  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 2
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=105, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  2  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 3
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=9136, involuntary=6), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  3  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 5
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=6, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  5  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 7
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=43683, involuntary=10), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  7  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 8
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  8  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 9
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=1268, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  9  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 10
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=169, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  10  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 11
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=170, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  11  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 12
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=1339, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  12  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 13
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=9655, involuntary=361), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  13  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 15
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=7, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  15  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 16
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  16  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 17
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=230, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  17  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 18
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  18  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 19
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  19  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 20
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=7, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  20  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 21
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning for process id  21  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 22
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=3, involuntary=0), 1, 0.0, 0.25]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.034, 0.041999999999999996, 0.05]
###############
Output Layer:
###############
[0.15460000000000002, 0.622, 0.275]
Error before Backpropagation:
0.19147108
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.138879758798
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.02946545490249987, 0.03384274240579456, 0.043082762411555536]
###############
Output Layer:
###############
[0.15065758812879426, 0.5301228494409036, 0.26960198056756013]
Weights updated in this iteration:
[-0.0023339626652003637, 0.007763928472399553, 0.03786181960999947, 0.02081225934376121, 0.07159161448346973, 0.05537096962317825, 0.014185113759431055, -0.0022419182971733995, 0.09233104964622213, 0.229712933067824, 0.339645387907312, 0.4495778427468, -0.004892293728, 10.993956578336, 0.5528054504000001, 0.76955938125, 0.20945570624999998, 0.87935203125, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.102396822107
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.026050223537219416, 0.02719449521410158, 0.037827081694928516]
###############
Output Layer:
###############
[0.1475183183670693, 0.455434894926914, 0.2654350218602504]
Weights updated in this iteration:
[-0.01167703540635618, -0.0029671192953615675, 0.02420089414887766, 0.0026246022073693268, 0.05070206102269488, 0.02877798085640632, -0.00019288936473029449, -0.018755868536557595, 0.07130832677971405, 0.22948422993702536, 0.33938270941927823, 0.449243445647567, -0.00870980153206908, 10.989571954590238, 0.5472237011186403, 0.7692135561532757, 0.20905850655616773, 0.8788463848743568, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0791811080268
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.02357359716321533, 0.022310158487186423, 0.03398719926465847]
###############
Output Layer:
###############
[0.14521507796271213, 0.4006001000826946, 0.26239344457607117]
Weights updated in this iteration:
[-0.018499308526053845, -0.010089064849411702, 0.014294388652861317, -0.010830103560667681, 0.036656349242400396, 0.009240633948745691, -0.01077047464840676, -0.02979808059138723, 0.0559487970586339, 0.2292958005465944, 0.339186003154975, 0.4489698305695477, -0.011587675708607222, 10.986567668066215, 0.5430447895086076, 0.7689319876191775, 0.2087645699555072, 0.8784375240472381, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0641259659608
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.021729019524819655, 0.01874190175525885, 0.03111108508744521]
###############
Output Layer:
###############
[0.14351721017807317, 0.3605343993357857, 0.2601345690973809]
Weights updated in this iteration:
[-0.023616921171357132, -0.014932396497197135, 0.00691607809927862, -0.0207299068505328, 0.027287130838452103, -0.005032392978964599, -0.018749991463673156, -0.03734993123035355, 0.04444434034978084, 0.2291342337505695, 0.3390330956118187, 0.4487368918774579, -0.013798661377616761, 10.98447518127487, 0.5398571041509741, 0.7686929421270593, 0.20853833622554743, 0.8780928805582666, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0538375581365
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.020295067930871633, 0.0160523756633828, 0.028865845598192528]
###############
Output Layer:
###############
[0.1422207595251142, 0.3303226365760522, 0.25838922446872364]
Weights updated in this iteration:
[-0.02762299966009705, -0.018387753945519118, 0.0011802717234865251, -0.028243725773739722, 0.020806246788221686, -0.015790497346468806, -0.025022592057672426, -0.042760228917666634, 0.03546338239277011, 0.22899129294668083, 0.3389098050740087, 0.44853223268852543, -0.01555470244653255, 10.982960545552157, 0.5373428469821366, 0.7684832760246805, 0.2083574932064824, 0.8777926856993311, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0464336952786
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.019136516211601142, 0.01395159429843346, 0.027046030219450585]
###############
Output Layer:
###############
[0.14119361159009297, 0.30671312249099014, 0.25698859043578254]
Weights updated in this iteration:
[-0.030881228942040546, -0.02096484906091077, -0.003453935153595427, -0.03415181536903995, 0.016133245743843835, -0.024193622806266163, -0.030140512696106833, -0.046808246149407956, 0.028184120877802338, 0.22886200077847674, 0.3388075414865492, 0.4483483393516247, -0.016992779296860126, 10.981823099256687, 0.5352974581979608, 0.7682950890324908, 0.2082086467821255, 0.8775250257595547, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0408704052479
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.018171078607688, 0.012258585424770305, 0.025525751479732045]
###############
Output Layer:
###############
[0.14035386221268514, 0.2876778692457249, 0.25582902996058576]
Weights updated in this iteration:
[-0.03361362437437081, -0.022956918678663566, -0.007315685569247998, -0.0389433935037031, 0.012639916370572737, -0.03096565830091878, -0.03444322751417337, -0.0499451667288774, 0.022103005918928186, 0.22874320829540853, 0.3387209350995169, 0.448180447502796, -0.018200162052846757, 10.980942849466798, 0.5335910394254018, 0.7681233912761787, 0.20808346948174697, 0.8772823618047114, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0365419404573
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.017346762780260032, 0.01085857121002214, 0.024225066566602366]
###############
Output Layer:
###############
[0.13964959773948515, 0.2719307719758143, 0.2548449445431462]
Weights updated in this iteration:
[-0.03596085520275968, -0.024540409138568652, -0.01061294887895987, -0.04292991956046099, 0.009950523447350049, -0.036565715159911444, -0.03814691440763542, -0.05244375050314481, 0.016900266266409454, 0.22863281121742365, 0.33864645894758666, 0.4480253676400333, -0.01923412473876988, 10.980245316855495, 0.5321385843934245, 0.7679648494266297, 0.20797651387123767, 0.8770596507627255, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.033078338246
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.016629251686653766, 0.00967622518255425, 0.02309100602469292]
###############
Output Layer:
###############
[0.13904667900476522, 0.2586276481190369, 0.2539930944748495]
Weights updated in this iteration:
[-0.038015998324661696, -0.025826869224444444, -0.01348299325338494, -0.04631647389768523, 0.007830638492838235, -0.041295099269783, -0.04139516623718082, -0.054477062092517604, 0.012364024098771671, 0.22852933293087102, 0.3385816845365074, 0.44788085834964353, -0.020133696872861723, 10.979682210747438, 0.5308823157105649, 0.7678171242201238, 0.20788404216947529, 0.8768533498519158, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0302427199079
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.015994963037935385, 0.008660308599794731, 0.022087057444355074]
###############
Output Layer:
###############
[0.13852180616806123, 0.24719410573784467, 0.25324384748385204]
Weights updated in this iteration:
[-0.03984315918032056, -0.02689005716682752, -0.016020147848258462, -0.04924296961281991, 0.006127769832900265, -0.045358765600821076, -0.04428718637784402, -0.05615987011303716, 0.008348229777420287, 0.2284316941071016, 0.33852487047997887, 0.4477452792682521, -0.020926441787128842, 10.979220928517346, 0.5297815278978409, 0.7676785054712705, 0.20780338272347598, 0.8766608669654202, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.25]
###############
Hidden middle Layer:
###############
[0.015994963037935385, 0.008660308599794731, 0.022087057444355074]
###############
Output Layer:
###############
[0.13852180616806123, 0.24719410573784467, 0.25324384748385204]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.03984315918032056, -0.02689005716682752, -0.016020147848258462, -0.04924296961281991, 0.006127769832900265, -0.045358765600821076, -0.04428718637784402, -0.05615987011303716, 0.008348229777420287, 0.2284316941071016, 0.33852487047997887, 0.4477452792682521, -0.020926441787128842, 10.979220928517346, 0.5297815278978409, 0.7676785054712705, 0.20780338272347598, 0.8766608669654202, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0825597397957
0.569913930495
0.0825597397957
self.sum1_output_product1_level3 =  0.0470519458077
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0470519458077
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0448621849555
self.product1_product2_sum1_level2 =  0.109314033595
0.569913930495
0.109314033595
self.sum1_output_product1_level3 =  0.0622995905443
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0622995905443
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0594002162009
self.product1_product2_sum1_level2 =  0.118036852342
0.569913930495
0.118036852342
self.sum1_output_product1_level3 =  0.0672708464616
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0672708464616
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0641401137459
self.product1_product2_sum1_level2 =  0.120880790869
0.569913930495
0.120880790869
self.sum1_output_product1_level3 =  0.0688916466456
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0688916466456
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0656854831537
self.product1_product2_sum1_level2 =  0.121808012514
0.569913930495
0.121808012514
self.sum1_output_product1_level3 =  0.0694200831775
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0694200831775
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0661893266617
self.product1_product2_sum1_level2 =  0.122110318619
0.569913930495
0.122110318619
self.sum1_output_product1_level3 =  0.0695923716379
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0695923716379
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0663535969516
self.product1_product2_sum1_level2 =  0.122208880792
0.569913930495
0.122208880792
self.sum1_output_product1_level3 =  0.0696485435939
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0696485435939
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0664071547085
self.product1_product2_sum1_level2 =  0.122241015447
0.569913930495
0.122241015447
self.sum1_output_product1_level3 =  0.0696668575809
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0696668575809
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0664246163769
self.product1_product2_sum1_level2 =  0.122251492448
0.569913930495
0.122251492448
self.sum1_output_product1_level3 =  0.0696728285698
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0696728285698
####################################################################################################################
self.cell_input_product1_level1 =  0.274655742072
self.forget_feedback_product2_level1 =  0.0664303094806
self.product1_product2_sum1_level2 =  0.12225490831
0.569913930495
0.12225490831
self.sum1_output_product1_level3 =  0.0696747753172
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0696747753172
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0696747753172
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.085, 0.085, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.0675, 0.0675, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.09, 0.09, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.095, 0.095, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   0.07250000000000001,
   0.07250000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.085, 0.085, 0.05, 0.05, 0.05],
  [0.085, 0.085, 0.05, 0.05, 0.05],
  [0.085, 0.085, 0.05, 0.05, 0.05],
  [0.085, 0.085, 0.05, 0.05, 0.05],
  [0.085, 0.085, 0.05, 0.05, 0.05]],
 [[0.09, 0.09, 0.05, 0.05, 0.05],
  [0.09, 0.09, 0.05, 0.05, 0.05],
  [0.09, 0.09, 0.05, 0.05, 0.05],
  [0.09, 0.09, 0.05, 0.05, 0.05],
  [0.09, 0.09, 0.05, 0.05, 0.05]],
 [[0.095, 0.095, 0.05, 0.05, 0.05],
  [0.095, 0.095, 0.05, 0.05, 0.05],
  [0.095, 0.095, 0.05, 0.05, 0.05],
  [0.095, 0.095, 0.05, 0.05, 0.05],
  [0.095, 0.095, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06547542143183492, 0.06595816173431536, 0.06644100601636693]
Scheduled Classes by Deep Learning for process id  22  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 23
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=72, involuntary=23), 1, 0.0, 0.95]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[0.0732, 0.1036, 0.13399999999999998]
###############
Output Layer:
###############
[0.49956, 1.6916, 0.93412]
Error before Backpropagation:
1.759933864
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
2562.88775436
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[1.47973558747176, -6.478488444365354, 0.48023208713495114]
###############
Output Layer:
###############
[-1.3702504688466544, -71.56824917008363, 0.6543231153296817]
Weights updated in this iteration:
[0.8187855852547749, 1.167674680770419, 1.536563776286063, -3.7418301188338092, -5.25767213539867, -6.84051415196353, 0.23209024963298058, 0.3027725390980435, 0.48445482856310645, 0.22250505780408775, 0.32939240421452853, 0.4362797506249694, 0.1440079409513472, 11.203814517521305, 0.823621094091264, 0.7667380456048138, 0.2053833541620043, 0.8740286627191948, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
5.26194347816e+33
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[-26641179.243132785, -42601638831.03407, 1855333.4851435872]
###############
Output Layer:
###############
[-1322797074618.8772, -1.0258599784852856e+17, -36503261293.76063]
Weights updated in this iteration:
[-86409760.89956395, 378313969.9012102, -28043346.592771355, -138176963627.62332, 604957986876.9036, -44843830348.47797, 6017709.602042345, -26346369.241122603, 1952982.5948879865, -6.795370670499029, 31.054629530858627, -1.8412954707799425, -550087.9279162898, 2408373.378460371, -178524.27581208828, 0.6180261777929269, 0.8564646184880096, 0.8257658438306801, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[1.2821266985270384e+91, 7.711176906074798e+100, 9.763551617308562e+87]
###############
Output Layer:
###############
[7.603729352688528e+147, 3.5465920342869867e+162, 1.5978719548962238e+143]
Weights updated in this iteration:
[-1.9379331879446267e+92, -3.0989292552720794e+95, 1.3496070510810931e+91, -1.1655435973342204e+102, -1.863808914849587e+105, 8.117028322183999e+100, -1.475759824136731e+89, -2.3598725287443545e+92, 1.0277422755061645e+88, 6.166420288683462e+43, 9.860659981354578e+46, -4.2943917537029683e+42, 2.87619088759093e+58, 4.59928760224998e+61, -2.0030244212210794e+57, 1.2958312407458981e+39, 2.0721505606197e+42, -9.024371894764189e+37, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.95]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0908242346482
0.569913930495
0.0908242346482
self.sum1_output_product1_level3 =  0.0517619965526
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0517619965526
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0493530336131
self.product1_product2_sum1_level2 =  0.120273037642
0.569913930495
0.120273037642
self.sum1_output_product1_level3 =  0.0685452796151
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0685452796151
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0653552357747
self.product1_product2_sum1_level2 =  0.129874358939
0.569913930495
0.129874358939
self.sum1_output_product1_level3 =  0.0740172063734
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0740172063734
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0705725033303
self.product1_product2_sum1_level2 =  0.133004719472
0.569913930495
0.133004719472
self.sum1_output_product1_level3 =  0.0758012424488
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0758012424488
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0722735117586
self.product1_product2_sum1_level2 =  0.134025324529
0.569913930495
0.134025324529
self.sum1_output_product1_level3 =  0.0763828994883
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0763828994883
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0728280989332
self.product1_product2_sum1_level2 =  0.134358076834
0.569913930495
0.134358076834
self.sum1_output_product1_level3 =  0.0765725396622
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0765725396622
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0730089134013
self.product1_product2_sum1_level2 =  0.134466565515
0.569913930495
0.134466565515
self.sum1_output_product1_level3 =  0.0766343688728
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0766343688728
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0730678651287
self.product1_product2_sum1_level2 =  0.134501936551
0.569913930495
0.134501936551
self.sum1_output_product1_level3 =  0.0766545273191
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0766545273191
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0730870854167
self.product1_product2_sum1_level2 =  0.134513468724
0.569913930495
0.134513468724
self.sum1_output_product1_level3 =  0.0766610996651
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0766610996651
####################################################################################################################
self.cell_input_product1_level1 =  0.302204058247
self.forget_feedback_product2_level1 =  0.0730933518907
self.product1_product2_sum1_level2 =  0.134517228608
0.569913930495
0.134517228608
self.sum1_output_product1_level3 =  0.0766632424756
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0766632424756
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0766632424756
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.183, 0.183, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.1165, 0.1165, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.202, 0.202, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.126, 0.126, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   0.22099999999999997,
   0.22099999999999997,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05, 0.1355, 0.1355, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.183, 0.183, 0.05, 0.05, 0.05],
  [0.183, 0.183, 0.05, 0.05, 0.05],
  [0.183, 0.183, 0.05, 0.05, 0.05],
  [0.183, 0.183, 0.05, 0.05, 0.05],
  [0.183, 0.183, 0.05, 0.05, 0.05]],
 [[0.202, 0.202, 0.05, 0.05, 0.05],
  [0.202, 0.202, 0.05, 0.05, 0.05],
  [0.202, 0.202, 0.05, 0.05, 0.05],
  [0.202, 0.202, 0.05, 0.05, 0.05],
  [0.202, 0.202, 0.05, 0.05, 0.05]],
 [[0.22099999999999997, 0.22099999999999997, 0.05, 0.05, 0.05],
  [0.22099999999999997, 0.22099999999999997, 0.05, 0.05, 0.05],
  [0.22099999999999997, 0.22099999999999997, 0.05, 0.05, 0.05],
  [0.22099999999999997, 0.22099999999999997, 0.05, 0.05, 0.05],
  [0.22099999999999997, 0.22099999999999997, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.07497630526437837, 0.07683457562693616, 0.07870098390781932]
