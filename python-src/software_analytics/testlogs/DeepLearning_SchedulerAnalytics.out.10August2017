========================================================================================
Process id: 3130
Process cmdline: ['/usr/bin/mongod', '--config', '/etc/mongodb.conf']
Process executable: /usr/bin/mongod
Process name: mongod
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 434, 1015936, 10, 0.43985327302984095, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: mongod
mongod 0.01
get_expected_priority(): proc_name: mongod
mongod 0.01
get_expected_priority(): proc_name: mongod
mongod 0.01
Expected output layer: [0.001, 0.002, 0.003]
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02012508901958532, 0.02047304765953855, 0.020143632259025823]
###############
Output Layer:
###############
[0.031781843197583565, 0.24544275192280862, 0.04017158774597387]
Error before Backpropagation:
0.030796811135
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0307602541339
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.020123406510082854, 0.020462921433226106, 0.020141524029840877]
###############
Output Layer:
###############
[0.031777224722555245, 0.24529382824646784, 0.04016697247922499]
Weights updated in this iteration:
[0.009656782593299246, 0.02265084843509862, 0.05565646635308056, 0.040934337292286974, 0.0968986224099489, 0.08593243399247273, 0.032569939455007225, 0.020562503796850005, 0.11956954319759679, 0.2299809372858802, 0.3399806076954833, 0.44998091972151205, -0.0009073546424210716, 10.999076957382881, 0.5590918093218041, 0.7699711555992588, 0.20997065688551678, 0.8799711290220531, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.030723802309
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.020121727591070577, 0.020452819534497214, 0.020139420235530874]
###############
Output Layer:
###############
[0.03177261625862594, 0.2451452430751224, 0.040162366401624086]
Weights updated in this iteration:
[0.00931416844034944, 0.02230245381860015, 0.05531354373751125, 0.03887285996617751, 0.09480236457510276, 0.08386910067562095, 0.032140621721256354, 0.020125942767720423, 0.11913983894019646, 0.2299618817039757, 0.33996123061461075, 0.4499618469834724, -0.0018137070970337954, 10.998155313273557, 0.5581846408593005, 0.7699423203655978, 0.20994133515409835, 0.8799422678274339, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0306874551829
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02012005224892701, 0.020442741866627957, 0.020137320859461842]
###############
Output Layer:
###############
[0.031768017766464055, 0.24499699507589554, 0.040157769477667526]
Weights updated in this iteration:
[0.008972155510883484, 0.021954813254778178, 0.0549712300827157, 0.036815553210430875, 0.09271120596996088, 0.0818099849699812, 0.0317120443335401, 0.019690313375171808, 0.1187108847127029, 0.229942833237182, 0.33994186871578885, 0.44994278176773167, -0.002719060022379987, 10.99723506326454, 0.5572784918747021, 0.7699134942785401, 0.20991203474965697, 0.8799134163941574, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0306512122808
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02011838047010448, 0.020432688333429427, 0.020135225885089106]
###############
Output Layer:
###############
[0.03176342920695475, 0.2448490829232759, 0.040153181672046054]
Weights updated in this iteration:
[0.008630741784731392, 0.021607923863682137, 0.05462952332846431, 0.03476240228876142, 0.09062512618320703, 0.0797550718727867, 0.0312842048387758, 0.019255612100860676, 0.1182826780124505, 0.22992379186846595, 0.33992252195760786, 0.4499237240562161, -0.003623416065803157, 10.996316202969224, 0.5563733596418284, 0.7698846773176915, 0.20988275561634664, 0.8798845747003278, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0306150731311
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.020116712241128612, 0.020422658839243955, 0.020133135295956686]
###############
Output Layer:
###############
[0.03175885054119837, 0.2447015052990654, 0.04014860294964341]
Weights updated in this iteration:
[0.008289925251754267, 0.021261782781026424, 0.054288421424806374, 0.032713392539347776, 0.0885441049169008, 0.07770434645746307, 0.03085710079592687, 0.018821835445406332, 0.11785521634912341, 0.22990475758086512, 0.3399031902988407, 0.44990467383092714, -0.004526777863512511, 10.995398728021756, 0.5554692414460389, 0.7698558694627409, 0.20985349769856165, 0.8798557427241377, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0305790372649
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02011504754859781, 0.020412653288941363, 0.02013104907569667]
###############
Output Layer:
###############
[0.031754281730508996, 0.24455426089232765, 0.040144033275534986]
Weights updated in this iteration:
[0.007949703911778813, 0.02091638715808047, 0.05394792233200267, 0.030668509374345974, 0.08646812198567595, 0.07565779387312807, 0.0304307297759249, 0.018388979928258855, 0.1174284972446753, 0.2298857303574875, 0.3398838736984419, 0.449885631073941, -0.005429148040646483, 10.99448263407691, 0.5545661345841666, 0.7698270706934589, 0.20982426094093515, 0.8798269204438683, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0305431042162
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.020113386379182755, 0.020402671587915262, 0.020128967208028595]
###############
Output Layer:
###############
[0.03174972273641287, 0.24440734839933678, 0.0401394726149865]
Weights updated in this iteration:
[0.007610075774532363, 0.02057173416155976, 0.05360802402045867, 0.028627738279406477, 0.08439715731594555, 0.07361539934409529, 0.030005089361592362, 0.017957042087568188, 0.11700251823324982, 0.22986671018151109, 0.33986457211554666, 0.4498665957674082, -0.006330529211335824, 10.993567916809948, 0.5536640363644524, 0.7697982809896982, 0.20979504528833776, 0.8797981078378886, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0305072735221
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02011172871962589, 0.020392713642079372, 0.020126889676758866]
###############
Output Layer:
###############
[0.03174517352064696, 0.2442607665235268, 0.04013492093345262]
Weights updated in this iteration:
[0.0072710388595784384, 0.02022782097351778, 0.05326872447065824, 0.026591064813195023, 0.08233119094511399, 0.07157714816938196, 0.029580177147565945, 0.017526018480054355, 0.11657727686110173, 0.22984769703618343, 0.33984528550946946, 0.44984756789355307, -0.007230923978766235, 10.9926545719165, 0.5527629441064801, 0.7697695003313924, 0.20976585068587617, 0.8797693048846543, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.030471544722
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.020110074556740937, 0.02038277935786389, 0.020124816465780137]
###############
Output Layer:
###############
[0.031740634045157465, 0.24411451397544126, 0.04013037819657571]
Weights updated in this iteration:
[0.006932591196252825, 0.01988464479123888, 0.05293002167309792, 0.024558474606917312, 0.08027020302079559, 0.06954302572222054, 0.029155990740220485, 0.017095905680878765, 0.11615277068651857, 0.22982869090482133, 0.3398260138397033, 0.4498285474346736, -0.008130334935240548, 10.991742595112436, 0.5518628551411112, 0.7697407286985561, 0.20973667707889168, 0.8797405115627083, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0304359173587
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02010842387741239, 0.020372868642211855, 0.02012274755907072]
###############
Output Layer:
###############
[0.03173610427209836, 0.2439685894726834, 0.04012584437018443]
Weights updated in this iteration:
[0.006594730823600157, 0.01954220282713206, 0.05259191362822168, 0.022529953363847458, 0.07821417380004, 0.06751301744957429, 0.028732527757593513, 0.016666700283516613, 0.11572899727974305, 0.22980969177081034, 0.3398067570659184, 0.44980953437314064, -0.009028764662240476, 10.990831982133736, 0.550963766810421, 0.7697119660712842, 0.20970752441295895, 0.8797117278506794, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0043985327302984096, 0.00042719226407962705]
###############
Hidden middle Layer:
###############
[0.02010842387741239, 0.020372868642211855, 0.02012274755907072]
###############
Output Layer:
###############
[0.03173610427209836, 0.2439685894726834, 0.04012584437018443]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.006594730823600157, 0.01954220282713206, 0.05259191362822168, 0.022529953363847458, 0.07821417380004, 0.06751301744957429, 0.028732527757593513, 0.016666700283516613, 0.11572899727974305, 0.22980969177081034, 0.3398067570659184, 0.44980953437314064, -0.009028764662240476, 10.990831982133736, 0.550963766810421, 0.7697119660712842, 0.20970752441295895, 0.8797117278506794, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0830739478285
0.569913930495
0.0830739478285
self.sum1_output_product1_level3 =  0.0473450001287
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0473450001287
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.045141600757
self.product1_product2_sum1_level2 =  0.109995891108
0.569913930495
0.109995891108
self.sum1_output_product1_level3 =  0.06268819064
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.06268819064
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0597707311513
self.product1_product2_sum1_level2 =  0.118773369345
0.569913930495
0.118773369345
self.sum1_output_product1_level3 =  0.0676905977616
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0676905977616
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0645403301479
self.product1_product2_sum1_level2 =  0.121635128743
0.569913930495
0.121635128743
self.sum1_output_product1_level3 =  0.0693215543082
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0693215543082
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0660953832491
self.product1_product2_sum1_level2 =  0.122568160604
0.569913930495
0.122568160604
self.sum1_output_product1_level3 =  0.0698533021633
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0698533021633
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0666023839738
self.product1_product2_sum1_level2 =  0.122872361039
0.569913930495
0.122872361039
self.sum1_output_product1_level3 =  0.0700266702287
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0700266702287
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0667676836248
self.product1_product2_sum1_level2 =  0.122971540829
0.569913930495
0.122971540829
self.sum1_output_product1_level3 =  0.070083194173
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.070083194173
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0668215769888
self.product1_product2_sum1_level2 =  0.123003876847
0.569913930495
0.123003876847
self.sum1_output_product1_level3 =  0.0701016229203
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0701016229203
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0668391480766
self.product1_product2_sum1_level2 =  0.1230144195
0.569913930495
0.1230144195
self.sum1_output_product1_level3 =  0.070107631325
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.070107631325
####################################################################################################################
self.cell_input_product1_level1 =  0.276369768847
self.forget_feedback_product2_level1 =  0.0668448768548
self.product1_product2_sum1_level2 =  0.123017856767
0.569913930495
0.123017856767
self.sum1_output_product1_level3 =  0.0701095902713
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0701095902713
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0701095902713
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11157945822417775,
   0.11157945822417775,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08078972911208887,
   0.08078972911208887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12037652368477456,
   0.12037652368477456,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08518826184238729,
   0.08518826184238729,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12917358914537136,
   0.12917358914537136,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08958679457268569,
   0.08958679457268569,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.11157945822417775, 0.05, 0.05, 0.05, 0.05],
  [0.11157945822417775, 0.05, 0.05, 0.05, 0.05],
  [0.11157945822417775, 0.05, 0.05, 0.05, 0.05],
  [0.11157945822417775, 0.05, 0.05, 0.05, 0.05],
  [0.11157945822417775, 0.05, 0.05, 0.05, 0.05]],
 [[0.12037652368477456, 0.05, 0.05, 0.05, 0.05],
  [0.12037652368477456, 0.05, 0.05, 0.05, 0.05],
  [0.12037652368477456, 0.05, 0.05, 0.05, 0.05],
  [0.12037652368477456, 0.05, 0.05, 0.05, 0.05],
  [0.12037652368477456, 0.05, 0.05, 0.05, 0.05]],
 [[0.12917358914537136, 0.05, 0.05, 0.05, 0.05],
  [0.12917358914537136, 0.05, 0.05, 0.05, 0.05],
  [0.12917358914537136, 0.05, 0.05, 0.05, 0.05],
  [0.12917358914537136, 0.05, 0.05, 0.05, 0.05],
  [0.12917358914537136, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06511974526810052, 0.06555363500746457, 0.06598814683577349]
Scheduled Classes by Deep Learning for process id  3130  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 3929
Process cmdline: ['/usr/bin/java', '-cp', '/home/shrinivaasanka/neo4j-community-2.3.2/lib/commons-lang3-3.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/concurrentlinkedhashmap-lru-1.4.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/jline-2.12.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/lucene-core-3.6.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-codegen-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-consistency-check-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-consistency-check-legacy-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-csv-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-cypher-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-cypher-compiler-1.9_2.11-2.0.5.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-cypher-compiler-2.2_2.11-2.2.6.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-cypher-compiler-2.3-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-cypher-frontend-2.3-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-function-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-graph-algo-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-graph-matching-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-import-tool-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-io-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-jmx-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-kernel-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-logging-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-lucene-index-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-primitive-collections-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-shell-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-udc-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/neo4j-unsafe-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/opencsv-2.3.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/parboiled-core-1.1.7.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/parboiled-scala_2.11-1.1.7.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/scala-library-2.11.7.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/scala-parser-combinators_2.11-1.0.4.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/scala-reflect-2.11.7.jar:/home/shrinivaasanka/neo4j-community-2.3.2/lib/server-api-2.3.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/asm-5.0.2.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/bcpkix-jdk15on-1.52.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/bcprov-jdk15on-1.52.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/commons-beanutils-1.8.3.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/commons-configuration-1.10.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/commons-digester-2.1.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/commons-io-2.4.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/commons-lang-2.6.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/commons-logging-1.1.1.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jackson-core-asl-1.9.13.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jackson-jaxrs-1.9.13.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jackson-mapper-asl-1.9.13.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/javax.servlet-api-3.1.0.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jersey-core-1.19.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jersey-multipart-1.19.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jersey-server-1.19.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jersey-servlet-1.19.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jetty-http-9.2.4.v20141103.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jetty-io-9.2.4.v20141103.jar:/home/shrinivaasanka/neo4j-community-2.3.2/system/lib/jetty-security-9.2.4.v20141103.jar:/home/shrinivaasanka/neo4j-c']
Process executable: /usr/lib/jvm/java-7-openjdk-i386/jre/bin/java
Process name: java
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 3, 5, 37, 2.5391219589442975, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: java
mongod 0.01
java 0.8
get_expected_priority(): proc_name: java
mongod 0.01
java 0.8
get_expected_priority(): proc_name: java
mongod 0.01
java 0.8
Expected output layer: [0.08, 0.16, 0.24000000000000005]
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.05418399805055718, 0.07531373073935485, 0.0925332156113783]
###############
Output Layer:
###############
[0.3280689880030088, 1.1844510381329034, 0.6055375619541936]
Error before Backpropagation:
0.62232793077
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.250859089326
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.09957996888782711, 0.030322729266150365, 0.14172291700405776]
###############
Output Layer:
###############
[0.319757114492518, 0.7036949782175467, 0.6255262317451342]
Weights updated in this iteration:
[0.05282847631568157, 0.08252998023893346, 0.1291407200614466, 0.0005535888874093967, 0.04000107610363509, 0.01551175526498158, 0.07940764196064316, 0.08550488662005118, 0.19925307275319604, 0.2270369881616394, 0.3358815243651162, 0.444939889576951, 0.0121271980649132, 11.016856351737491, 0.5807103328229019, 0.7652690342888286, 0.20342413460564818, 0.8719206502666452, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.11165590603
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.07091598404396475, 0.0028453185850859063, 0.11085415099377346]
###############
Output Layer:
###############
[0.29921287652750095, 0.3901824887473228, 0.5896763484970803]
Weights updated in this iteration:
[0.01956231947509489, 0.07240022544117197, 0.08179609003577129, -0.03133548230309625, 0.030290652581129833, -0.029872996710181815, 0.043582713320500885, 0.07459596965344843, 0.1482667805796922, 0.22184387586557633, 0.33430018888207724, 0.4375490153765709, 0.0008383202701121115, 11.013418817171921, 0.5646439219909828, 0.7562762775130786, 0.200685783387899, 0.8591220951314961, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0748970579614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.05417852562113537, -0.01614755966406675, 0.0915405445124643]
###############
Output Layer:
###############
[0.2859186745028859, 0.17714115366064542, 0.567257996487283]
Weights updated in this iteration:
[0.001736111950929261, 0.0716849954369086, 0.05393059360121434, -0.051563821672041675, 0.029479043312390037, -0.06149344754349724, 0.023012782780372474, 0.07377065493140399, 0.11611236268973997, 0.21858418371092486, 0.3341694022506242, 0.43245354344346015, -0.0030457231897574205, 11.013262980075522, 0.5585724791989061, 0.750276285783077, 0.2004450493952754, 0.8497430531825522, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0712184681592
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.04225645272928058, -0.024659065212403294, 0.07720641733007666]
###############
Output Layer:
###############
[0.27804627929771697, 0.08329407087052126, 0.5519788582757893]
Weights updated in this iteration:
[-0.01011252389392034, 0.07521640482926517, 0.03391102746255378, -0.060022898432305204, 0.03200021651013372, -0.07578598289453636, 0.008766950308243707, 0.0780165332968769, 0.09204247066921428, 0.21630639669124566, 0.33484828196136995, 0.42860497315339824, -0.0031810899520729116, 11.0133033252648, 0.5583437622489856, 0.7459239025588597, 0.20174224910656688, 0.8423892257074769, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0708684747768
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.030717981173985665, -0.02904806909800214, 0.06299155070299604]
###############
Output Layer:
###############
[0.2721590931822651, 0.035276460019303346, 0.5387572108513901]
Weights updated in this iteration:
[-0.020782089176446073, 0.08144270848459984, 0.014416752464823667, -0.06408138774419042, 0.03436857793523442, -0.08320121534555427, -0.0043774637272906945, 0.08568705322806068, 0.06802641986418345, 0.21462648608750032, 0.3358286062066197, 0.425535622302277, -0.0029335951398157404, 11.013158897838432, 0.5587959579854469, 0.7426637407653398, 0.20364474060968554, 0.8364326109010558, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0694404264602
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.018719958233049817, -0.03110935508713619, 0.04794028356983092]
###############
Output Layer:
###############
[0.2673968291270686, 0.01277708792653115, 0.5265112484948429]
Weights updated in this iteration:
[-0.03072761680885589, 0.09084757048532895, -0.005977953963850948, -0.06579005065175049, 0.035984353331504, -0.08670506950726753, -0.016853918715965643, 0.09748525456676278, 0.04244169010732249, 0.213457221167497, 0.33693430674070596, 0.42313787975284145, -0.002803209737385011, 11.013035600537163, 0.5590633316335271, 0.7403832214667628, 0.2058012847454861, 0.8317560845951172, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0665876314033
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[0.004844389749004703, -0.032002810425071, 0.030282220717603316]
###############
Output Layer:
###############
[0.2630381359741169, 0.003031762239232849, 0.5139220252551832]
Weights updated in this iteration:
[-0.040012938736278425, 0.1062781806043505, -0.029756904787144732, -0.06638793753833377, 0.036977938638966056, -0.0882362089750299, -0.02867042903668694, 0.11712226501741266, 0.012180572612888758, 0.2127700069109714, 0.3380763387933979, 0.42137798018891487, -0.0027684459256848994, 11.01297782905344, 0.5591523589150051, 0.7390461215165576, 0.2080233151912854, 0.8283318808668804, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0597418366183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[-0.026060921788187957, -0.032484698140025095, -0.009544318205942988]
###############
Output Layer:
###############
[0.2556224102932805, -0.0021806430640354812, 0.48967555197631385]
Weights updated in this iteration:
[-0.04863883027886511, 0.1632621942959287, -0.08367724669035032, -0.06652243581575436, 0.03786645568606656, -0.08907695605237322, -0.039786298386772244, 0.19055546877516666, -0.05730458992930415, 0.21259811937188866, 0.33921185525939374, 0.420303513296944, -0.00276614751664574, 11.012962645397176, 0.5591667262416302, 0.7387146324537056, 0.21021318474887346, 0.8262597468135007, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0593753871073
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[-0.027930920919392612, -0.03246322083505239, -0.012000180354518799]
###############
Output Layer:
###############
[0.25536393968147536, -0.001937543698877442, 0.4885447541486405]
Weights updated in this iteration:
[-0.05607751123887126, 0.15398994723154144, -0.0864015224735257, -0.06643700111318858, 0.03797294925318167, -0.08904566721333117, -0.0495554874201608, 0.17837826493710793, -0.06088237008253457, 0.21346900659979262, 0.34029740813977133, 0.4206224592226547, -0.002756910762470497, 11.012974158925884, 0.5591701090276174, 0.7403406326271701, 0.2122399789367354, 0.8268552385303554, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.058981705854
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[-0.030053572827435832, -0.0324415504163134, -0.014789428406025328]
###############
Output Layer:
###############
[0.2550951785450272, -0.001691367453472481, 0.48730748441827515]
Weights updated in this iteration:
[-0.0634659395316086, 0.14540261207051025, -0.08957587079899458, -0.06636157172492195, 0.03806061843157429, -0.08901325989436583, -0.05926417264599756, 0.16709416895567106, -0.0650535885701571, 0.21440039147820814, 0.34137992715436777, 0.42102261740407354, -0.00274813014698235, 11.012984364355933, 0.5591738815122336, 0.7420752426367395, 0.21425606096875482, 0.8276004926170236, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.025391219589442973, 0.6]
###############
Hidden middle Layer:
###############
[-0.030053572827435832, -0.0324415504163134, -0.014789428406025328]
###############
Output Layer:
###############
[0.2550951785450272, -0.001691367453472481, 0.48730748441827515]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.0634659395316086, 0.14540261207051025, -0.08957587079899458, -0.06636157172492195, 0.03806061843157429, -0.08901325989436583, -0.05926417264599756, 0.16709416895567106, -0.0650535885701571, 0.21440039147820814, 0.34137992715436777, 0.42102261740407354, -0.00274813014698235, 11.012984364355933, 0.5591738815122336, 0.7420752426367395, 0.21425606096875482, 0.8276004926170236, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0993260860966
0.569913930495
0.0993260860966
self.sum1_output_product1_level3 =  0.0566073201281
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0566073201281
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0539728596092
self.product1_product2_sum1_level2 =  0.131546784688
0.569913930495
0.131546784688
self.sum1_output_product1_level3 =  0.0749703451055
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0749703451055
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0714812837294
self.product1_product2_sum1_level2 =  0.14205183916
0.569913930495
0.14205183916
self.sum1_output_product1_level3 =  0.0809573219898
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0809573219898
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.07718963138
self.product1_product2_sum1_level2 =  0.14547684775
0.569913930495
0.14547684775
self.sum1_output_product1_level3 =  0.0829092820975
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0829092820975
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0790507487871
self.product1_product2_sum1_level2 =  0.146593518195
0.569913930495
0.146593518195
self.sum1_output_product1_level3 =  0.0835456881395
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0835456881395
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0796575369883
self.product1_product2_sum1_level2 =  0.146957591115
0.569913930495
0.146957591115
self.sum1_output_product1_level3 =  0.0837531783687
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0837531783687
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0798553707841
self.product1_product2_sum1_level2 =  0.147076291393
0.569913930495
0.147076291393
self.sum1_output_product1_level3 =  0.0838208273104
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0838208273104
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0799198713968
self.product1_product2_sum1_level2 =  0.14711499176
0.569913930495
0.14711499176
self.sum1_output_product1_level3 =  0.083842883189
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.083842883189
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0799409008121
self.product1_product2_sum1_level2 =  0.14712760941
0.569913930495
0.14712760941
self.sum1_output_product1_level3 =  0.0838500741631
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0838500741631
####################################################################################################################
self.cell_input_product1_level1 =  0.330543563075
self.forget_feedback_product2_level1 =  0.0799477571238
self.product1_product2_sum1_level2 =  0.147131723197
0.569913930495
0.147131723197
self.sum1_output_product1_level3 =  0.0838524186676
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0838524186676
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0838524186676
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4054770742522017,
   0.4054770742522017,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22773853712610087,
   0.22773853712610087,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4562595134310876,
   0.4562595134310876,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.25312975671554383,
   0.25312975671554383,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5070419526099735,
   0.5070419526099735,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.2785209763049868,
   0.2785209763049868,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.4054770742522017, 0.05, 0.05, 0.05, 0.05],
  [0.4054770742522017, 0.05, 0.05, 0.05, 0.05],
  [0.4054770742522017, 0.05, 0.05, 0.05, 0.05],
  [0.4054770742522017, 0.05, 0.05, 0.05, 0.05],
  [0.4054770742522017, 0.05, 0.05, 0.05, 0.05]],
 [[0.4562595134310876, 0.05, 0.05, 0.05, 0.05],
  [0.4562595134310876, 0.05, 0.05, 0.05, 0.05],
  [0.4562595134310876, 0.05, 0.05, 0.05, 0.05],
  [0.4562595134310876, 0.05, 0.05, 0.05, 0.05],
  [0.4562595134310876, 0.05, 0.05, 0.05, 0.05]],
 [[0.5070419526099735, 0.05, 0.05, 0.05, 0.05],
  [0.5070419526099735, 0.05, 0.05, 0.05, 0.05],
  [0.5070419526099735, 0.05, 0.05, 0.05, 0.05],
  [0.5070419526099735, 0.05, 0.05, 0.05, 0.05],
  [0.5070419526099735, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.08012096704744016, 0.08286593738712084, 0.08567434572926536]
Scheduled Classes by Deep Learning for process id  3929  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 3976
Process cmdline: ['/usr/bin/X', '-core', ':0', '-seat', 'seat0', '-auth', '/var/run/lightdm/root/:0', '-nolisten', 'tcp', 'vt7', '-novtswitch']
Process executable: /usr/bin/Xorg
Process name: Xorg
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 23529, 567675, 2, 0.3306407905891604, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: Xorg
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
X 0.5
get_expected_priority(): proc_name: Xorg
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
X 0.5
get_expected_priority(): proc_name: Xorg
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
X 0.5
Expected output layer: [0.05, 0.1, 0.15]
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022397136032912268, 0.023974759405803892, 0.02504319596118821]
###############
Output Layer:
###############
[0.05195436471740997, 0.3069332399746104, 0.0787547444517675]
Error before Backpropagation:
0.023950535893
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0236189476991
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022364926376213783, 0.023835650860793974, 0.02499575135949919]
###############
Output Layer:
###############
[0.05189945644313329, 0.3053101483219476, 0.07871163868184675]
Weights updated in this iteration:
[0.009354310178832691, 0.02230882867834018, 0.05527802658795858, 0.040211365138377135, 0.09601493754470214, 0.08488190806176295, 0.03204890335632243, 0.01998190942045761, 0.11893653815422449, 0.22999784399688858, 0.3399976921310029, 0.449997589280686, -0.0009859201425878675, 10.998944633002312, 0.558897600510323, 0.7701157711712903, 0.21012392593292875, 0.880129448699379, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0232965340268
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.02233330303775165, 0.023699342433774136, 0.02494914716028142]
###############
Output Layer:
###############
[0.05184564228045345, 0.30371946799997235, 0.07866950150187457]
Weights updated in this iteration:
[0.008719907864077702, 0.021632707870902074, 0.0545689984313369, 0.03747685367224743, 0.09310060375772278, 0.08182573128088283, 0.031113966904357918, 0.018985491257876196, 0.11789162350987799, 0.22999575367091535, 0.3399954643445463, 0.44999525306605204, -0.0019598105415060303, 10.997906699269162, 0.5578091496988483, 0.7702313878910104, 0.21024714564394248, 0.8802586656138689, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0229829360736
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022302248229586676, 0.023565746134083455, 0.024903358346541657]
###############
Output Layer:
###############
[0.05179288798945425, 0.3021601803948101, 0.07862830279057999]
Weights updated in this iteration:
[0.008096459575818853, 0.020971125728385092, 0.05387252719892212, 0.03479480894933688, 0.09025450901734743, 0.07882954534415683, 0.030194722534989796, 0.01801002036343894, 0.11686471033859931, 0.22999372742667276, 0.33999331416299555, 0.4499929894928714, -0.0029219596353850415, 10.996885699338716, 0.5567343064796101, 0.7703468528123173, 0.2103696730956371, 0.88038765465363, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0226778128679
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022271744902223473, 0.023434777632905704, 0.024858360922972774]
###############
Output Layer:
###############
[0.05174116075556305, 0.30063130938315147, 0.07858801367891051]
Weights updated in this iteration:
[0.007483645394082255, 0.020323593547525744, 0.053188240529231864, 0.032163641735122574, 0.08747427726814466, 0.07589150522682354, 0.029290720850967525, 0.017054803921977623, 0.11585527498251634, 0.22999176372667457, 0.339991239212741, 0.4499907967666388, -0.003872645945388019, 10.995881153429082, 0.5556727416719084, 0.7704621685120151, 0.21049152182032072, 0.8805164196177266, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0223808402101
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.02224177670650632, 0.023306356078130835, 0.024814131864048027]
###############
Output Layer:
###############
[0.0516904291172446, 0.29913191917569953, 0.07854860648628026]
Weights updated in this iteration:
[0.0068811582718474144, 0.01968964448793261, 0.0525157812135515, 0.02958182687714793, 0.08475763978187714, 0.07300984122350515, 0.028401530234280745, 0.016119179682978702, 0.1148628147430533, 0.22998986108861197, 0.3399892372187298, 0.4499886731582788, -0.0048121385058106795, 10.994892600473849, 0.5546241374761454, 0.7705773374935613, 0.21061270493752104, 0.8806449641984487, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0220917096849
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022212327957824764, 0.02318040391996934, 0.024770649065212175]
###############
Output Layer:
###############
[0.051640662897922046, 0.29766111228656567, 0.07851005466057916]
Weights updated in this iteration:
[0.006288703388970742, 0.019068832371942263, 0.051854806428074926, 0.027047900158612283, 0.0821024293120766, 0.07018285520717737, 0.027526735968146754, 0.015202514304548666, 0.113886846835444, 0.22998801808276764, 0.339987305999339, 0.4499866170010459, -0.005740697272796751, 10.993919597212985, 0.5535881869767066, 0.7706923621899249, 0.21073323517063164, 0.880773291985426, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0218101277412
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022183383602470817, 0.023056846746633997, 0.024727891296961065]
###############
Output Layer:
###############
[0.05159183314185288, 0.29621802762051813, 0.07847233272169654]
Weights updated in this iteration:
[0.0057059975442715965, 0.018460730561438654, 0.05120498701181667, 0.02456045532950559, 0.07950657461477459, 0.0674089171018212, 0.026665939410008405, 0.014304201802209749, 0.11292690740457151, 0.2299862333295768, 0.3399854434615654, 0.44998462668760186, -0.006658573512394265, 10.992961717335522, 0.5525645936703437, 0.7708072449663075, 0.21085312486274552, 0.8809014064695455, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0215358148336
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.02215492918600144, 0.022935613129449042, 0.024685838161616597]
###############
Output Layer:
###############
[0.05154391205370558, 0.2948018386705335, 0.07843541620832983]
Weights updated in this iteration:
[0.005132768583232683, 0.017864930906063524, 0.05056600678721012, 0.02211814130430953, 0.07696809530993905, 0.0646864615553479, 0.025818757212197903, 0.013423662095038221, 0.11198255059807219, 0.22998450549732583, 0.33998364759651034, 0.4499827006672595, -0.007566010169107783, 10.99201855067073, 0.5515530710186058, 0.7709219881227332, 0.21097238599172122, 0.8810293110466835, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0212685046208
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022126950823470996, 0.022816634476789645, 0.024644470052618026]
###############
Output Layer:
###############
[0.051496872941600144, 0.29341175181861406, 0.07839928162786534]
Weights updated in this iteration:
[0.004568754858952787, 0.017281042757688984, 0.049937561920534515, 0.01971965951599157, 0.07448509706006659, 0.06201398480026926, 0.02498482058716093, 0.012560339642244657, 0.11105334769293652, 0.2299828332999793, 0.33998191647513804, 0.4499808374433817, -0.008463242216034754, 10.991089702424686, 0.5505533420229541, 0.7710365938965156, 0.2110910301845239, 0.8811570090212618, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.021007943217
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022099435171408194, 0.0226998448962945, 0.02460376811616368]
###############
Output Layer:
###############
[0.051450690163392085, 0.292047004733307, 0.07836390640913253]
Weights updated in this iteration:
[0.004013704724161497, 0.016708692046429118, 0.04931936031952333, 0.017363761416681195, 0.07205576704413373, 0.05939004169040851, 0.024163774614356665, 0.011713702163833325, 0.11013888627210967, 0.22998121549512718, 0.3399802482442887, 0.44997903557092545, -0.009350496987610903, 10.990174792459353, 0.549565138821275, 0.7711510644646088, 0.21120906873088172, 0.881284503609639, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003306407905891604, 0.041448011626370726]
###############
Hidden middle Layer:
###############
[0.022099435171408194, 0.0226998448962945, 0.02460376811616368]
###############
Output Layer:
###############
[0.051450690163392085, 0.292047004733307, 0.07836390640913253]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.004013704724161497, 0.016708692046429118, 0.04931936031952333, 0.017363761416681195, 0.07205576704413373, 0.05939004169040851, 0.024163774614356665, 0.011713702163833325, 0.11013888627210967, 0.22998121549512718, 0.3399802482442887, 0.44997903557092545, -0.009350496987610903, 10.990174792459353, 0.549565138821275, 0.7711510644646088, 0.21120906873088172, 0.881284503609639, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0822078782818
0.569913930495
0.0822078782818
self.sum1_output_product1_level3 =  0.0468514150293
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0468514150293
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0446709867231
self.product1_product2_sum1_level2 =  0.108847453141
0.569913930495
0.108847453141
self.sum1_output_product1_level3 =  0.0620336798442
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0620336798442
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0591466807774
self.product1_product2_sum1_level2 =  0.117532869574
0.569913930495
0.117532869574
self.sum1_output_product1_level3 =  0.0669836196613
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0669836196613
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0638662542569
self.product1_product2_sum1_level2 =  0.120364613662
0.569913930495
0.120364613662
self.sum1_output_product1_level3 =  0.0685974700645
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0685974700645
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0654049973213
self.product1_product2_sum1_level2 =  0.1212878595
0.569913930495
0.1212878595
self.sum1_output_product1_level3 =  0.0691236407292
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0691236407292
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0659066804137
self.product1_product2_sum1_level2 =  0.121588869356
0.569913930495
0.121588869356
self.sum1_output_product1_level3 =  0.0692951904391
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0692951904391
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0660702463339
self.product1_product2_sum1_level2 =  0.121687008908
0.569913930495
0.121687008908
self.sum1_output_product1_level3 =  0.0693511215369
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0693511215369
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0661235744421
self.product1_product2_sum1_level2 =  0.121719005773
0.569913930495
0.121719005773
self.sum1_output_product1_level3 =  0.069369356996
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.069369356996
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0661409612371
self.product1_product2_sum1_level2 =  0.12172943785
0.569913930495
0.12172943785
self.sum1_output_product1_level3 =  0.069375302382
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.069375302382
####################################################################################################################
self.cell_input_product1_level1 =  0.273482870359
self.forget_feedback_product2_level1 =  0.0661466299295
self.product1_product2_sum1_level2 =  0.121732839065
0.569913930495
0.121732839065
self.sum1_output_product1_level3 =  0.0693772407821
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0693772407821
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0693772407821
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09628971068248246,
   0.09628971068248246,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07314485534124124,
   0.07314485534124124,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10290252649426568,
   0.10290252649426568,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07645126324713283,
   0.07645126324713283,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10951534230604887,
   0.10951534230604887,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07975767115302444,
   0.07975767115302444,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.09628971068248246, 0.05, 0.05, 0.05, 0.05],
  [0.09628971068248246, 0.05, 0.05, 0.05, 0.05],
  [0.09628971068248246, 0.05, 0.05, 0.05, 0.05],
  [0.09628971068248246, 0.05, 0.05, 0.05, 0.05],
  [0.09628971068248246, 0.05, 0.05, 0.05, 0.05]],
 [[0.10290252649426568, 0.05, 0.05, 0.05, 0.05],
  [0.10290252649426568, 0.05, 0.05, 0.05, 0.05],
  [0.10290252649426568, 0.05, 0.05, 0.05, 0.05],
  [0.10290252649426568, 0.05, 0.05, 0.05, 0.05],
  [0.10290252649426568, 0.05, 0.05, 0.05, 0.05]],
 [[0.10951534230604887, 0.05, 0.05, 0.05, 0.05],
  [0.10951534230604887, 0.05, 0.05, 0.05, 0.05],
  [0.10951534230604887, 0.05, 0.05, 0.05, 0.05],
  [0.10951534230604887, 0.05, 0.05, 0.05, 0.05],
  [0.10951534230604887, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06436702283822365, 0.06469236374823553, 0.06501802591341173]
Scheduled Classes by Deep Learning for process id  3976  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 4616
Process cmdline: ['/usr/bin/gnome-keyring-daemon', '--daemonize', '--login']
Process executable: /usr/bin/gnome-keyring-daemon
Process name: gnome-keyring-daemon
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 18, 330, 3, 0.10320579590644312, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: gnome-keyring-daemon
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-keyring-daemon
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-keyring-daemon
mongod 0.01
java 0.8
gnome 0.1
Expected output layer: [0.01, 0.02, 0.030000000000000006]
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.023078282787603936, 0.02490217373794738, 0.0265671277625949]
###############
Output Layer:
###############
[0.058320198657505554, 0.32446936566287576, 0.09099973423142399]
Error before Backpropagation:
0.0493787019009
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0482731104707
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.023000131770524724, 0.024590262420644767, 0.026449416906349853]
###############
Output Layer:
###############
[0.05818929498090343, 0.3208653425346948, 0.09086097643110796]
Weights updated in this iteration:
[0.008777074154261886, 0.021680425568944225, 0.054592199103940475, 0.0381191369509503, 0.09373339932796698, 0.0823812755303665, 0.03115803230967401, 0.019012460464830547, 0.11787957399543836, 0.2299387572827465, 0.3399339172329559, 0.44992949895324635, -0.0015401600043454013, 10.998338120198742, 0.5582270072696978, 0.7698835510524944, 0.20987434802021132, 0.8798659469556435, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0472179934899
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022924409837328345, 0.024289174259368097, 0.02633528089944468]
###############
Output Layer:
###############
[0.05806275359590876, 0.3173856770731334, 0.0907265820566941]
Weights updated in this iteration:
[0.007590748131436297, 0.020412082043403827, 0.053227962069230216, 0.033402026520595963, 0.08869016807577805, 0.07695674920110336, 0.02936987781575931, 0.01710068062405729, 0.11582325344152207, 0.22987801548875667, 0.3398689760113571, 0.4498596478302656, -0.003048090039096788, 10.996725938336635, 0.5564929358933826, 0.7697679193448912, 0.20975072203134837, 0.8797329741852045, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0462100860803
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022850990887445688, 0.023998336164775425, 0.02622453756011952]
###############
Output Layer:
###############
[0.05794034419513661, 0.31402377671544995, 0.09059632237184945]
Weights updated in this iteration:
[0.006439162309430152, 0.01919193858480194, 0.05190503439878842, 0.028840193005304604, 0.08385675382514315, 0.07171617080042947, 0.027632854068452936, 0.015260246222894005, 0.11382778186171147, 0.22981775586563744, 0.3398051289375553, 0.44979042231418903, -0.0045250915263504365, 10.99516100616347, 0.5547961746972716, 0.769653075943463, 0.2096290416299936, 0.8796010434939666, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0452463870381
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022779757526859675, 0.023717213699415915, 0.026117017070811228]
###############
Output Layer:
###############
[0.057821852088657644, 0.31077349525033926, 0.09046998428102279]
Weights updated in this iteration:
[0.005320583258699744, 0.01801719582556992, 0.050621316835977676, 0.024425720659006625, 0.0792206313970883, 0.0666499792981081, 0.025944457425712605, 0.013487075404545745, 0.11189012316708351, 0.22975796075675672, 0.3397423315240429, 0.4497217995097567, -0.005972393266259676, 10.993641035591633, 0.5531352043250883, 0.76953899356183, 0.20950923118683198, 0.8794701188563079, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0443241340305
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022710600345164693, 0.023445308067400375, 0.02601256096968804]
###############
Output Layer:
###############
[0.05770707697158045, 0.3076290978230204, 0.09034736906743415]
Weights updated in this iteration:
[0.004233393436535487, 0.016885264841945694, 0.04937485250855029, 0.020151211078494224, 0.0747702127716189, 0.06174924936048886, 0.024302348721732082, 0.011777388942604465, 0.11000744373103513, 0.22969861351837018, 0.339680541966642, 0.4496537578356424, -0.007391156274578203, 10.99216388617872, 0.5515085909748804, 0.7694256464382309, 0.20939121948501974, 0.8793401662543772, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0434407813794
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022643417260290143, 0.023182153352723785, 0.025911021234626706]
###############
Output Layer:
###############
[0.05759583179615682, 0.30458522886545647, 0.09022829124289394]
Weights updated in this iteration:
[0.0031760818282409054, 0.01579374828126689, 0.048163815272338366, 0.016009743994561464, 0.07049476561663098, 0.057005642261273894, 0.022704340223254037, 0.010127683475918354, 0.10817709610195583, 0.22963969844419588, 0.3396197209384509, 0.44958627692647657, -0.008782478278002864, 10.99072755369541, 0.549914980595036, 0.7693130102237331, 0.20927493939077693, 0.8792111535311339, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0425939801467
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.02257811292414791, 0.022927313987214194, 0.025812259450680046]
###############
Output Layer:
###############
[0.057487941739886736, 0.30163688272164385, 0.09011257749941859]
Weights updated in this iteration:
[0.002147235443166874, 0.014740423405387441, 0.04698649913562727, 0.011994841225960723, 0.06638433958825928, 0.0524113608550564, 0.0211483837523748, 0.00853470742007619, 0.10639660419591258, 0.22958120069668422, 0.33955983140243184, 0.4495193375435853, -0.010147397888196371, 10.989330159682222, 0.5483530935089685, 0.7692010618802797, 0.20916032755301023, 0.8790830502563031, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0417815602663
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022514598183124655, 0.02268038242982648, 0.02571614605291897]
###############
Output Layer:
###############
[0.057383243261565534, 0.29877937674843386, 0.0900000657533765]
Weights updated in this iteration:
[0.0011455315822189425, 0.01372322683899084, 0.04584130865675948, 0.008100433558698055, 0.06242969963754624, 0.04795910829294838, 0.0196325598652736, 0.006995439265751449, 0.1046636498260393, 0.22952310624431208, 0.3395008384407558, 0.44945292149356764, -0.011486898475351651, 10.98796994190623, 0.5468217194385957, 0.7690897795875896, 0.20904732412910607, 0.8789558276039567, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0410015144966
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022452789587948437, 0.02244097703994918, 0.025622559637311824]
###############
Output Layer:
###############
[0.057281583237732456, 0.296008326684887, 0.08989060427365422]
Weights updated in this iteration:
[0.00016973080150202706, 0.012740240835777551, 0.04472675021821372, 0.004320830322544853, 0.0586222656282102, 0.04364205017570191, 0.018155067986456573, 0.005507068006536624, 0.1029760604389838, 0.22946540180430286, 0.33944270909922086, 0.4493870115539316, -0.012801911761724684, 10.986645245636163, 0.5453197128978241, 0.7689791426580358, 0.20893587253433485, 0.8788294582405726, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0402519839937
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022392608948014613, 0.022208740128457725, 0.02553138633302135]
###############
Output Layer:
###############
[0.0571828181725166, 0.29331962409673584, 0.08978405088606281]
Weights updated in this iteration:
[-0.0007813294958256615, 0.011789680897007465, 0.04364142409018382, 0.0006506914483753452, 0.05495405763623443, 0.0394537798580083, 0.01671421740756279, 0.004066975467821155, 0.10133179794137516, 0.22940807479023848, 0.33938541224524427, 0.44932159140509265, -0.014093321154845634, 10.985354515661312, 0.5438459889286125, 0.7688691314587202, 0.2088259192125745, 0.8787039162225442, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0010320579590644312, 0.05454545454545454]
###############
Hidden middle Layer:
###############
[0.022392608948014613, 0.022208740128457725, 0.02553138633302135]
###############
Output Layer:
###############
[0.0571828181725166, 0.29331962409673584, 0.08978405088606281]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.0007813294958256615, 0.011789680897007465, 0.04364142409018382, 0.0006506914483753452, 0.05495405763623443, 0.0394537798580083, 0.01671421740756279, 0.004066975467821155, 0.10133179794137516, 0.22940807479023848, 0.33938541224524427, 0.44932159140509265, -0.014093321154845634, 10.985354515661312, 0.5438459889286125, 0.7688691314587202, 0.2088259192125745, 0.8787039162225442, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0804025198547
0.569913930495
0.0804025198547
self.sum1_output_product1_level3 =  0.0458225161122
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0458225161122
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0436899719589
self.product1_product2_sum1_level2 =  0.106453485856
0.569913930495
0.106453485856
self.sum1_output_product1_level3 =  0.0606693245391
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0606693245391
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0578458215037
self.product1_product2_sum1_level2 =  0.114946995583
0.569913930495
0.114946995583
self.sum1_output_product1_level3 =  0.0655098940512
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0655098940512
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.062461114687
self.product1_product2_sum1_level2 =  0.117716171493
0.569913930495
0.117716171493
self.sum1_output_product1_level3 =  0.0670880859783
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0670880859783
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0639658587929
self.product1_product2_sum1_level2 =  0.118619017956
0.569913930495
0.118619017956
self.sum1_output_product1_level3 =  0.067602630755
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.067602630755
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0644564570571
self.product1_product2_sum1_level2 =  0.118913376915
0.569913930495
0.118913376915
self.sum1_output_product1_level3 =  0.067770390026
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.067770390026
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0646164089425
self.product1_product2_sum1_level2 =  0.119009348046
0.569913930495
0.119009348046
self.sum1_output_product1_level3 =  0.0678250853106
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0678250853106
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0646685587513
self.product1_product2_sum1_level2 =  0.119040637931
0.569913930495
0.119040637931
self.sum1_output_product1_level3 =  0.0678429178521
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0678429178521
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0646855613802
self.product1_product2_sum1_level2 =  0.119050839509
0.569913930495
0.119050839509
self.sum1_output_product1_level3 =  0.0678487318732
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0678487318732
####################################################################################################################
self.cell_input_product1_level1 =  0.267465008935
self.forget_feedback_product2_level1 =  0.0646911048212
self.product1_product2_sum1_level2 =  0.119054165573
0.569913930495
0.119054165573
self.sum1_output_product1_level3 =  0.0678506274437
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0678506274437
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0678506274437
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06444881142690204,
   0.06444881142690204,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05722440571345102,
   0.05722440571345102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0665129273450309,
   0.0665129273450309,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.058256463672515454,
   0.058256463672515454,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06857704326315976,
   0.06857704326315976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05928852163157988,
   0.05928852163157988,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.06444881142690204, 0.05, 0.05, 0.05, 0.05],
  [0.06444881142690204, 0.05, 0.05, 0.05, 0.05],
  [0.06444881142690204, 0.05, 0.05, 0.05, 0.05],
  [0.06444881142690204, 0.05, 0.05, 0.05, 0.05],
  [0.06444881142690204, 0.05, 0.05, 0.05, 0.05]],
 [[0.0665129273450309, 0.05, 0.05, 0.05, 0.05],
  [0.0665129273450309, 0.05, 0.05, 0.05, 0.05],
  [0.0665129273450309, 0.05, 0.05, 0.05, 0.05],
  [0.0665129273450309, 0.05, 0.05, 0.05, 0.05],
  [0.0665129273450309, 0.05, 0.05, 0.05, 0.05]],
 [[0.06857704326315976, 0.05, 0.05, 0.05, 0.05],
  [0.06857704326315976, 0.05, 0.05, 0.05, 0.05],
  [0.06857704326315976, 0.05, 0.05, 0.05, 0.05],
  [0.06857704326315976, 0.05, 0.05, 0.05, 0.05],
  [0.06857704326315976, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06280463859072616, 0.06290572792876228, 0.06300684263185315]
Scheduled Classes by Deep Learning for process id  4616  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 4888
Process cmdline: ['gnome-session', '--session=ubuntu']
Process executable: /usr/bin/gnome-session
Process name: gnome-session
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 94, 686, 4, 0.2236808155988189, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: gnome-session
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-session
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-session
mongod 0.01
java 0.8
gnome 0.1
Expected output layer: [0.01, 0.02, 0.030000000000000006]
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.02772491597534283, 0.0322797530453437, 0.0364901216593224]
###############
Output Layer:
###############
[0.09901365428992064, 0.4518119773763317, 0.1687100238195449]
Error before Backpropagation:
0.106812742582
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0953154739778
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.026869859845020827, 0.029905819220670475, 0.035145888027870276]
###############
Output Layer:
###############
[0.09795656707103963, 0.4249810238115912, 0.16742256225690552]
Weights updated in this iteration:
[0.005326316828423184, 0.01755849248651103, 0.04984873636119805, 0.03002422662868461, 0.08389247865093924, 0.07092194828059385, 0.025652503292831287, 0.01244540674461145, 0.11032959923216712, 0.22977983911220906, 0.33974366958967683, 0.44971023545798083, -0.00296518780384854, 10.996547671050601, 0.5560973709063213, 0.769460648966302, 0.20937204072365775, 0.8792901336525518, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0859931927116
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.02613080199866052, 0.02786187451931754, 0.03398000213306134]
###############
Output Layer:
###############
[0.09704296751813174, 0.40187831938567936, 0.16630427877397264]
Weights updated in this iteration:
[0.0012593120680449111, 0.013031966861331143, 0.044529077280670404, 0.018776483476655705, 0.07137388066474828, 0.056209853626303846, 0.019236681435858303, 0.005304676956167268, 0.10193767765563087, 0.2295710082519576, 0.33951124344415917, 0.4494370838273622, -0.0056243926317998995, 10.993588009209635, 0.5526191199714297, 0.7689459396078675, 0.20879917562077635, 0.8786168916787279, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0783510967985
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.025485670402822274, 0.026092167875193892, 0.03295923152916397]
###############
Output Layer:
###############
[0.09624789111873851, 0.38187125859492155, 0.16532437724759574]
Weights updated in this iteration:
[-0.0023134124085159827, 0.009222562118869099, 0.039883173747338255, 0.00897588731212021, 0.06092402992672692, 0.043465343197834794, 0.013583676459781476, -0.0007228194562506861, 0.09458661659156006, 0.22937170353317346, 0.3392987354979371, 0.4491779117296301, -0.008023014915872654, 10.99103048674624, 0.5494999967874807, 0.7684521144354699, 0.2082726362903522, 0.8779747307373114, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0720103925028
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.02491689974764183, 0.024548717944521688, 0.03205693089906133]
###############
Output Layer:
###############
[0.09555028908821166, 0.3644169891552903, 0.1644576908316156]
Weights updated in this iteration:
[-0.005482075485060687, 0.0059784925053824114, 0.03578531411378572, 0.0003772144353261439, 0.052120729379438124, 0.0323451471931649, 0.008556893406703276, -0.0058692278190682285, 0.08808575144363227, 0.22918050476356397, 0.3391029866588707, 0.4489306447589738, -0.010199952930956149, 10.988801742858179, 0.5466846812423252, 0.7679762023597106, 0.2077853986553098, 0.8773592595315496, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0666859026799
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.024410698493649175, 0.0231920371325725, 0.03125205407728597]
###############
Output Layer:
###############
[0.09493304088767612, 0.3490695036754657, 0.1636841264927147]
Weights updated in this iteration:
[-0.008318010277644854, 0.003184462588446493, 0.03213673156134685, -0.007223435082412684, 0.04463239001555743, 0.02256650302611931, 0.004047662809952463, -0.01031182827047147, 0.0822843838934155, 0.2289962868259546, 0.33892149079716655, 0.44869363847985716, -0.012187646654266686, 10.98684342007052, 0.5441274064215403, 0.7675158373713203, 0.20733183619906573, 0.8767669752276978, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0621634574488
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.023956255688769454, 0.02199024970570539, 0.030528021622775518]
###############
Output Layer:
###############
[0.09438242385339424, 0.3354691370140812, 0.16298776272216206]
Weights updated in this iteration:
[-0.010877466630695085, 0.0007527826103179377, 0.028859960584277547, -0.013991993043243528, 0.038201740454161304, 0.013900985344510846, -3.014290203768883e-05, -0.01418605667554953, 0.07706373014354688, 0.2288181494777028, 0.33875224664397574, 0.44846557626188344, -0.014012863086986311, 10.985109324370896, 0.5417906538426273, 0.7670691157690802, 0.206907416390277, 0.8761950551872151, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0582814114231
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.023545057895385118, 0.020917786172944375, 0.029871722819575605]
###############
Output Layer:
###############
[0.09388747794230858, 0.32332756253893685, 0.1623559734005069]
Weights updated in this iteration:
[-0.013204970447117796, -0.0013837111213845371, 0.025893967577776334, -0.0200624608464166, 0.03262945467809988, 0.006165245053405723, -0.0037449925285134205, -0.017596041606693997, 0.07232980965130671, 0.22864536413252884, 0.33859364118679475, 0.448245391821456, -0.015697644430743344, 10.983562807130053, 0.5396436972413928, 0.7666344880452113, 0.20650845704115883, 0.8756411988281703, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0549166911397
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.023170349038958742, 0.019954122765651467, 0.02927271578804775]
###############
Output Layer:
###############
[0.09343942882175413, 0.31241344006706534, 0.16177869351838248]
Weights updated in this iteration:
[-0.015336019733933694, -0.003276967595999439, 0.023190295584551126, -0.02554302043720822, 0.027760442499933667, -0.0007879660076884256, -0.007151673436750635, -0.02062258854128128, 0.06800773797589375, 0.2284773341797047, 0.33844436083083035, 0.4480322114505072, -0.01726019058073528, 10.982174617455765, 0.5376612877823571, 0.7662106781654572, 0.2061319379227066, 0.875103509292113, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0519746032913
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.02282671843209384, 0.01908271399714164, 0.028722611719109458]
###############
Output Layer:
###############
[0.09303121319103443, 0.3025403134989888, 0.16124783985479757]
Weights updated in this iteration:
[-0.01729916199216379, -0.004967610303249825, 0.020710121234132347, -0.03052132953854635, 0.023473161768580247, -0.0070774106415820975, -0.010294386927873569, -0.023329068737746816, 0.06403732912817968, 0.22831356548029016, 0.3383033245120509, 0.44782531104581846, -0.018715606075284, 10.980921225053068, 0.5358225600163743, 0.7657966233943895, 0.2057753572031112, 0.8745804050603309, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0493814765105
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.022509790959103236, 0.018290137435600182, 0.02821460855276986]
###############
Output Layer:
###############
[0.09265710465919258, 0.29355693762664553, 0.16075686577291773]
Weights updated in this iteration:
[-0.019117568646895924, -0.006487764600766582, 0.0184220399031828, -0.03506882578521735, 0.019671539098354944, -0.01279947574593786, -0.013209111642585506, -0.025765724636259434, 0.06036976269729679, 0.22815364433512647, 0.3381696333954617, 0.4476240840069045, -0.02007650624226771, 10.97978353770907, 0.5341101541267136, 0.7653914292102946, 0.2054366223669414, 0.8740705536221917, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.002236808155988189, 0.13702623906705538]
###############
Hidden middle Layer:
###############
[0.022509790959103236, 0.018290137435600182, 0.02821460855276986]
###############
Output Layer:
###############
[0.09265710465919258, 0.29355693762664553, 0.16075686577291773]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.019117568646895924, -0.006487764600766582, 0.0184220399031828, -0.03506882578521735, 0.019671539098354944, -0.01279947574593786, -0.013209111642585506, -0.025765724636259434, 0.06036976269729679, 0.22815364433512647, 0.3381696333954617, 0.4476240840069045, -0.02007650624226771, 10.97978353770907, 0.5341101541267136, 0.7653914292102946, 0.2054366223669414, 0.8740705536221917, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0813590638125
0.569913930495
0.0813590638125
self.sum1_output_product1_level3 =  0.0463676638388
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0463676638388
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0442097489356
self.product1_product2_sum1_level2 =  0.107721896
0.569913930495
0.107721896
self.sum1_output_product1_level3 =  0.0613922091496
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0613922091496
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0585350636284
self.product1_product2_sum1_level2 =  0.116317084815
0.569913930495
0.116317084815
self.sum1_output_product1_level3 =  0.0662907269909
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0662907269909
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0632056082708
self.product1_product2_sum1_level2 =  0.119119411601
0.569913930495
0.119119411601
self.sum1_output_product1_level3 =  0.0678878120637
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0678878120637
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0647283662502
self.product1_product2_sum1_level2 =  0.120033066388
0.569913930495
0.120033066388
self.sum1_output_product1_level3 =  0.0684085166548
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0684085166548
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0652248376559
self.product1_product2_sum1_level2 =  0.120330949232
0.569913930495
0.120330949232
self.sum1_output_product1_level3 =  0.068578284237
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.068578284237
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0653867043872
self.product1_product2_sum1_level2 =  0.120428069271
0.569913930495
0.120428069271
self.sum1_output_product1_level3 =  0.0686336343
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0686336343
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0654394785014
self.product1_product2_sum1_level2 =  0.120459733739
0.569913930495
0.120459733739
self.sum1_output_product1_level3 =  0.0686516803217
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0686516803217
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0654566846754
self.product1_product2_sum1_level2 =  0.120470057444
0.569913930495
0.120470057444
self.sum1_output_product1_level3 =  0.0686575639447
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0686575639447
####################################################################################################################
self.cell_input_product1_level1 =  0.270653488794
self.forget_feedback_product2_level1 =  0.0654622944791
self.product1_product2_sum1_level2 =  0.120473423326
0.569913930495
0.120473423326
self.sum1_output_product1_level3 =  0.0686594822078
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0686594822078
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0686594822078
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08131531418383464,
   0.08131531418383464,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06565765709191733,
   0.06565765709191733,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08578893049581102,
   0.08578893049581102,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06789446524790552,
   0.06789446524790552,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09026254680778739,
   0.09026254680778739,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0701312734038937,
   0.0701312734038937,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.08131531418383464, 0.05, 0.05, 0.05, 0.05],
  [0.08131531418383464, 0.05, 0.05, 0.05, 0.05],
  [0.08131531418383464, 0.05, 0.05, 0.05, 0.05],
  [0.08131531418383464, 0.05, 0.05, 0.05, 0.05],
  [0.08131531418383464, 0.05, 0.05, 0.05, 0.05]],
 [[0.08578893049581102, 0.05, 0.05, 0.05, 0.05],
  [0.08578893049581102, 0.05, 0.05, 0.05, 0.05],
  [0.08578893049581102, 0.05, 0.05, 0.05, 0.05],
  [0.08578893049581102, 0.05, 0.05, 0.05, 0.05],
  [0.08578893049581102, 0.05, 0.05, 0.05, 0.05]],
 [[0.09026254680778739, 0.05, 0.05, 0.05, 0.05],
  [0.09026254680778739, 0.05, 0.05, 0.05, 0.05],
  [0.09026254680778739, 0.05, 0.05, 0.05, 0.05],
  [0.09026254680778739, 0.05, 0.05, 0.05, 0.05],
  [0.09026254680778739, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06363143261920017, 0.06385103215888376, 0.06407076545231574]
Scheduled Classes by Deep Learning for process id  4888  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 5213
Process cmdline: ['/usr/lib/policykit-1-gnome/polkit-gnome-authentication-agent-1']
Process executable: /usr/lib/policykit-1-gnome/polkit-gnome-authentication-agent-1
Process name: polkit-gnome-authentication-agent-1
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 256, 1117, 3, 0.18218007227136024, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: polkit-gnome-authentication-agent-1
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: polkit-gnome-authentication-agent-1
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: polkit-gnome-authentication-agent-1
mongod 0.01
java 0.8
gnome 0.1
Expected output layer: [0.01, 0.02, 0.030000000000000006]
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.03287627921429475, 0.04034866623931946, 0.047540495953046276]
###############
Output Layer:
###############
[0.14441348375766627, 0.5921791066092374, 0.25547103458297216]
Error before Backpropagation:
0.198146551046
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.150894832142
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.028952345755498054, 0.032679069851662045, 0.041499991305674035]
###############
Output Layer:
###############
[0.14068465701653451, 0.5059942311991295, 0.25027435607867426]
Weights updated in this iteration:
[-0.0017607131844846, 0.00856621666569842, 0.038993509274345604, 0.02001288151209921, 0.07078818303536254, 0.0547596504360177, 0.014895553685286913, -0.001219371512163555, 0.09382020175864282, 0.22945399430316094, 0.3393298937059475, 0.4492104525742794, -0.004542942161603077, 10.994424501147826, 0.5534307127019804, 0.768590072819425, 0.20826961314995673, 0.8779611854192712, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.118655835232
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.02599364462484314, 0.02656149373892765, 0.03690779426925331]
###############
Output Layer:
###############
[0.13775402623448021, 0.4373961440178123, 0.24626612819396418]
Weights updated in this iteration:
[-0.010711064594296035, -0.001536217223017007, 0.026164168645897025, 0.0015066338151170401, 0.04989982510094561, 0.02823298744949388, 0.0010037223025199035, -0.01689934894676381, 0.07390779512764502, 0.22899658219015862, 0.338813603850955, 0.44855480280852245, -0.008060104845170803, 10.99045461193793, 0.5483892482893435, 0.7673934248299256, 0.2069189335343239, 0.8762459226360906, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0976135150231
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.02378165861540463, 0.02197876074978329, 0.03345103119360451]
###############
Output Layer:
###############
[0.1355482021079016, 0.3860553744384436, 0.2432326003181558]
Weights updated in this iteration:
[-0.017469828631434346, -0.008442631127120381, 0.016567550849610063, -0.01249599133109652, 0.03559130294033125, 0.008350973380870683, -0.009558481012017925, -0.02769229088535779, 0.058910759021734184, 0.22860214555123456, 0.33841055047131574, 0.4479947510523424, -0.010729994203164915, 10.98772639700349, 0.5445983321416196, 0.766349958603216, 0.20585267206743288, 0.874764328293391, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0834738723481
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.02207161826831165, 0.01851976029835875, 0.03076472191743819]
###############
Output Layer:
###############
[0.133860228690764, 0.34732116273887953, 0.2408725796971382]
Weights updated in this iteration:
[-0.022746862021979113, -0.013319610325041972, 0.009144931115153744, -0.02317016104358706, 0.025726346668465225, -0.006663201609727391, -0.017848194042332177, -0.03535355717145354, 0.04725053575272088, 0.22825229127886978, 0.33808721880324555, 0.44750264970788406, -0.012793319643017366, 10.985819493161127, 0.5416960802521591, 0.765416532500697, 0.2049900094030391, 0.8734513809427776, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0735031621197
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.020695357220607263, 0.015828894417252964, 0.028594204289666265]
###############
Output Layer:
###############
[0.13252432344229353, 0.31719858591581035, 0.23896677949885728]
Weights updated in this iteration:
[-0.02703453837865024, -0.016917295705297673, 0.0031685151957852684, -0.03155342673045297, 0.018692151479383207, -0.018348291233931002, -0.024610339298029752, -0.04102750947145327, 0.0378250592956757, 0.22793533044739767, 0.33782126464657825, 0.44706085102686216, -0.014431037328846047, 10.984445323917656, 0.5394133328536085, 0.7645654800434621, 0.20427591189928035, 0.8722651338871382, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0661452282941
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.019548507230479614, 0.013668269563297782, 0.026779963784916153]
###############
Output Layer:
###############
[0.13143195666050098, 0.2930200734313707, 0.23737592915611158]
Weights updated in this iteration:
[-0.030640390247533657, -0.019675240296849646, -0.0018135908813429738, -0.038346724227646206, 0.01349628132929042, -0.027734403025313846, -0.030314557435511747, -0.04539039461852745, 0.02994369841858166, 0.22764382401506042, 0.3375983052412281, 0.4466580846291069, -0.01576316337618444, 10.983426444063154, 0.537572771034301, 0.7637789934528815, 0.20367436571333236, 0.8711784670563886, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0605052057812
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.018565840749223596, 0.011885046065259383, 0.02522167297612677]
###############
Output Layer:
###############
[0.13051388102953962, 0.2730728113867069, 0.23601205833261102]
Weights updated in this iteration:
[-0.03375758936746172, -0.02185477847597055, -0.006083915901960822, -0.04400343769528724, 0.009541120783962295, -0.035483668993248195, -0.03525774297242099, -0.04884665813897288, 0.02317191128927342, 0.2273728351711209, 0.3374088304888114, 0.4462868505833941, -0.01686880055519415, 10.982653385188442, 0.5360581324348395, 0.7630451234591706, 0.20316124556660242, 0.8701731211431328, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0560466131995
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.01770512995245034, 0.010379454171699415, 0.023854074557360376]
###############
Output Layer:
###############
[0.12972494351778222, 0.25623884049990425, 0.23481790580379025]
Weights updated in this iteration:
[-0.03651173855340704, -0.023617865340050517, -0.009825424354697253, -0.04882111246311803, 0.006457054335449788, -0.04202847453913478, -0.039633858719854594, -0.05164805765261087, 0.017226963156931792, 0.22711893028941146, 0.3372462915951479, 0.44594192109810893, -0.01780147372773754, 10.982056328272504, 0.5347910970191404, 0.7623554747006693, 0.20271976234080344, 0.8692362341100993, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0524327047546
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.016937946095851877, 0.009084370330449232, 0.022633075191253196]
###############
Output Layer:
###############
[0.12903471656180318, 0.24176592860527474, 0.23375480036560572]
Weights updated in this iteration:
[-0.038987732975501226, -0.0250693921406179, -0.013161325062958874, -0.05300084114158276, 0.004006730481020336, -0.04765981188562155, -0.043574488588249013, -0.05395821223698709, 0.011917763002169172, 0.22687961866982087, 0.3371059975676561, 0.4456194971886834, -0.01859860321830758, 10.981589019134503, 0.5337171265737952, 0.7617039017462437, 0.20233778425659038, 0.8683583715992805, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0494429272698
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.01624444639683096, 0.007953175913763958, 0.02152780828826853]
###############
Output Layer:
###############
[0.12842200350004018, 0.22913151752840633, 0.2327956902966482]
Weights updated in this iteration:
[-0.04124505351417575, -0.026280066422600088, -0.01617763561167957, -0.05668284474392404, 0.002031952423962527, -0.05257983370613402, -0.047172098917924454, -0.0558877274423416, 0.007110510420327828, 0.2266530281137584, 0.336984469696862, 0.4453167189731921, -0.019287182517484996, 10.981219711703709, 0.5327970228631547, 0.7610857474375258, 0.202006248034313, 0.8675323722207351, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0018218007227136024, 0.22918531781557744]
###############
Hidden middle Layer:
###############
[0.01624444639683096, 0.007953175913763958, 0.02152780828826853]
###############
Output Layer:
###############
[0.12842200350004018, 0.22913151752840633, 0.2327956902966482]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.04124505351417575, -0.026280066422600088, -0.01617763561167957, -0.05668284474392404, 0.002031952423962527, -0.05257983370613402, -0.047172098917924454, -0.0558877274423416, 0.007110510420327828, 0.2266530281137584, 0.336984469696862, 0.4453167189731921, -0.019287182517484996, 10.981219711703709, 0.5327970228631547, 0.7610857474375258, 0.202006248034313, 0.8675323722207351, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0810296016036
0.569913930495
0.0810296016036
self.sum1_output_product1_level3 =  0.0461798987364
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0461798987364
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0440307222746
self.product1_product2_sum1_level2 =  0.107285017794
0.569913930495
0.107285017794
self.sum1_output_product1_level3 =  0.0611432261743
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0611432261743
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0582976681266
self.product1_product2_sum1_level2 =  0.115845185305
0.569913930495
0.115845185305
self.sum1_output_product1_level3 =  0.0660217848864
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0660217848864
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.062949182522
self.product1_product2_sum1_level2 =  0.118636093943
0.569913930495
0.118636093943
self.sum1_output_product1_level3 =  0.0676123625974
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0676123625974
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0644657359872
self.product1_product2_sum1_level2 =  0.119546026022
0.569913930495
0.119546026022
self.sum1_output_product1_level3 =  0.0681309455652
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0681309455652
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0649601845082
self.product1_product2_sum1_level2 =  0.119842695134
0.569913930495
0.119842695134
self.sum1_output_product1_level3 =  0.0683000214252
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0683000214252
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0651213917096
self.product1_product2_sum1_level2 =  0.119939419455
0.569913930495
0.119939419455
self.sum1_output_product1_level3 =  0.068355145963
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.068355145963
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0651739507944
self.product1_product2_sum1_level2 =  0.119970954906
0.569913930495
0.119970954906
self.sum1_output_product1_level3 =  0.0683731184558
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0683731184558
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0651910868614
self.product1_product2_sum1_level2 =  0.119981236546
0.569913930495
0.119981236546
self.sum1_output_product1_level3 =  0.0683789781058
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0683789781058
####################################################################################################################
self.cell_input_product1_level1 =  0.269555281431
self.forget_feedback_product2_level1 =  0.0651966738078
self.product1_product2_sum1_level2 =  0.119984588714
0.569913930495
0.119984588714
self.sum1_output_product1_level3 =  0.0683808885529
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0683808885529
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0683808885529
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07550521011799044,
   0.07550521011799044,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06275260505899521,
   0.06275260505899521,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07914881156341765,
   0.07914881156341765,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06457440578170882,
   0.06457440578170882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08279241300884485,
   0.08279241300884485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06639620650442242,
   0.06639620650442242,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.07550521011799044, 0.05, 0.05, 0.05, 0.05],
  [0.07550521011799044, 0.05, 0.05, 0.05, 0.05],
  [0.07550521011799044, 0.05, 0.05, 0.05, 0.05],
  [0.07550521011799044, 0.05, 0.05, 0.05, 0.05],
  [0.07550521011799044, 0.05, 0.05, 0.05, 0.05]],
 [[0.07914881156341765, 0.05, 0.05, 0.05, 0.05],
  [0.07914881156341765, 0.05, 0.05, 0.05, 0.05],
  [0.07914881156341765, 0.05, 0.05, 0.05, 0.05],
  [0.07914881156341765, 0.05, 0.05, 0.05, 0.05],
  [0.07914881156341765, 0.05, 0.05, 0.05, 0.05]],
 [[0.08279241300884485, 0.05, 0.05, 0.05, 0.05],
  [0.08279241300884485, 0.05, 0.05, 0.05, 0.05],
  [0.08279241300884485, 0.05, 0.05, 0.05, 0.05],
  [0.08279241300884485, 0.05, 0.05, 0.05, 0.05],
  [0.08279241300884485, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06334642201502065, 0.06352513103947843, 0.06370392542503298]
Scheduled Classes by Deep Learning for process id  5213  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 5759
Process cmdline: ['/usr/lib/gnome-terminal/gnome-terminal-server', '--app-id', 'com.canonical.Terminal.dbwJthHkPjaRKiUjhhnxygBExzoXzIwI']
Process executable: /usr/lib/gnome-terminal/gnome-terminal-server
Process name: gnome-terminal-server
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 83966, 221351, 4, 0.4268842907400101, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: gnome-terminal-server
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-terminal-server
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-terminal-server
mongod 0.01
java 0.8
gnome 0.1
Expected output layer: [0.01, 0.02, 0.030000000000000006]
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[0.04134089744734429, 0.05380402325714903, 0.06560974725921416]
###############
Output Layer:
###############
[0.2185021551634152, 0.8242713964333802, 0.39694541401162065]
Error before Backpropagation:
0.412487282346
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.303815838909
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[0.02695381537058795, 0.04160003168972512, 0.04544110675953843]
###############
Output Layer:
###############
[0.21003830308582266, 0.6867375439378398, 0.3808238911759166]
Weights updated in this iteration:
[-0.013679511700671101, -0.007818223041325987, 0.01841961655812003, 0.022913608640449255, 0.07285812271642181, 0.05612205003221462, -0.000195303686372994, -0.02220276050718564, 0.0673176534730573, 0.22852811473282653, 0.3380843824387769, 0.44766405602353426, -0.0048160968409088345, 10.99373198448904, 0.5523566512577432, 0.7663686444110995, 0.20527389213528718, 0.8742368855756081, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.177242943887
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[0.01226956254061887, 0.022560929801542156, 0.02457287288516875]
###############
Output Layer:
###############
[0.19963146624627542, 0.4748429921094066, 0.3641311827902327]
Weights updated in this iteration:
[-0.03640691909358649, -0.04289529301347775, -0.019896240826431877, -0.0065539734025977375, 0.027378388958655485, 0.006443014869544551, -0.032493906708617154, -0.07205184089843043, 0.012865834762475548, 0.22763349608621283, 0.3367036441819601, 0.4461558294192144, -0.00868220766908415, 10.987765098882797, 0.5458388224282475, 0.7641389374626916, 0.20183260271346434, 0.8704778499774177, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.104257414856
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[0.0014677312683857545, 0.005702482036023089, 0.008919649387852795]
###############
Output Layer:
###############
[0.19120943539955165, 0.28862622483092665, 0.3517420108843123]
Weights updated in this iteration:
[-0.05047984494078164, -0.06877219702613406, -0.04808080053022794, -0.028517627464925567, -0.013007766280791262, -0.03754470495181587, -0.052887361114219926, -0.10955075675077572, -0.0279771668636513, 0.2272617396600685, 0.3360200688217344, 0.4454112940395994, -0.010073856902247649, 10.98520617304059, 0.5430516961884929, 0.7631897073950534, 0.2000871849081759, 0.8685767789560482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0608889215091
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.015438291936457415, -0.01674303972422805, -0.016007245670247788]
###############
Output Layer:
###############
[0.179733688877014, 0.042047747988362444, 0.33410888623507523]
Weights updated in this iteration:
[-0.05776107704476427, -0.09706150050247235, -0.09233007073248502, -0.03818465865559388, -0.05056646172719263, -0.09629287684096044, -0.06362309159030569, -0.15126160089651375, -0.09322000377240562, 0.2272206083304384, 0.33586026390827006, 0.445161332042428, -0.01015480909090064, 10.984891654700949, 0.5425597361440918, 0.7630820295295097, 0.19966883101249575, 0.8679224024852524, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0604507747981
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.01703056721850679, -0.017178070912563844, -0.018410086683782276]
###############
Output Layer:
###############
[0.1793639515579406, 0.037290050341955205, 0.3331805661122157]
Weights updated in this iteration:
[-0.06176233615050851, -0.10140092097917683, -0.09647878988869636, -0.03927785687529554, -0.0517520502167908, -0.0974263631450844, -0.06966123687425822, -0.15781005241990656, -0.09948067531241118, 0.22760693230650222, 0.3362792375683279, 0.4455618933751592, -0.010141098692063906, 10.984906523816866, 0.542573951817906, 0.7641265563737011, 0.20080163472661774, 0.8690054237076104, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0600507555294
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.018673495974189307, -0.01757877281760919, -0.020889910847416825]
###############
Output Layer:
###############
[0.17901390036013515, 0.032908988378049425, 0.33227333749909704]
Weights updated in this iteration:
[-0.06572724798760299, -0.10540017337199552, -0.10076486971668601, -0.04024487855591651, -0.05272744738298984, -0.09847171605300063, -0.07564584463421122, -0.1638464935533902, -0.10595005159422072, 0.22803148914882132, 0.3367074715461044, 0.4460208404066353, -0.010130527747373429, 10.98491718631772, 0.5425853790361298, 0.7652737015482108, 0.2019587154569249, 0.870245490572715, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0596871908024
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.02035723485780291, -0.01794338742322864, -0.02343174022102938]
###############
Output Layer:
###############
[0.17868674443303764, 0.02892375413976328, 0.3313959695641303]
Weights updated in this iteration:
[-0.06965775419249544, -0.10910025587017953, -0.10516189983460887, -0.0410960318931287, -0.0535287023364535, -0.09942389540690524, -0.08157947022306752, -0.16943226369281295, -0.1125879575184509, 0.22849533173090514, 0.3371441216187451, 0.4465397378964444, -0.010122855904114155, 10.9849244084035, 0.5425939614741447, 0.7665260341463767, 0.2031376307637144, 0.8716464663923023, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0593575579631
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.022073930892320587, -0.018271936482026855, -0.026023581771905276]
###############
Output Layer:
###############
[0.17838452152440643, 0.025334078455395564, 0.33055501400625537]
Weights updated in this iteration:
[-0.0735559092821856, -0.11253618945834712, -0.10964878413107276, -0.04184207846166231, -0.05418628686148725, -0.10028261563628696, -0.08746484546135538, -0.17461978394147765, -0.1193621871547505, 0.2289992973530355, 0.33758832980659764, 0.44711981626271435, -0.010117753505600117, 10.98492890578804, 0.5425998344759213, 0.7678855129476823, 0.20433591014673885, 0.8732112640836733, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0590590880162
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.02381735846123059, -0.018565660203273823, -0.028655890181763757]
###############
Output Layer:
###############
[0.17810839571570766, 0.02212637010145141, 0.32975526996624416]
Weights updated in this iteration:
[-0.0774238203433465, -0.11573789492586982, -0.11420877393432052, -0.042493724248729214, -0.054725693752273936, -0.10105085929478065, -0.09330479831999004, -0.17945386846459838, -0.12624707283741896, 0.22954406038195954, 0.3380392633313429, 0.44776205281309006, -0.010114846137838775, 10.98493131239334, 0.5426032620540472, 0.769353635460201, 0.20555116453108183, 0.8749420752582702, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0587891178865
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.025582602618409635, -0.018826575026981412, -0.03132109751540195]
###############
Output Layer:
###############
[0.17785889948260133, 0.019278549856060673, 0.3290001758313153]
Weights updated in this iteration:
[-0.08126361110248326, -0.11873101654352719, -0.11882862388509838, -0.04306127085406408, -0.05516809702891439, -0.10173370383532793, -0.09910220512588014, -0.18397295419753157, -0.13322223144029338, 0.23013017416260462, 0.3384961397439654, 0.4484672364781687, -0.01011375035282323, 10.984932166559107, 0.542604580449415, 0.770931557611651, 0.20678115678814624, 0.8768405546367003, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.004268842907400101, 0.379334179651323]
###############
Hidden middle Layer:
###############
[-0.025582602618409635, -0.018826575026981412, -0.03132109751540195]
###############
Output Layer:
###############
[0.17785889948260133, 0.019278549856060673, 0.3290001758313153]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.08126361110248326, -0.11873101654352719, -0.11882862388509838, -0.04306127085406408, -0.05516809702891439, -0.10173370383532793, -0.09910220512588014, -0.18397295419753157, -0.13322223144029338, 0.23013017416260462, 0.3384961397439654, 0.4484672364781687, -0.01011375035282323, 10.984932166559107, 0.542604580449415, 0.770931557611651, 0.20678115678814624, 0.8768405546367003, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0829711415504
0.569913930495
0.0829711415504
self.sum1_output_product1_level3 =  0.0472864093987
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0472864093987
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0450857367939
self.product1_product2_sum1_level2 =  0.109859566453
0.569913930495
0.109859566453
self.sum1_output_product1_level3 =  0.0626104973195
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0626104973195
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0596966536173
self.product1_product2_sum1_level2 =  0.118626116547
0.569913930495
0.118626116547
self.sum1_output_product1_level3 =  0.0676066763405
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0676066763405
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0644603143642
self.product1_product2_sum1_level2 =  0.121484312995
0.569913930495
0.121484312995
self.sum1_output_product1_level3 =  0.0692356023124
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0692356023124
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0660134313921
self.product1_product2_sum1_level2 =  0.122416183211
0.569913930495
0.122416183211
self.sum1_output_product1_level3 =  0.0697666881303
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0697666881303
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0665198008904
self.product1_product2_sum1_level2 =  0.12272000491
0.569913930495
0.12272000491
self.sum1_output_product1_level3 =  0.0699398403489
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0699398403489
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0666848947399
self.product1_product2_sum1_level2 =  0.12281906122
0.569913930495
0.12281906122
self.sum1_output_product1_level3 =  0.0699962939197
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0699962939197
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0667387210055
self.product1_product2_sum1_level2 =  0.12285135698
0.569913930495
0.12285135698
self.sum1_output_product1_level3 =  0.0700146997229
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0700146997229
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.066756270217
self.product1_product2_sum1_level2 =  0.122861886506
0.569913930495
0.122861886506
self.sum1_output_product1_level3 =  0.0700207006469
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0700207006469
####################################################################################################################
self.cell_input_product1_level1 =  0.276027081254
self.forget_feedback_product2_level1 =  0.0667619918627
self.product1_product2_sum1_level2 =  0.122865319494
0.569913930495
0.122865319494
self.sum1_output_product1_level3 =  0.0700226571543
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0700226571543
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0700226571543
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10976380070360142,
   0.10976380070360142,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0798819003518007,
   0.0798819003518007,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11830148651840162,
   0.11830148651840162,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08415074325920081,
   0.08415074325920081,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1268391723332018,
   0.1268391723332018,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08841958616660091,
   0.08841958616660091,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.10976380070360142, 0.05, 0.05, 0.05, 0.05],
  [0.10976380070360142, 0.05, 0.05, 0.05, 0.05],
  [0.10976380070360142, 0.05, 0.05, 0.05, 0.05],
  [0.10976380070360142, 0.05, 0.05, 0.05, 0.05],
  [0.10976380070360142, 0.05, 0.05, 0.05, 0.05]],
 [[0.11830148651840162, 0.05, 0.05, 0.05, 0.05],
  [0.11830148651840162, 0.05, 0.05, 0.05, 0.05],
  [0.11830148651840162, 0.05, 0.05, 0.05, 0.05],
  [0.11830148651840162, 0.05, 0.05, 0.05, 0.05],
  [0.11830148651840162, 0.05, 0.05, 0.05, 0.05]],
 [[0.1268391723332018, 0.05, 0.05, 0.05, 0.05],
  [0.1268391723332018, 0.05, 0.05, 0.05, 0.05],
  [0.1268391723332018, 0.05, 0.05, 0.05, 0.05],
  [0.1268391723332018, 0.05, 0.05, 0.05, 0.05],
  [0.1268391723332018, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06503026818726855, 0.06545123476613271, 0.0658727812418006]
Scheduled Classes by Deep Learning for process id  5759  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 5794
Process cmdline: ['gnome-pty-helper']
Process executable: /usr/lib/libvte-2.91-0/gnome-pty-helper
Process name: gnome-pty-helper
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 1, 3, 1, 0.02866827664067865, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: gnome-pty-helper
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-pty-helper
mongod 0.01
java 0.8
gnome 0.1
get_expected_priority(): proc_name: gnome-pty-helper
mongod 0.01
java 0.8
gnome 0.1
Expected output layer: [0.01, 0.02, 0.030000000000000006]
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.03867326037029402, 0.0493617149272076, 0.06000602033809453]
###############
Output Layer:
###############
[0.19567783296041819, 0.7496455308659503, 0.35347770395317335]
Error before Backpropagation:
0.33574834166
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.233915074433
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.027716641893848697, 0.03629651150928662, 0.04409674481944979]
###############
Output Layer:
###############
[0.1880474306940925, 0.6027971349243293, 0.3406070842938791]
Weights updated in this iteration:
[-0.01116930534568857, -0.0040200444874709425, 0.023153383113970326, 0.01775669330349496, 0.06677998810956008, 0.048832100512664296, 0.0022616556826442237, -0.01823375932103063, 0.07230591637204184, 0.22886983328131116, 0.33855747959044624, 0.44824641609065957, -0.005295829792738862, 10.993240517142098, 0.5517829097622639, 0.7671410887686111, 0.20635094740255183, 0.8755640697512201, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.148784757636
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.018561153512859894, 0.021939402319667536, 0.030629417832006626]
###############
Output Layer:
###############
[0.18065608098081998, 0.442780462491265, 0.3295040865036079]
Weights updated in this iteration:
[-0.028420910707240434, -0.026611998265904976, -0.004293651857572873, -0.00929627990395596, 0.031352588313245305, 0.005791242218747647, -0.023114708831648845, -0.05146554512626954, 0.031932516350574175, 0.2281163486715958, 0.33757074884170163, 0.44704763383243173, -0.009163429962862086, 10.988175675787009, 0.5456296182337026, 0.7652075631929907, 0.203818886210659, 0.8724878607271777, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.10556623469
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.012650205828248877, 0.012086751737997326, 0.021806845321647235]
###############
Output Layer:
###############
[0.1757111968764472, 0.3334548243213077, 0.32226416427669546]
Weights updated in this iteration:
[-0.039160239287968454, -0.039305953424050355, -0.022015577496861782, -0.027197106917386134, 0.01019369614470006, -0.023748511857040046, -0.039144034444312095, -0.07041231065382778, 0.005481093952963707, 0.22764748611610847, 0.33701655029847327, 0.4462739218372233, -0.011099560599740307, 10.985887156975046, 0.5424346360784608, 0.7639793757427726, 0.20236716077906716, 0.8704611188782571, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0846909172075
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.008651841221627526, 0.00595096648421968, 0.01577741859872466]
###############
Output Layer:
###############
[0.17255430226464566, 0.26557310101901865, 0.317492235175378]
Weights updated in this iteration:
[-0.046115307948806375, -0.04595123567207131, -0.03400495605303059, -0.03787017251463313, -3.979364200490387e-06, -0.042147097124895554, -0.049632091680006006, -0.08043321827721675, -0.012598567751245883, 0.22734386757893163, 0.33672645526510114, 0.445750534110028, -0.011980891799439553, 10.985045081236963, 0.5409153680594325, 0.7631718699910731, 0.20159562222227922, 0.8690691136205774, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0726935796302
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.005482619739461991, 0.0016216438044368599, 0.010967607702030065]
###############
Output Layer:
###############
[0.17025261537482456, 0.2177986320841046, 0.31386961738022373]
Weights updated in this iteration:
[-0.05132732200287712, -0.049536197549726095, -0.04350953725916154, -0.044990056933760854, -0.004901225217930643, -0.05513085329627665, -0.057542173740242855, -0.08587398308346708, -0.027023321120811596, 0.2271430636050744, 0.33658833695158713, 0.4453843498180749, -0.012395293944416557, 10.984760044420803, 0.5401596680943005, 0.7626328866215979, 0.201224895126724, 0.8680862284727658, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.064720989685
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.0026203993718626195, -0.0018300884627566044, 0.006604265121007778]
###############
Output Layer:
###############
[0.16835764814108192, 0.17979408187947232, 0.31076802888050536]
Weights updated in this iteration:
[-0.055619179727688164, -0.050805638951775124, -0.05209510658104086, -0.05016587915997156, -0.006432124759547372, -0.06548473345031, -0.06408494234546531, -0.08780919666323728, -0.040111684486730546, 0.2270189461289595, 0.3365516256081548, 0.44513606125317656, -0.0125800442341222, 10.984705399164172, 0.5397900877118159, 0.7622977182985078, 0.2011257593740344, 0.8674157470502917, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0583970053315
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[-0.0005146122404754948, -0.005132019485082139, 0.0018083503839166348]
###############
Output Layer:
###############
[0.16648568218313636, 0.1435107159011212, 0.30758124034652823]
Weights updated in this iteration:
[-0.05935174070850204, -0.04819881616709882, -0.06150238341155754, -0.054097175250476096, -0.0036865052573573486, -0.07539288788266976, -0.06979498314548198, -0.08382130080957889, -0.05450285848105039, 0.22696084614961098, 0.3365922026679091, 0.4449896302658763, -0.012641792742115791, 10.984748524358478, 0.5396344612367087, 0.7621401325686168, 0.20123581734048995, 0.8670185793701805, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0651061986308
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.003373277946087258, -0.0016882799218149239, 0.0077753045924232305]
###############
Output Layer:
###############
[0.1685139768783882, 0.18128092282567967, 0.311201316370247]
Weights updated in this iteration:
[-0.06267905700180297, -0.08138079418315566, -0.0498101747481118, -0.05704438060065454, -0.03307779076132964, -0.06503639126775852, -0.07490159444534927, -0.13474746741114374, -0.036558196892559104, 0.2269720210760738, 0.33670364568430267, 0.4449503615097724, -0.012633980224390952, 10.984826435434115, 0.5396070080043215, 0.7621705553483933, 0.20153921140367664, 0.8669116735464133, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0591392807288
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[0.0004843135580882875, -0.004748398958190344, 0.003355683465653868]
###############
Output Layer:
###############
[0.16677013073565053, 0.1476402752604153, 0.30822589329217975]
Weights updated in this iteration:
[-0.06643984860830411, -0.0794985689230787, -0.0584786867167424, -0.061027977641414995, -0.031084054399249487, -0.0742184630864521, -0.0806549620211199, -0.131867984867388, -0.049819536766931585, 0.22689709886766618, 0.3367411432348082, 0.4447776680642821, -0.012714726433136076, 10.984866847817905, 0.5394208904125574, 0.7619672245292781, 0.20164097572457892, 0.8664430021007411, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0495741191022
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[-0.007301596859909956, -0.011799453435309196, -0.008591340242843755]
###############
Output Layer:
###############
[0.1626036393603425, 0.07026556626609606, 0.30080203157707636]
Weights updated in this iteration:
[-0.0698150924434923, -0.046406362349492865, -0.08186487886671819, -0.06408465654956896, -0.0011151825793224053, -0.09539740119504692, -0.08583407608178759, -0.08108993127108471, -0.08570427947105767, 0.2268865483675469, 0.33684458445163573, 0.44470456637589584, -0.012722505761902538, 10.984943119390305, 0.5393669894542226, 0.7619384930759426, 0.20192267009552284, 0.8662439292866689, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00028668276640678647, 0.3333333333333333]
###############
Hidden middle Layer:
###############
[-0.007301596859909956, -0.011799453435309196, -0.008591340242843755]
###############
Output Layer:
###############
[0.1626036393603425, 0.07026556626609606, 0.30080203157707636]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.0698150924434923, -0.046406362349492865, -0.08186487886671819, -0.06408465654956896, -0.0011151825793224053, -0.09539740119504692, -0.08583407608178759, -0.08108993127108471, -0.08570427947105767, 0.2268865483675469, 0.33684458445163573, 0.44470456637589584, -0.012722505761902538, 10.984943119390305, 0.5393669894542226, 0.7619384930759426, 0.20192267009552284, 0.8662439292866689, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0798105723027
0.569913930495
0.0798105723027
self.sum1_output_product1_level3 =  0.0454851569561
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0454851569561
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0433683132348
self.product1_product2_sum1_level2 =  0.105668543069
0.569913930495
0.105668543069
self.sum1_output_product1_level3 =  0.0602219747104
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0602219747104
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0574192909871
self.product1_product2_sum1_level2 =  0.114099129721
0.569913930495
0.114099129721
self.sum1_output_product1_level3 =  0.0650266834853
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0650266834853
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0620003923638
self.product1_product2_sum1_level2 =  0.116847790547
0.569913930495
0.116847790547
self.sum1_output_product1_level3 =  0.0665931835802
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0665931835802
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0634939887664
self.product1_product2_sum1_level2 =  0.117743948388
0.569913930495
0.117743948388
self.sum1_output_product1_level3 =  0.067103916418
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.067103916418
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0639809524964
self.product1_product2_sum1_level2 =  0.118036126626
0.569913930495
0.118036126626
self.sum1_output_product1_level3 =  0.067270432866
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.067270432866
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0641397193988
self.product1_product2_sum1_level2 =  0.118131386768
0.569913930495
0.118131386768
self.sum1_output_product1_level3 =  0.0673247229477
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0673247229477
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0641914828624
self.product1_product2_sum1_level2 =  0.118162444846
0.569913930495
0.118162444846
self.sum1_output_product1_level3 =  0.0673424233791
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0673424233791
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0642083595295
self.product1_product2_sum1_level2 =  0.118172570846
0.569913930495
0.118172570846
self.sum1_output_product1_level3 =  0.0673481943277
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0673481943277
####################################################################################################################
self.cell_input_product1_level1 =  0.265491850428
self.forget_feedback_product2_level1 =  0.0642138619027
self.product1_product2_sum1_level2 =  0.11817587227
0.569913930495
0.11817587227
self.sum1_output_product1_level3 =  0.0673500758552
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0673500758552
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0673500758552
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054013558729695016,
   0.054013558729695016,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05200677936484751,
   0.05200677936484751,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.054586924262508586,
   0.054586924262508586,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.0522934621312543,
   0.0522934621312543,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05516028979532216,
   0.05516028979532216,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05258014489766108,
   0.05258014489766108,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.054013558729695016, 0.05, 0.05, 0.05, 0.05],
  [0.054013558729695016, 0.05, 0.05, 0.05, 0.05],
  [0.054013558729695016, 0.05, 0.05, 0.05, 0.05],
  [0.054013558729695016, 0.05, 0.05, 0.05, 0.05],
  [0.054013558729695016, 0.05, 0.05, 0.05, 0.05]],
 [[0.054586924262508586, 0.05, 0.05, 0.05, 0.05],
  [0.054586924262508586, 0.05, 0.05, 0.05, 0.05],
  [0.054586924262508586, 0.05, 0.05, 0.05, 0.05],
  [0.054586924262508586, 0.05, 0.05, 0.05, 0.05],
  [0.054586924262508586, 0.05, 0.05, 0.05, 0.05]],
 [[0.05516028979532216, 0.05, 0.05, 0.05, 0.05],
  [0.05516028979532216, 0.05, 0.05, 0.05, 0.05],
  [0.05516028979532216, 0.05, 0.05, 0.05, 0.05],
  [0.05516028979532216, 0.05, 0.05, 0.05, 0.05],
  [0.05516028979532216, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06229395230127362, 0.06232199608354791, 0.06235004167752407]
Scheduled Classes by Deep Learning for process id  5794  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6299
Process cmdline: ['/opt/google/chrome/chrome', '', '', '', '']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 103595, 391285, 42, 2.9320821223261713, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.03550070752327309, 0.04620127772774845, 0.0523864414638415]
###############
Output Layer:
###############
[0.16301373797586108, 0.6764773413566136, 0.29002297738220284]
Error before Backpropagation:
0.218873079189
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.154023379641
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.029288318086812423, 0.036236351357704186, 0.04301097602724943]
###############
Output Layer:
###############
[0.15789584825136563, 0.5646257066027532, 0.28246378235233965]
Weights updated in this iteration:
[-0.00448635080313615, 0.004147194988197853, 0.034623288905561586, 0.019763301689256982, 0.06875933081258317, 0.05371088237971962, 0.011137835265116882, -0.007451825755415184, 0.08773920597481033, 0.2293557179340573, 0.3391615193966455, 0.4490492683924234, -0.004789741601696355, 10.993766541643176, 0.5529320417662184, 0.7685378443847757, 0.20809712362448315, 0.8778423773808518, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.107781271516
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.024555009356184786, 0.027485471069554818, 0.035795999703036416]
###############
Output Layer:
###############
[0.15361945762481236, 0.4668178857341446, 0.2764939156545337]
Weights updated in this iteration:
[-0.015621443175353425, -0.009629462258017925, 0.018270994804443764, -0.0008231138378446551, 0.043289224129617276, 0.023478971414444706, -0.0058353715138162975, -0.02845156642626391, 0.0628134254055617, 0.22885765135785124, 0.3385452970720506, 0.44831783924358404, -0.00842292426186901, 10.98927146333139, 0.5475965789432553, 0.7673953605963876, 0.20668360974998204, 0.8761645977730154, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0806658698555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.021285155367785587, 0.021225972827539012, 0.03076511657211391]
###############
Output Layer:
###############
[0.15058189732591204, 0.3969864773251557, 0.27229607743497375]
Weights updated in this iteration:
[-0.023429527724544503, -0.0183693848893801, 0.006888462297538872, -0.01577017009251845, 0.026558345460276918, 0.0016893311218089575, -0.017848617665695038, -0.0418985061023727, 0.04530065848100576, 0.2284629770682618, 0.3381035212775048, 0.44774248777260683, -0.010909279634865094, 10.98648837953444, 0.54397199974223, 0.7664792824178669, 0.20565820430590556, 0.8748291499329995, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0646823543786
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.01895728473472205, 0.016869175785503406, 0.027158826424924537]
###############
Output Layer:
###############
[0.14843964440419083, 0.3484274543241834, 0.26929406845342313]
Weights updated in this iteration:
[-0.02908090806339028, -0.02400505178669476, -0.0012799241630590506, -0.02634718274691085, 0.016010741775013045, -0.013598460847499069, -0.02660362189339046, -0.05062916738395796, 0.03264635922876489, 0.2281346904237135, 0.3377761474211048, 0.4472679891507231, -0.012626365697874158, 10.98477606776193, 0.5414901596062272, 0.76571041713831, 0.2048914768261235, 0.8737178482043959, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0545044228568
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.017187422672071618, 0.01369047810561733, 0.02440368751245321]
###############
Output Layer:
###############
[0.14684620229020084, 0.3130252266699258, 0.2670142519479083]
Weights updated in this iteration:
[-0.03344674331667508, -0.027889998569359038, -0.007534562604322975, -0.03418828555556641, 0.009033321565190232, -0.024831881305135455, -0.03339990440918775, -0.05667685227402767, 0.022909782510939152, 0.22785087331734782, 0.33752359223056916, 0.4468613834771747, -0.013867697479577827, 10.98367146632102, 0.539711787077313, 0.7650415937130558, 0.20429632303232034, 0.8727596698803591, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0475327033546
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.01576519786545663, 0.011249306387449825, 0.02218187107886026]
###############
Output Layer:
###############
[0.145597391625779, 0.28585725023113967, 0.26518890454729144]
Weights updated in this iteration:
[-0.0370087991981936, -0.030727320312551896, -0.01259217364310454, -0.04030236121790242, 0.0041632127310534455, -0.03351299625400983, -0.03896459028593799, -0.061109349898239795, 0.01500872265850192, 0.22759927055406035, 0.3373231804417966, 0.4465041434575576, -0.014802876397897595, 10.98292655844563, 0.538383966465326, 0.764446138688381, 0.20382201899420316, 0.8719142087180234, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0424788860682
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.014574568297683832, 0.00929544051851542, 0.020316816446184292]
###############
Output Layer:
###############
[0.14457806149681587, 0.26412951707356647, 0.26366859307140245]
Weights updated in this iteration:
[-0.04003502237554196, -0.03288669135307563, -0.016850115278133675, -0.04526850210508011, 0.0006196073237586947, -0.040500431353711425, -0.04370499965354911, -0.06449188387169781, 0.008338895210045355, 0.22737256423752455, 0.33716141343852485, 0.4461851642522805, -0.015529764742957195, 10.982407884986989, 0.5373612236049801, 0.7639079468870148, 0.20343799054619696, 0.8711569647395153, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.038654874508
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.01354739814928772, 0.007681164005124879, 0.018704347592397853]
###############
Output Layer:
###############
[0.14372020683666872, 0.2461927437663688, 0.26236509432530847]
Weights updated in this iteration:
[-0.04268395429609549, -0.03457614030292996, -0.020542702427824907, -0.0494315013818767, -0.0020354910205425255, -0.04630361471014164, -0.04786333722241516, -0.0671440090487739, 0.0025422102323263357, 0.227166035756164, 0.3370296926784546, 0.44589726541637614, -0.01610802031739182, 10.982039082276208, 0.536555140561914, 0.7634165315434179, 0.20312457322892274, 0.8704719361931617, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0356649153958
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.012641106962140318, 0.006314474718476673, 0.017279128135495498]
###############
Output Layer:
###############
[0.14298133374435856, 0.23101959932721436, 0.261223087411789]
Weights updated in this iteration:
[-0.04505543720684233, -0.035920734222150905, -0.023816913474347702, -0.05300770330632493, -0.004063141879727932, -0.05124113283121337, -0.051592694446715416, -0.06925849658046675, -0.0026067632495816615, 0.22697644067555284, 0.33692219521711986, 0.4456354991162732, -0.01657613709696085, 10.981773667344292, 0.5359088305574093, 0.762964620732664, 0.2028683468292231, 0.8698480011178095, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0332666034557
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.011827651283891912, 0.005135163670248966, 0.01599801010646786]
###############
Output Layer:
###############
[0.14233350853806873, 0.21793764059999857, 0.2602060264876035]
Weights updated in this iteration:
[-0.04721577430054028, -0.03699986392197851, -0.02676987808177244, -0.056139661777596585, -0.005627615042196552, -0.055522206625006405, -0.054995027079604686, -0.0709580268082048, -0.0072574114641204245, 0.22680143125437519, 0.33683477466377504, 0.44539627875190924, -0.016960193568788508, 10.98158182379063, 0.5353838637955702, 0.7625469132040694, 0.2026596939290517, 0.8692770367290606, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.02932082122326171, 0.2647558684846084]
###############
Hidden middle Layer:
###############
[0.011827651283891912, 0.005135163670248966, 0.01599801010646786]
###############
Output Layer:
###############
[0.14233350853806873, 0.21793764059999857, 0.2602060264876035]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.04721577430054028, -0.03699986392197851, -0.02676987808177244, -0.056139661777596585, -0.005627615042196552, -0.055522206625006405, -0.054995027079604686, -0.0709580268082048, -0.0072574114641204245, 0.22680143125437519, 0.33683477466377504, 0.44539627875190924, -0.016960193568788508, 10.98158182379063, 0.5353838637955702, 0.7625469132040694, 0.2026596939290517, 0.8692770367290606, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.102224301901
0.569913930495
0.102224301901
self.sum1_output_product1_level3 =  0.0582590536885
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0582590536885
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0555477227783
self.product1_product2_sum1_level2 =  0.135389918394
0.569913930495
0.135389918394
self.sum1_output_product1_level3 =  0.0771606005412
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0771606005412
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0735696063858
self.product1_product2_sum1_level2 =  0.146203048558
0.569913930495
0.146203048558
self.sum1_output_product1_level3 =  0.0833231540542
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0833231540542
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0794453594658
self.product1_product2_sum1_level2 =  0.149728500406
0.569913930495
0.149728500406
self.sum1_output_product1_level3 =  0.0853323581737
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0853323581737
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0813610567929
self.product1_product2_sum1_level2 =  0.150877918802
0.569913930495
0.150877918802
self.sum1_output_product1_level3 =  0.0859874277297
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0859874277297
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0819856399228
self.product1_product2_sum1_level2 =  0.15125266868
0.569913930495
0.15125266868
self.sum1_output_product1_level3 =  0.0862010029056
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0862010029056
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0821892754766
self.product1_product2_sum1_level2 =  0.151374850013
0.569913930495
0.151374850013
self.sum1_output_product1_level3 =  0.0862706357489
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0862706357489
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0822556676617
self.product1_product2_sum1_level2 =  0.151414685324
0.569913930495
0.151414685324
self.sum1_output_product1_level3 =  0.0862933384476
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0862933384476
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0822773137946
self.product1_product2_sum1_level2 =  0.151427673003
0.569913930495
0.151427673003
self.sum1_output_product1_level3 =  0.0863007403072
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0863007403072
####################################################################################################################
self.cell_input_product1_level1 =  0.340204282422
self.forget_feedback_product2_level1 =  0.0822843711774
self.product1_product2_sum1_level2 =  0.151431907433
0.569913930495
0.151431907433
self.sum1_output_product1_level3 =  0.0863031535677
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0863031535677
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0863031535677
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.460491497125664,
   0.460491497125664,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.255245748562832,
   0.255245748562832,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5191331395721874,
   0.5191331395721874,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.2845665697860937,
   0.2845665697860937,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5777747820187109,
   0.5777747820187109,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3138873910093554,
   0.3138873910093554,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.460491497125664, 0.05, 0.05, 0.05, 0.05],
  [0.460491497125664, 0.05, 0.05, 0.05, 0.05],
  [0.460491497125664, 0.05, 0.05, 0.05, 0.05],
  [0.460491497125664, 0.05, 0.05, 0.05, 0.05],
  [0.460491497125664, 0.05, 0.05, 0.05, 0.05]],
 [[0.5191331395721874, 0.05, 0.05, 0.05, 0.05],
  [0.5191331395721874, 0.05, 0.05, 0.05, 0.05],
  [0.5191331395721874, 0.05, 0.05, 0.05, 0.05],
  [0.5191331395721874, 0.05, 0.05, 0.05, 0.05],
  [0.5191331395721874, 0.05, 0.05, 0.05, 0.05]],
 [[0.5777747820187109, 0.05, 0.05, 0.05, 0.05],
  [0.5777747820187109, 0.05, 0.05, 0.05, 0.05],
  [0.5777747820187109, 0.05, 0.05, 0.05, 0.05],
  [0.5777747820187109, 0.05, 0.05, 0.05, 0.05],
  [0.5777747820187109, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.08309744681854662, 0.08635324687943821, 0.08970704537869259]
Scheduled Classes by Deep Learning for process id  6299  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6316
Process cmdline: ['/opt/google/chrome/chrome --type=zygote']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 45, 164, 1, 0.43152582124373906, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.03546510459742265, 0.04457355202644594, 0.053017449690753865]
###############
Output Layer:
###############
[0.16678759150249642, 0.6639676088762712, 0.29813199109971544]
Error before Backpropagation:
0.21340332174
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.150778996884
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.029364331814365874, 0.03477880058210417, 0.04383138684091302]
###############
Output Layer:
###############
[0.1617283960446169, 0.5539157813231976, 0.2906310653996145]
Weights updated in this iteration:
[-0.0046789162926673955, 0.0045511205594864, 0.0340562128634512, 0.019433094351870427, 0.06938044277795291, 0.05276939096191882, 0.010897644703883946, -0.006778868690267947, 0.08695877473757235, 0.22932583311830135, 0.3391526878908998, 0.44899217529063123, -0.004779064900093699, 10.993993535324929, 0.5528556862926051, 0.7684554423873515, 0.20805875606778185, 0.8776910118706089, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.106825290613
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.02472554920582485, 0.026324190487744523, 0.03677719631360332]
###############
Output Layer:
###############
[0.15755309803776113, 0.45931987718944634, 0.28474194229211375]
Weights updated in this iteration:
[-0.015865162978553934, -0.008697750390825501, 0.01735878862486863, -0.0009548742445365938, 0.04523315079899001, 0.022336791844771137, -0.006113265715250529, -0.026926409077945995, 0.06156702396283052, 0.22880142360572941, 0.3385315828721854, 0.44820940262773173, -0.00836278121225566, 10.98974902004866, 0.5475063646394008, 0.7672408445549835, 0.20662019938389656, 0.8758780127605591, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0807576558212
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.02146545059105256, 0.02022437222081222, 0.03177483042103507]
###############
Output Layer:
###############
[0.154553355507222, 0.39120549265655746, 0.2805379093734119]
Weights updated in this iteration:
[-0.02376408725058731, -0.017107383093191944, 0.005609796268799079, -0.015734182559119472, 0.02949827979120941, 0.0003538009417162237, -0.018233546122522586, -0.03983033155707717, 0.04353911573706887, 0.22838281664742102, 0.33808591069648264, 0.4475867596216738, -0.010814792807844295, 10.987138472562611, 0.5438592014939629, 0.7662601818790906, 0.20557613153045748, 0.8744193586595516, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0651304026646
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.019098117019877566, 0.015909474727170376, 0.028118088713782886]
###############
Output Layer:
###############
[0.15240008601598443, 0.34307725928924426, 0.2774734369068873]
Weights updated in this iteration:
[-0.029534699940909785, -0.022544353470132054, -0.002932313708472939, -0.02625217737718461, 0.01958840898207804, -0.015215753505589584, -0.027147220290809473, -0.04822863950173388, 0.0303444015649035, 0.22803346718064552, 0.3377567597373057, 0.4470696253543365, -0.012508012104036521, 10.985543150947889, 0.5413527665242466, 0.7654346746578432, 0.20479835306069472, 0.8731973786291202, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.055062196842
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.01726982390652864, 0.012719583050255445, 0.025280529460910674]
###############
Output Layer:
###############
[0.15077536555611776, 0.3075312551300111, 0.2751107303550293]
Weights updated in this iteration:
[-0.034020444208148884, -0.02628115301837607, -0.00953665879245654, -0.03407862391267731, 0.013068674432835577, -0.026738602473873538, -0.03410921399881375, -0.054028251203396986, 0.020094283372480908, 0.22773150789507982, 0.3375052159009504, 0.44662505178875145, -0.013726445405524541, 10.984528148589458, 0.5395588715749482, 0.7647168712380228, 0.20420039487692068, 0.8721405592465307, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.048116306579
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.01578216967348807, 0.010245362184007777, 0.022963517747155507]
###############
Output Layer:
###############
[0.14948801526996539, 0.2799866325514103, 0.27319691281135705]
Weights updated in this iteration:
[-0.037695061903380116, -0.02898758507034173, -0.014915768701432425, -0.040190135294217604, 0.008567419700834746, -0.035684972996697026, -0.03983240696551611, -0.05824350137888306, 0.011716354958833379, 0.22746444150029474, 0.337308515968931, 0.446234105135333, -0.014636793295470569, 10.983857658510725, 0.5382262536643071, 0.7640793440587642, 0.20373084290306942, 0.8712073114772072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0430598932523
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.014523389055438886, 0.008249451557723025, 0.02099764326918666]
###############
Output Layer:
###############
[0.1484279602274648, 0.2577889103016248, 0.27158786221195047]
Weights updated in this iteration:
[-0.04082598961260679, -0.0310200995481742, -0.019471359947968395, -0.04515450461749131, 0.005344683994377496, -0.04290827533198581, -0.04472206826763643, -0.06141773863737008, 0.004601754975952623, 0.22722468087135836, 0.3371528697851845, 0.4458852464187208, -0.015336701132794794, 10.983403297073917, 0.5372078673222335, 0.7635052574590792, 0.20335816124646305, 0.8703719987101409, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0392247873875
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.013426863223079366, 0.006589584046808936, 0.019281437785747195]
###############
Output Layer:
###############
[0.14752919634713926, 0.239347257921608, 0.2701971804912341]
Weights updated in this iteration:
[-0.04357307733536731, -0.032580476882088684, -0.02344304781031938, -0.049312912496336556, 0.002982660624843042, -0.048920423383274715, -0.049021617693796295, -0.06385993202458495, -0.0016144528749919024, 0.22700728097466838, 0.33702938415042305, 0.44557093372902906, -0.01588631993207336, 10.983091107292141, 0.5364132387794577, 0.7629835316540249, 0.20306181500532197, 0.8696176973348186, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0362227470218
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.012450518308342273, 0.005176198422392817, 0.017750589585509555]
###############
Output Layer:
###############
[0.14675003050883814, 0.22366010329744715, 0.2689700952908175]
Weights updated in this iteration:
[-0.04603765090609256, -0.03379003077520665, -0.02698226029672149, -0.05288070170955427, 0.001231674821890584, -0.05404389150865719, -0.052885916015167636, -0.06575643742182836, -0.007163718286515225, 0.22680881876533296, 0.33693198363424864, 0.4452859351646924, -0.016324733713328883, 10.982875944270752, 0.5357836614979105, 0.7625064322149441, 0.202827665986279, 0.868932566116945, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0338142127225
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.01156643791046348, 0.003950234453824041, 0.016362311654612077]
###############
Output Layer:
###############
[0.14606284054651958, 0.210066842997687, 0.2678701355541282]
Weights updated in this iteration:
[-0.04828727840768504, -0.03472529451574605, -0.030189533561344333, -0.056000284799815135, -6.526564841193754e-05, -0.058491452464895725, -0.056418522575265875, -0.06722508893098386, -0.012200123009792282, 0.2266268072289524, 0.3368563138666022, 0.44502644298917754, -0.016678544181989035, 10.982728850539845, 0.5352792371627376, 0.7620682981300302, 0.20264551542009668, 0.8683079223766504, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.00431525821243739, 0.27439024390243905]
###############
Hidden middle Layer:
###############
[0.01156643791046348, 0.003950234453824041, 0.016362311654612077]
###############
Output Layer:
###############
[0.14606284054651958, 0.210066842997687, 0.2678701355541282]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.04828727840768504, -0.03472529451574605, -0.030189533561344333, -0.056000284799815135, -6.526564841193754e-05, -0.058491452464895725, -0.056418522575265875, -0.06722508893098386, -0.012200123009792282, 0.2266268072289524, 0.3368563138666022, 0.44502644298917754, -0.016678544181989035, 10.982728850539845, 0.5352792371627376, 0.7620682981300302, 0.20264551542009668, 0.8683079223766504, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0830079367014
0.569913930495
0.0830079367014
self.sum1_output_product1_level3 =  0.0473073794678
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0473073794678
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0451057309323
self.product1_product2_sum1_level2 =  0.109908358087
0.569913930495
0.109908358087
self.sum1_output_product1_level3 =  0.0626383043514
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0626383043514
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0597231665317
self.product1_product2_sum1_level2 =  0.118678819446
0.569913930495
0.118678819446
self.sum1_output_product1_level3 =  0.0676367124571
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0676367124571
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0644889526234
self.product1_product2_sum1_level2 =  0.121538291101
0.569913930495
0.121538291101
self.sum1_output_product1_level3 =  0.0692663651872
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0692663651872
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0660427625868
self.product1_product2_sum1_level2 =  0.122470577079
0.569913930495
0.122470577079
self.sum1_output_product1_level3 =  0.0697976879533
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0697976879533
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0665493580058
self.product1_product2_sum1_level2 =  0.122774534331
0.569913930495
0.122774534331
self.sum1_output_product1_level3 =  0.0699709174251
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0699709174251
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0667145255132
self.product1_product2_sum1_level2 =  0.122873634835
0.569913930495
0.122873634835
self.sum1_output_product1_level3 =  0.0700273961831
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0700273961831
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0667683757938
self.product1_product2_sum1_level2 =  0.122905945003
0.569913930495
0.122905945003
self.sum1_output_product1_level3 =  0.0700458101982
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0700458101982
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.066785932835
self.product1_product2_sum1_level2 =  0.122916479228
0.569913930495
0.122916479228
self.sum1_output_product1_level3 =  0.0700518137996
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0700518137996
####################################################################################################################
self.cell_input_product1_level1 =  0.276149731757
self.forget_feedback_product2_level1 =  0.0667916570335
self.product1_product2_sum1_level2 =  0.122919913747
0.569913930495
0.122919913747
self.sum1_output_product1_level3 =  0.0700537711799
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0700537711799
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0700537711799
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11041361497412347,
   0.11041361497412347,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08020680748706174,
   0.08020680748706174,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.11904413139899825,
   0.11904413139899825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08452206569949913,
   0.08452206569949913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12767464782387303,
   0.12767464782387303,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08883732391193652,
   0.08883732391193652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.11041361497412347, 0.05, 0.05, 0.05, 0.05],
  [0.11041361497412347, 0.05, 0.05, 0.05, 0.05],
  [0.11041361497412347, 0.05, 0.05, 0.05, 0.05],
  [0.11041361497412347, 0.05, 0.05, 0.05, 0.05],
  [0.11041361497412347, 0.05, 0.05, 0.05, 0.05]],
 [[0.11904413139899825, 0.05, 0.05, 0.05, 0.05],
  [0.11904413139899825, 0.05, 0.05, 0.05, 0.05],
  [0.11904413139899825, 0.05, 0.05, 0.05, 0.05],
  [0.11904413139899825, 0.05, 0.05, 0.05, 0.05],
  [0.11904413139899825, 0.05, 0.05, 0.05, 0.05]],
 [[0.12767464782387303, 0.05, 0.05, 0.05, 0.05],
  [0.12767464782387303, 0.05, 0.05, 0.05, 0.05],
  [0.12767464782387303, 0.05, 0.05, 0.05, 0.05],
  [0.12767464782387303, 0.05, 0.05, 0.05, 0.05],
  [0.12767464782387303, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06506228866632964, 0.06548787931915169, 0.06591406475808186]
Scheduled Classes by Deep Learning for process id  6316  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6323
Process cmdline: ['/opt/google/chrome/chrome --type=zygote']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 46, 261, 1, 0.29555628060509176, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.029937709745305456, 0.03580217926186034, 0.04121149210628339]
###############
Output Layer:
###############
[0.11836875901803898, 0.5125212898881266, 0.20566627958948885]
Error before Backpropagation:
0.112981621803
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0937860933317
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.028321366896648124, 0.031932482053339, 0.03868117173705021]
###############
Output Layer:
###############
[0.11659595895683778, 0.46890831742296546, 0.2034341857558654]
Weights updated in this iteration:
[0.0034334835875942825, 0.015147174926773982, 0.04696070001353146, 0.027279059203371314, 0.08019949904804415, 0.06635895153452057, 0.022720380025180724, 0.00870671503554171, 0.1058493358025061, 0.22972391590036073, 0.33966983404840495, 0.44961994962908813, -0.0033847387516035858, 10.99595222799037, 0.5553406544612046, 0.7694342931580279, 0.20932347738226295, 0.8792212623059431, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0793934058452
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.027000929301462077, 0.02873997079871884, 0.03660376947666791]
###############
Output Layer:
###############
[0.1151324090114893, 0.43294253432880275, 0.20159368581943957]
Weights updated in this iteration:
[-0.001977099710547669, 0.00904671534456146, 0.03957095422489109, 0.014197526129523784, 0.06545000543655341, 0.04849226348729621, 0.014208082272886658, -0.0008909430224095221, 0.09422328496834378, 0.22947130347045772, 0.33938501227713364, 0.44927493292069093, -0.006268754256625392, 10.992700486162663, 0.5514016814783975, 0.7689136936332754, 0.20873649882851586, 0.8785102302538298, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0685098458109
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.025909429557712138, 0.0261096720105935, 0.034879327263921726]
###############
Output Layer:
###############
[0.11392072974085658, 0.40331186610562686, 0.20006346047910045]
Weights updated in this iteration:
[-0.006486078816959022, 0.004247327649291334, 0.033458363434275, 0.0033317771019240067, 0.053884429071649725, 0.03376212704802728, 0.007084421500623856, -0.008473415473227193, 0.08456610488292465, 0.22923712362478718, 0.33913574967394405, 0.44895746736269837, -0.008740922122884949, 10.99006909405417, 0.5480502908031406, 0.7684287187781149, 0.20822028832998793, 0.8778527748158307, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0601319785785
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.024992516872662053, 0.023923717865328854, 0.033425623356394374]
###############
Output Layer:
###############
[0.11290635212686474, 0.37868310611706535, 0.1987730288700077]
Weights updated in this iteration:
[-0.010302730580077116, 0.0004011786828813494, 0.028320378994992162, -0.0057672634922890514, 0.04471506604122116, 0.021512979919321044, 0.0010333746864848769, -0.014571228135833201, 0.07642017323863487, 0.22901763984149076, 0.33891456959814387, 0.44866199784301375, -0.010881519884464445, 10.987911952564932, 0.5451686137017501, 0.7679723411719699, 0.20776038358438614, 0.8772383983454726, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.053549811243
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.024209454109391022, 0.02208422621766746, 0.03218047315615758]
###############
Output Layer:
###############
[0.11204525655924545, 0.35795275463389054, 0.19766779633769674]
Weights updated in this iteration:
[-0.013585407471939247, -0.0027411153167217132, 0.0239300440020349, -0.013478595211508376, 0.03733350757888154, 0.011199650095993507, -0.00418641864418087, -0.019567798252977685, 0.06943908979041677, 0.22881010756685277, 0.33871591239160137, 0.4483844389368144, -0.012755470443539802, 10.986118141053378, 0.5426623448895572, 0.7675393847043508, 0.20734594239656734, 0.8766593514295463, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0482742233885
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.02353042467743481, 0.020515681199326907, 0.03109805657466782]
###############
Output Layer:
###############
[0.11130395184376148, 0.34027078850719383, 0.19670707240447116]
Weights updated in this iteration:
[-0.016450867658409855, -0.005355030966773035, 0.02012112438765227, -0.02009775351189782, 0.031295412353114026, 0.0024011185372247827, -0.008754146453120015, -0.023734547894565534, 0.0633674269814986, 0.22861249125889424, 0.3385356438390955, 0.44812175700004936, -0.014413243579370955, 10.9846058955982, 0.5404587460948236, 0.767125993753324, 0.20696884098661567, 0.8761098505468579, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0439678100284
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.0229335910615827, 0.019161178656413064, 0.030144638543321178]
###############
Output Layer:
###############
[0.11065738110256412, 0.32499718793808485, 0.19586074492126465]
Weights updated in this iteration:
[-0.018985145470909726, -0.0075646143629296975, 0.016771796193647142, -0.025849248820318565, 0.026280804848807487, -0.005200118114561117, -0.01280255475101624, -0.027264269739492433, 0.058017008023439384, 0.22842325460773596, 0.3383706523925062, 0.4478716598528711, -0.01589370802658401, 10.983315109858362, 0.5385021487206858, 0.7667292437962838, 0.20662292306349847, 0.8755855016335182, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0403947162243
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.02240278139021137, 0.01797793408731821, 0.029295129513254844]
###############
Output Layer:
###############
[0.11008677235120382, 0.3116509287293782, 0.19510638140767045]
Weights updated in this iteration:
[-0.021252287054850535, -0.009458827302046923, 0.013791793480992517, -0.0309030055833344, 0.02205835508234168, -0.01184293608833132, -0.01643089340396685, -0.03029577252370393, 0.053247804765552006, 0.22824121509119893, 0.33821855708002213, 0.4476323813459912, -0.01722691788031725, 10.982201203530945, 0.5367497354577448, 0.7663468721551209, 0.206303448834199, 0.8750829002362438, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0373871054334
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.02192583729595615, 0.0169335461890922, 0.028530599334892777]
###############
Output Layer:
###############
[0.1095779307897753, 0.2998677816033007, 0.19442706137145305]
Weights updated in this iteration:
[-0.023300664981971624, -0.011102623343184518, 0.011113219976127431, -0.03538843929316823, 0.0184588541907398, -0.01770833999349622, -0.019714395099108467, -0.032930738960355685, 0.04895411436431483, 0.22806544477538418, 0.33807750373949336, 0.4474025342534945, -0.01843633924082914, 10.98123065897559, 0.5351682284664847, 0.7659770942277047, 0.20600670696584608, 0.8745993579436099, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0348232502158
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.02149346907053747, 0.01600319989993554, 0.02783654041435431]
###############
Output Layer:
###############
[0.10911998748375137, 0.28936879916359176, 0.19380983567314808]
Weights updated in this iteration:
[-0.025167390724664708, -0.012544314538808358, 0.008684176856176465, -0.03940515760290529, 0.015356701962370878, -0.022935022066767445, -0.02271095584928925, -0.03524501317422873, 0.045054893869695835, 0.22789520233348773, 0.33794602377755234, 0.4471810093541723, -0.01954051440821378, 10.980377893311713, 0.5337314404947889, 0.7656184772676464, 0.20572974345583872, 0.8741327141085757, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0029555628060509175, 0.17624521072796934]
###############
Hidden middle Layer:
###############
[0.02149346907053747, 0.01600319989993554, 0.02783654041435431]
###############
Output Layer:
###############
[0.10911998748375137, 0.28936879916359176, 0.19380983567314808]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.025167390724664708, -0.012544314538808358, 0.008684176856176465, -0.03940515760290529, 0.015356701962370878, -0.022935022066767445, -0.02271095584928925, -0.03524501317422873, 0.045054893869695835, 0.22789520233348773, 0.33794602377755234, 0.4471810093541723, -0.01954051440821378, 10.980377893311713, 0.5337314404947889, 0.7656184772676464, 0.20572974345583872, 0.8741327141085757, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.08192951081
0.569913930495
0.08192951081
self.sum1_output_product1_level3 =  0.0466927695293
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0466927695293
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0445197244609
self.product1_product2_sum1_level2 =  0.108478328312
0.569913930495
0.108478328312
self.sum1_output_product1_level3 =  0.061823310462
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.061823310462
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0589461018221
self.product1_product2_sum1_level2 =  0.117134154729
0.569913930495
0.117134154729
self.sum1_output_product1_level3 =  0.0667563865169
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0667563865169
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0636495963657
self.product1_product2_sum1_level2 =  0.119956251455
0.569913930495
0.119956251455
self.sum1_output_product1_level3 =  0.0683647387543
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0683647387543
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0651830971447
self.product1_product2_sum1_level2 =  0.120876351923
0.569913930495
0.120876351923
self.sum1_output_product1_level3 =  0.0688891168281
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0688891168281
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.065683071072
self.product1_product2_sum1_level2 =  0.121176336279
0.569913930495
0.121176336279
self.sum1_output_product1_level3 =  0.0690600820918
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0690600820918
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0658460797457
self.product1_product2_sum1_level2 =  0.121274141483
0.569913930495
0.121274141483
self.sum1_output_product1_level3 =  0.0691158226401
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0691158226401
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0658992261724
self.product1_product2_sum1_level2 =  0.121306029339
0.569913930495
0.121306029339
self.sum1_output_product1_level3 =  0.0691339959735
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0691339959735
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.065916553733
self.product1_product2_sum1_level2 =  0.121316425876
0.569913930495
0.121316425876
self.sum1_output_product1_level3 =  0.0691399211044
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0691399211044
####################################################################################################################
self.cell_input_product1_level1 =  0.272554978786
self.forget_feedback_product2_level1 =  0.0659222031129
self.product1_product2_sum1_level2 =  0.121319815504
0.569913930495
0.121319815504
self.sum1_output_product1_level3 =  0.0691418529006
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0691418529006
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0691418529006
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09137787928471286,
   0.09137787928471286,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07068893964235642,
   0.07068893964235642,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09728900489681469,
   0.09728900489681469,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07364450244840734,
   0.07364450244840734,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10320013050891652,
   0.10320013050891652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07660006525445825,
   0.07660006525445825,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.09137787928471286, 0.05, 0.05, 0.05, 0.05],
  [0.09137787928471286, 0.05, 0.05, 0.05, 0.05],
  [0.09137787928471286, 0.05, 0.05, 0.05, 0.05],
  [0.09137787928471286, 0.05, 0.05, 0.05, 0.05],
  [0.09137787928471286, 0.05, 0.05, 0.05, 0.05]],
 [[0.09728900489681469, 0.05, 0.05, 0.05, 0.05],
  [0.09728900489681469, 0.05, 0.05, 0.05, 0.05],
  [0.09728900489681469, 0.05, 0.05, 0.05, 0.05],
  [0.09728900489681469, 0.05, 0.05, 0.05, 0.05],
  [0.09728900489681469, 0.05, 0.05, 0.05, 0.05]],
 [[0.10320013050891652, 0.05, 0.05, 0.05, 0.05],
  [0.10320013050891652, 0.05, 0.05, 0.05, 0.05],
  [0.10320013050891652, 0.05, 0.05, 0.05, 0.05],
  [0.10320013050891652, 0.05, 0.05, 0.05, 0.05],
  [0.10320013050891652, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06412556913757585, 0.06441616650039561, 0.06470701290190663]
Scheduled Classes by Deep Learning for process id  6323  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6348
Process cmdline: ['/opt/google/chrome/chrome --type=gpu-process --channel=6299.0.607536934 --supports-dual-gpus=false --gpu-driver-bug-workarounds=2,45,57 --disable-gl-extensions=GL_ARB_occlusion_query GL_ARB_occlusion_query2 --disable-accelerated-video-decode --gpu-vendor-id=0x8086 --gpu-device-id=0x2a42 --gpu-driver-vendor --gpu-driver-version --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 168082, 233219, 3, 1.2177874370151138, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[0.060639547244018904, 0.08462761205815349, 0.10674028421928479]
###############
Output Layer:
###############
[0.38703754218156516, 1.3544982939747428, 0.718684274865192]
Error before Backpropagation:
1.09922277855
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
6.53998122971
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[0.2795639823366666, -0.35847048042902874, 0.29330925694127585]
###############
Output Layer:
###############
[0.2813543887280111, -3.480083097474063, 0.7861286473493914]
Weights updated in this iteration:
[0.1802884009920927, 0.26065185249768696, 0.3557488132290819, -0.3016598622961239, -0.3820019606683309, -0.518684801131929, 0.17812099586380156, 0.22352861206291472, 0.37544808707679933, 0.22486362405494273, 0.3328317533586794, 0.4409587348001417, 0.03769207479941409, 11.052602475261825, 0.6263471770446115, 0.7622923728246478, 0.19924335500296925, 0.8664327101249173, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2368455602.3
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[29.02349039599804, -2216.7848190976097, 24.19473785251023]
###############
Output Layer:
###############
[-771.7614587531244, -68818.92279132112, -512.891582327995]
Weights updated in this iteration:
[38.99593225513786, -49.51064695451879, 41.07983480384234, -2993.351897284977, 3837.4519979322663, -3140.727712404524, 32.454481804036504, -41.162784705912905, 34.238735218296064, 0.2106555189069957, 0.3510500740300356, 0.4260520621713719, -15.392465401237864, 30.837897838774012, -15.562462218055273, 0.7295720964321517, 0.24119887838674922, 0.832103681487606, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
3.19502371348e+92
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[4.405603476183797e+24, 3.498637264562702e+28, 1.9470358200191476e+24]
###############
Output Layer:
###############
[3.569852303474096e+40, 2.527854312838621e+46, 1.0486259198390434e+40]
Weights updated in this iteration:
[-1.3377246173349796e+25, 1.0217405223770875e+27, -1.1151620977927668e+25, -1.0623319191642538e+29, 8.113983670175195e+30, -8.85587568753526e+28, -5.91201128597428e+24, 4.5155343793138745e+26, -4.928406655904931e+24, -13359129137.230389, 1020356761512.6493, -11136518144.296646, -9459756128564760.0, 7.22526598009019e+17, -7885899199458814.0, -3924176096.516316, 299724598287.6986, -3271295445.185049, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[-5.048956969924379e+268, -2.531659268297406e+280, -4.356546456528607e+267]
###############
Output Layer:
###############
[-inf, -inf, -inf]
Weights updated in this iteration:
[-5.203656968397334e+266, -4.13239826962511e+270, -2.2997318227418186e+266, -2.6092292866739978e+278, -2.0720763599118706e+282, -1.1531366613587944e+278, -4.4900310027798375e+265, -3.565683990919389e+269, -1.9843481699314237e+265, 2.0042696540802422e+146, 1.5916576555072588e+150, 8.857775854244433e+145, 7.116419292995366e+163, 5.651386890200427e+167, 3.145068172530936e+163, 5.080040459935045e+144, 4.0342302603252976e+148, 2.245100085858802e+144, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.012177874370151139, 0.7207045738125968]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0892070136741
0.569913930495
0.0892070136741
self.sum1_output_product1_level3 =  0.0508403197908
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0508403197908
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0484742509688
self.product1_product2_sum1_level2 =  0.118128547081
0.569913930495
0.118128547081
self.sum1_output_product1_level3 =  0.0673231045707
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0673231045707
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0641899398034
self.product1_product2_sum1_level2 =  0.127557960382
0.569913930495
0.127557960382
self.sum1_output_product1_level3 =  0.0726970585673
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0726970585673
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0693137941732
self.product1_product2_sum1_level2 =  0.130632273004
0.569913930495
0.130632273004
self.sum1_output_product1_level3 =  0.0744491521572
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0744491521572
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0709843466943
self.product1_product2_sum1_level2 =  0.131634604516
0.569913930495
0.131634604516
self.sum1_output_product1_level3 =  0.0750203948492
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0750203948492
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0715290041971
self.product1_product2_sum1_level2 =  0.131961399018
0.569913930495
0.131961399018
self.sum1_output_product1_level3 =  0.0752066395881
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0752066395881
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0717065812511
self.product1_product2_sum1_level2 =  0.132067945251
0.569913930495
0.132067945251
self.sum1_output_product1_level3 =  0.0752673617702
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0752673617702
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0717644774702
self.product1_product2_sum1_level2 =  0.132102682982
0.569913930495
0.132102682982
self.sum1_output_product1_level3 =  0.0752871592873
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0752871592873
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0717833536263
self.product1_product2_sum1_level2 =  0.132114008676
0.569913930495
0.132114008676
self.sum1_output_product1_level3 =  0.0752936139579
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0752936139579
####################################################################################################################
self.cell_input_product1_level1 =  0.296813321666
self.forget_feedback_product2_level1 =  0.0717895079016
self.product1_product2_sum1_level2 =  0.132117701241
0.569913930495
0.132117701241
self.sum1_output_product1_level3 =  0.0752957184022
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0752957184022
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0752957184022
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22049024118211596,
   0.22049024118211596,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.13524512059105798,
   0.13524512059105798,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24484598992241824,
   0.24484598992241824,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.14742299496120911,
   0.14742299496120911,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.26920173866272046,
   0.26920173866272046,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.15960086933136025,
   0.15960086933136025,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.22049024118211596, 0.05, 0.05, 0.05, 0.05],
  [0.22049024118211596, 0.05, 0.05, 0.05, 0.05],
  [0.22049024118211596, 0.05, 0.05, 0.05, 0.05],
  [0.22049024118211596, 0.05, 0.05, 0.05, 0.05],
  [0.22049024118211596, 0.05, 0.05, 0.05, 0.05]],
 [[0.24484598992241824, 0.05, 0.05, 0.05, 0.05],
  [0.24484598992241824, 0.05, 0.05, 0.05, 0.05],
  [0.24484598992241824, 0.05, 0.05, 0.05, 0.05],
  [0.24484598992241824, 0.05, 0.05, 0.05, 0.05],
  [0.24484598992241824, 0.05, 0.05, 0.05, 0.05]],
 [[0.26920173866272046, 0.05, 0.05, 0.05, 0.05],
  [0.26920173866272046, 0.05, 0.05, 0.05, 0.05],
  [0.26920173866272046, 0.05, 0.05, 0.05, 0.05],
  [0.26920173866272046, 0.05, 0.05, 0.05, 0.05],
  [0.26920173866272046, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.07054213077567675, 0.07177270715557302, 0.07301124534320355]
Scheduled Classes by Deep Learning for process id  6348  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6350
Process cmdline: ['/opt/google/chrome/chrome --type=gpu-broker', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 2, 4, 1, 0.2771266741932269, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.04806373913506444, 0.06427435540745129, 0.08005819660158058]
###############
Output Layer:
###############
[0.27790794083959824, 1.0070179094819642, 0.5105066937695644]
Error before Backpropagation:
0.567563573758
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.565273532128
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.03786881372388903, 0.0646333893301037, 0.06736302061997461]
###############
Output Layer:
###############
[0.2733965542429343, 1.0112751714215253, 0.49789794051355224]
Weights updated in this iteration:
[-0.0021870211198818934, 0.006702621957559156, 0.03570047794114861, 0.04342918940763631, 0.09957394353872491, 0.08871488674402356, 0.017824177562019893, 0.0007057777332287948, 0.09472212944323062, 0.2276088791999686, 0.3368024179789365, 0.44601718837210924, 0.0003216775132889749, 11.000430170752168, 0.5605358077017027, 0.7649494501099189, 0.2032460344434311, 0.8715874644103329, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.565967700412
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.028450797564615368, 0.06517397115447693, 0.05551075777487727]
###############
Output Layer:
###############
[0.2695506993377708, 1.0176414857200629, 0.48682949499479844]
Weights updated in this iteration:
[-0.012719876965586985, -0.011274549907316519, 0.016964084699607543, 0.04403376157364655, 0.10060580971939784, 0.08979033123991279, 0.004568925725369344, -0.021917900390766652, 0.07114299624656636, 0.225777886486513, 0.3336773328951462, 0.44276012305153056, 0.0007324301504103702, 11.001131231400832, 0.5612664759123803, 0.7610878655812339, 0.1966551943370203, 0.8647182764783251, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.570139858822
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.020044479796167112, 0.0659404589128525, 0.04484445686200309]
###############
Output Layer:
###############
[0.2663697932658111, 1.0266284286224514, 0.47734805983027956]
Weights updated in this iteration:
[-0.021281106277556597, -0.030886278223823314, 0.0002601478235817224, 0.044814374161110296, 0.10239400641731601, 0.09131339561658998, -0.006293933678819344, -0.04680211264723607, 0.04994831600046909, 0.2244359794532618, 0.33060334489609355, 0.4401419090591096, 0.0012215635942352653, 11.002251719108992, 0.5622208311657995, 0.7582672950821114, 0.190193942066975, 0.8592150210631421, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.57762865059
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.01314549503976295, 0.0669055800607648, 0.036036590117120806]
###############
Output Layer:
###############
[0.26388883341993863, 1.0379545641174102, 0.46968168945046224]
Weights updated in this iteration:
[-0.02739863612706584, -0.05101115698305405, -0.013426278874844853, 0.0456701750708406, 0.10520934037955951, 0.0932280338296606, -0.014104124543830237, -0.07249534964180582, 0.032474988137055794, 0.22351011173183086, 0.3275575116759455, 0.4380705140657823, 0.0017512431108241103, 11.00399420934849, 0.5634058551960512, 0.7563302313790434, 0.18382157067980842, 0.8548813306628655, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.585764534083
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.008728860659471495, 0.06780129409896497, 0.03037331993087381]
###############
Output Layer:
###############
[0.2621651274774824, 1.0486766343233542, 0.4643620366612446]
Weights updated in this iteration:
[-0.030588032731626823, -0.06724397508739485, -0.02216957669754697, 0.046316999567982846, 0.10850143155207995, 0.0950012153805053, -0.018193757768470323, -0.09331003043569512, 0.02126381382981788, 0.2229128690285934, 0.3245177730144273, 0.4364332542720729, 0.00225769474248311, 11.006571855917628, 0.5647942235921471, 0.7550870432600578, 0.1774942150363895, 0.8514732992006161, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.587694077986
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.008017483524368553, 0.06798902161578195, 0.029458587957896]
###############
Output Layer:
###############
[0.26117529841656895, 1.0517438821935319, 0.4620083460168401]
Weights updated in this iteration:
[-0.030991915101699866, -0.07038112533219751, -0.023574943207477123, 0.046423581338296556, 0.10932930385254874, 0.09537208190419354, -0.018713094276487843, -0.09734396896331016, 0.019456708123223414, 0.2225208666992599, 0.32147290080767726, 0.4350692260641562, 0.002698223196245439, 11.009993654067205, 0.5663271053950268, 0.7542742550046981, 0.1711808944677961, 0.8486450861406093, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.585474287245
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.008877190814192354, 0.06774655826281681, 0.03056598271251322]
###############
Output Layer:
###############
[0.2604229668240757, 1.050124088972334, 0.46082087223708423]
Weights updated in this iteration:
[-0.030529867698068865, -0.0664629194740248, -0.021877245414995118, 0.04629327005207241, 0.10822425177118868, 0.0948932799864256, -0.01811792790158093, -0.09229690158792449, 0.021643524092534464, 0.2221632209977381, 0.3184400312972316, 0.4337551307717841, 0.00313094283930124, 11.013663157730777, 0.5679170443861484, 0.7535329172490036, 0.16489427990309993, 0.8459211936104585, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.584977590083
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.009116577227462362, 0.0676811362483769, 0.03087480029072267]
###############
Output Layer:
###############
[0.25956997392642034, 1.0504439825211203, 0.459152441038617]
Weights updated in this iteration:
[-0.030392506489975945, -0.06541464313504598, -0.021404282695164235, 0.04625573054923855, 0.1079377678857882, 0.09476402380407323, -0.017940726718590927, -0.09094458523932468, 0.022253663990310465, 0.22176925021298396, 0.3154334309794235, 0.4323986088215318, 0.0035935925500068843, 11.017193883411641, 0.5695100419182922, 0.7527150083382881, 0.1586523825694437, 0.8431049660280378, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.583723687242
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.009638818941283838, 0.06753715187969801, 0.031549584411182316]
###############
Output Layer:
###############
[0.25875368954781564, 1.0499142520686613, 0.45768042857062535]
Weights updated in this iteration:
[-0.030087798118569254, -0.06315249931871617, -0.020372337275368607, 0.04617172109283737, 0.10731408475088172, 0.09447951185137399, -0.017547015597945664, -0.08802168826450132, 0.023587031976876924, 0.22136701010212187, 0.31244721509285034, 0.4310363559955069, 0.004072050744540662, 11.020745939566646, 0.5711304199214743, 0.7518792719127212, 0.15244790588096285, 0.8402746060594596, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.582736918482
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.010066043238178241, 0.06742042113748176, 0.03210245624730179]
###############
Output Layer:
###############
[0.25791563490690544, 1.0496891327440203, 0.4561198176256335]
Weights updated in this iteration:
[-0.029829813587687215, -0.061344856644760914, -0.01952790760162706, 0.04610123182996285, 0.10682018149555089, 0.09424878784227193, -0.017213157289052646, -0.0856824141842058, 0.0246798101441979, 0.2209441068492311, 0.3094840220507475, 0.4296521177443147, 0.0045720850473512705, 11.024249573451637, 0.5727671219428642, 0.7509996177650768, 0.14628435657757144, 0.8373953401501388, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0027712667419322688, 0.5]
###############
Hidden middle Layer:
###############
[0.010066043238178241, 0.06742042113748176, 0.03210245624730179]
###############
Output Layer:
###############
[0.25791563490690544, 1.0496891327440203, 0.4561198176256335]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.029829813587687215, -0.061344856644760914, -0.01952790760162706, 0.04610123182996285, 0.10682018149555089, 0.09424878784227193, -0.017213157289052646, -0.0856824141842058, 0.0246798101441979, 0.2209441068492311, 0.3094840220507475, 0.4296521177443147, 0.0045720850473512705, 11.024249573451637, 0.5727671219428642, 0.7509996177650768, 0.14628435657757144, 0.8373953401501388, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0817832630952
0.569913930495
0.0817832630952
self.sum1_output_product1_level3 =  0.0466094209194
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0466094209194
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0444402548302
self.product1_product2_sum1_level2 =  0.108284398819
0.569913930495
0.108284398819
self.sum1_output_product1_level3 =  0.0617127873424
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0617127873424
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0588407223622
self.product1_product2_sum1_level2 =  0.116924679338
0.569913930495
0.116924679338
self.sum1_output_product1_level3 =  0.0666370035736
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0666370035736
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0635357694115
self.product1_product2_sum1_level2 =  0.119741707568
0.569913930495
0.119741707568
self.sum1_output_product1_level3 =  0.0682424672043
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0682424672043
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0650665160172
self.product1_product2_sum1_level2 =  0.120660155531
0.569913930495
0.120660155531
self.sum1_output_product1_level3 =  0.0687659034931
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0687659034931
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0655655919895
self.product1_product2_sum1_level2 =  0.120959601115
0.569913930495
0.120959601115
self.sum1_output_product1_level3 =  0.0689365617024
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0689365617024
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0657283078989
self.product1_product2_sum1_level2 =  0.12105723066
0.569913930495
0.12105723066
self.sum1_output_product1_level3 =  0.0689922021405
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0689922021405
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0657813588744
self.product1_product2_sum1_level2 =  0.121089061246
0.569913930495
0.121089061246
self.sum1_output_product1_level3 =  0.0690103428345
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0690103428345
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0657986553147
self.product1_product2_sum1_level2 =  0.12109943911
0.569913930495
0.12109943911
self.sum1_output_product1_level3 =  0.0690162573239
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0690162573239
####################################################################################################################
self.cell_input_product1_level1 =  0.272067486403
self.forget_feedback_product2_level1 =  0.0658042945483
self.product1_product2_sum1_level2 =  0.12110282265
0.569913930495
0.12110282265
self.sum1_output_product1_level3 =  0.0690181856505
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0690181856505
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0690181856505
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.08879773438705177,
   0.08879773438705177,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06939886719352589,
   0.06939886719352589,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09434026787091632,
   0.09434026787091632,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07217013393545815,
   0.07217013393545815,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09988280135478084,
   0.09988280135478084,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07494140067739043,
   0.07494140067739043,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.08879773438705177, 0.05, 0.05, 0.05, 0.05],
  [0.08879773438705177, 0.05, 0.05, 0.05, 0.05],
  [0.08879773438705177, 0.05, 0.05, 0.05, 0.05],
  [0.08879773438705177, 0.05, 0.05, 0.05, 0.05],
  [0.08879773438705177, 0.05, 0.05, 0.05, 0.05]],
 [[0.09434026787091632, 0.05, 0.05, 0.05, 0.05],
  [0.09434026787091632, 0.05, 0.05, 0.05, 0.05],
  [0.09434026787091632, 0.05, 0.05, 0.05, 0.05],
  [0.09434026787091632, 0.05, 0.05, 0.05, 0.05],
  [0.09434026787091632, 0.05, 0.05, 0.05, 0.05]],
 [[0.09988280135478084, 0.05, 0.05, 0.05, 0.05],
  [0.09988280135478084, 0.05, 0.05, 0.05, 0.05],
  [0.09988280135478084, 0.05, 0.05, 0.05, 0.05],
  [0.09988280135478084, 0.05, 0.05, 0.05, 0.05],
  [0.09988280135478084, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06399880244424506, 0.06427117275326588, 0.06454375849453302]
Scheduled Classes by Deep Learning for process id  6350  - [BackPropagation, LSTM, GRU, Convolution] =  ['Normal', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6424
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --extension-process --enable-webrtc-hw-h264-encoding --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.2.2069587092 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 1008, 8859, 6, 1.0792240999185, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.026620046805426227, 0.0310813001284757, 0.03388054833765068]
###############
Output Layer:
###############
[0.08789242009643401, 0.42561255403768244, 0.14715319176272193]
Error before Backpropagation:
0.0701452796516
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0639205935903
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.02612150909362026, 0.029561033028159815, 0.03310010524144594]
###############
Output Layer:
###############
[0.08723548275520726, 0.40840078158237086, 0.1464108330657634]
Weights updated in this iteration:
[0.0068330165116378475, 0.019302261787021116, 0.051969235593525565, 0.03334243410160616, 0.08772392274917698, 0.07570837517168143, 0.028042199735837405, 0.015211322049363825, 0.11368998136156024, 0.2298764541939076, 0.33985574915375466, 0.4498427576147471, -0.0023793004309710527, 10.9972219526385, 0.5569717557654754, 0.7698090630334673, 0.209777063909549, 0.879756985809554, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0586290427395
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.02567559812093544, 0.028207993770257695, 0.03240023367797362]
###############
Output Layer:
###############
[0.08664911918922548, 0.3930784930293518, 0.14574657169117336]
Weights updated in this iteration:
[0.003981812792378056, 0.01607562863144911, 0.048356307150759954, 0.02469095252624289, 0.07793326587421522, 0.06474557246328452, 0.02356714411484266, 0.010147017895229527, 0.10801937437306786, 0.2297574079954659, 0.3397210276646018, 0.4496919071534322, -0.004578129856118303, 10.994733594518488, 0.5541854892783628, 0.7696249083013041, 0.20956866078474268, 0.8795236324840067, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0540940306925
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.025274135844489523, 0.026997800458340018, 0.03176865418990638]
###############
Output Layer:
###############
[0.08612290061933918, 0.379369555953565, 0.14514860053542905]
Weights updated in this iteration:
[0.0013990621071341548, 0.01323814009870546, 0.045097114303929396, 0.01690534528736688, 0.06937976073453099, 0.05492085513142922, 0.01950396694653552, 0.005683087757703078, 0.10289202002252539, 0.22964229733777014, 0.33959456359159845, 0.44954664813393047, -0.006618359109854131, 10.992492136547062, 0.5516109083760476, 0.7694467016899886, 0.209372877576183, 0.8792987521979707, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0501761472086
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.024910431627371388, 0.025909779038716187, 0.03119527044925173]
###############
Output Layer:
###############
[0.08564805478742535, 0.3670401458053202, 0.14460711748008373]
Weights updated in this iteration:
[-0.0009542676100384013, 0.010724316223268832, 0.04213906586897231, 0.0098653577942074, 0.06185965485478957, 0.046071851171509, 0.015793916491476125, 0.0017200164783380285, 0.09822862378777007, 0.22953065655646807, 0.33947530904793566, 0.44940631979999657, -0.008518848202884316, 10.990462036462826, 0.549222063860862, 0.769273754188826, 0.20918813527065433, 0.8790813635841226, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0467653455306
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.02457901009469556, 0.024926584271035696, 0.03067178438366478]
###############
Output Layer:
###############
[0.08521726184544515, 0.35589426861936, 0.14411401716556893]
Weights updated in this iteration:
[-0.0031103476709948437, 0.008481739346046811, 0.039439012268220115, 0.003469132969835225, 0.0552068286669311, 0.038061875020342535, 0.012388350597528778, -0.001822172638811155, 0.09396384224228685, 0.22942209848047174, 0.33936239587950856, 0.449270372796315, -0.010295761145236004, 10.98861383798935, 0.5469968402826191, 0.7691054922819206, 0.2090131230952827, 0.8788706496241744, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0437744478873
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.024275375580641037, 0.02403372346858483, 0.030191357017507713]
###############
Output Layer:
###############
[0.08482442868706822, 0.34576826918141645, 0.14366259519170804]
Weights updated in this iteration:
[-0.005095763196841046, 0.006468247865822046, 0.03696144150006793, -0.002369135137219095, 0.04928600104178617, 0.03077638631010997, 0.00924691605152598, -0.0050080305167808075, 0.09004369237949039, 0.22931629869400685, 0.33925509996795583, 0.44913834680235565, -0.011962928120299926, 10.98692309544367, 0.5449164071897598, 0.768941434701884, 0.2088467455610236, 0.8786659245883816, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0411340024152
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.023995818268345685, 0.023219090027465104, 0.029748326017408018]
###############
Output Layer:
###############
[0.08446447858237491, 0.33652549882884014, 0.14324729007801773]
Weights updated in this iteration:
[-0.006932580747418961, 0.0046497151339108716, 0.034676986033609705, -0.0077216440365589, 0.04398677432858714, 0.02411945502669755, 0.006336002687280101, -0.007889966847895072, 0.08642338074579245, 0.2292129832051799, 0.33915281294544447, 0.44900985302073526, -0.013532194615052194, 10.985369450398792, 0.5429647057694752, 0.7687811742910768, 0.20868808048152743, 0.8784666082374407, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0387883030986
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.023737258602204575, 0.022472548987769438, 0.029337977021026404]
###############
Output Layer:
###############
[0.08413316909675438, 0.3280515993574213, 0.14286346831886138]
Weights updated in this iteration:
[-0.008639216457166268, 0.002998322055935146, 0.032561219239631, -0.012649224645520474, 0.03921869613689653, 0.01801058752348684, 0.003627474086761097, -0.01051082222836677, 0.08306553768602885, 0.22911191880128115, 0.3390550199272421, 0.4488845604884151, -0.015013733183655363, 10.983935868221312, 0.5411279985626903, 0.7686243638502063, 0.20853634588747844, 0.8782722056933897, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0366923413953
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.023497123902968597, 0.021785585863878226, 0.028956361055749814]
###############
Output Layer:
###############
[0.08382694025486136, 0.3202505178503482, 0.14250724951897792]
Weights updated in this iteration:
[-0.01023113167730867, 0.0014912233058838916, 0.030593697508051496, -0.01720328141481869, 0.03490727730137724, 0.012382017894232674, 0.0010976428491016412, -0.012905865285858313, 0.07993880211974763, 0.2290129054881987, 0.338961281995412, 0.4487621853501285, -0.016416311265938077, 10.982608018818334, 0.5393944873667351, 0.7684707050600997, 0.20839087396311098, 0.8780822916797805, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0348094787288
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.02327325096405126, 0.021151016362648416, 0.028600149598460203]
###############
Output Layer:
###############
[0.0835427903618329, 0.313041218504066, 0.14217536579522866]
Weights updated in this iteration:
[-0.011721386897317497, 0.00010951875392835168, 0.028757201756742134, -0.021427420858556034, 0.030990825529128282, 0.0071764571035675875, -0.0012735503206490304, -0.015104339984837045, 0.07701669446777437, 0.2289157705357536, 0.3388712223840478, 0.4486424824126223, -0.017747516557888372, 10.981373778946681, 0.5377539946310191, 0.7683199397618002, 0.20825109045748025, 0.8778964981149907, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.010792240999185, 0.11378259397223163]
###############
Hidden middle Layer:
###############
[0.02327325096405126, 0.021151016362648416, 0.028600149598460203]
###############
Output Layer:
###############
[0.0835427903618329, 0.313041218504066, 0.14217536579522866]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.011721386897317497, 0.00010951875392835168, 0.028757201756742134, -0.021427420858556034, 0.030990825529128282, 0.0071764571035675875, -0.0012735503206490304, -0.015104339984837045, 0.07701669446777437, 0.2289157705357536, 0.3388712223840478, 0.4486424824126223, -0.017747516557888372, 10.981373778946681, 0.5377539946310191, 0.7683199397618002, 0.20825109045748025, 0.8778964981149907, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0881209513058
0.569913930495
0.0881209513058
self.sum1_output_product1_level3 =  0.0502213577177
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0502213577177
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0478840949077
self.product1_product2_sum1_level2 =  0.116688391076
0.569913930495
0.116688391076
self.sum1_output_product1_level3 =  0.0665023396014
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0665023396014
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0634073725955
self.product1_product2_sum1_level2 =  0.126002357689
0.569913930495
0.126002357689
self.sum1_output_product1_level3 =  0.0718104989221
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0718104989221
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0684684943224
self.product1_product2_sum1_level2 =  0.129039030725
0.569913930495
0.129039030725
self.sum1_output_product1_level3 =  0.0735411411878
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0735411411878
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0701185938471
self.product1_product2_sum1_level2 =  0.13002909044
0.569913930495
0.13002909044
self.sum1_output_product1_level3 =  0.0741053900113
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0741053900113
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0706565829705
self.product1_product2_sum1_level2 =  0.130351883914
0.569913930495
0.130351883914
self.sum1_output_product1_level3 =  0.0742893545088
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0742893545088
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0708319859038
self.product1_product2_sum1_level2 =  0.130457125674
0.569913930495
0.130457125674
self.sum1_output_product1_level3 =  0.0743493332539
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0743493332539
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0708891732849
self.product1_product2_sum1_level2 =  0.130491438102
0.569913930495
0.130491438102
self.sum1_output_product1_level3 =  0.074368888385
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.074368888385
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0709078183355
self.product1_product2_sum1_level2 =  0.130502625133
0.569913930495
0.130502625133
self.sum1_output_product1_level3 =  0.0743752640294
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0743752640294
####################################################################################################################
self.cell_input_product1_level1 =  0.293193113772
self.forget_feedback_product2_level1 =  0.0709138972624
self.product1_product2_sum1_level2 =  0.130506272489
0.569913930495
0.130506272489
self.sum1_output_product1_level3 =  0.0743773427085
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0743773427085
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0743773427085
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20109137398859,
   0.20109137398859,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.12554568699429502,
   0.12554568699429502,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22267585598696005,
   0.22267585598696005,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.13633792799348002,
   0.13633792799348002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24426033798533003,
   0.24426033798533003,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.147130168992665,
   0.147130168992665,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.20109137398859, 0.05, 0.05, 0.05, 0.05],
  [0.20109137398859, 0.05, 0.05, 0.05, 0.05],
  [0.20109137398859, 0.05, 0.05, 0.05, 0.05],
  [0.20109137398859, 0.05, 0.05, 0.05, 0.05],
  [0.20109137398859, 0.05, 0.05, 0.05, 0.05]],
 [[0.22267585598696005, 0.05, 0.05, 0.05, 0.05],
  [0.22267585598696005, 0.05, 0.05, 0.05, 0.05],
  [0.22267585598696005, 0.05, 0.05, 0.05, 0.05],
  [0.22267585598696005, 0.05, 0.05, 0.05, 0.05],
  [0.22267585598696005, 0.05, 0.05, 0.05, 0.05]],
 [[0.24426033798533003, 0.05, 0.05, 0.05, 0.05],
  [0.24426033798533003, 0.05, 0.05, 0.05, 0.05],
  [0.24426033798533003, 0.05, 0.05, 0.05, 0.05],
  [0.24426033798533003, 0.05, 0.05, 0.05, 0.05],
  [0.24426033798533003, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06956725475779246, 0.07065225135276153, 0.07174302630968385]
Scheduled Classes by Deep Learning for process id  6424  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6494
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.4.835453967 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 83111, 203499, 14, 4.408771400527224, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[0.04388497010493543, 0.06030475218808706, 0.06993502631442679]
###############
Output Layer:
###############
[0.23438160006927, 0.9120618008970993, 0.42585610995594975]
Error before Backpropagation:
0.440290238795
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.378070249173
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[0.030365919505478536, 0.054262314911573795, 0.051514843050227244]
###############
Output Layer:
###############
[0.22800136749830105, 0.8433283661565486, 0.41145392763033933]
Weights updated in this iteration:
[-0.009002783796299438, -0.0031127708411212117, 0.02571719060845149, 0.034506568577168065, 0.08732870853151639, 0.07446487969263242, 0.00710803684159159, -0.014579571279126808, 0.07873860411695213, 0.22839049074374693, 0.3377882847678673, 0.44743508832476764, -0.002999084738962364, 10.995878792635798, 0.5552206628001112, 0.7663962663049328, 0.205047911234456, 0.8742570950785183, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.267641062853
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[0.013081648362414234, 0.04128800795210817, 0.027735559481337893]
###############
Output Layer:
###############
[0.21884606248113928, 0.6982658018189587, 0.39370389628839486]
Weights updated in this iteration:
[-0.03140226395115133, -0.043139473994267416, -0.012282834530773779, 0.017692568793432662, 0.05728296758092284, 0.04594044921229041, -0.02370862797401841, -0.06964734455855459, 0.026459085592206187, 0.22733219119966475, 0.3358971586611344, 0.4456397158874639, -0.0061418892733134, 10.990262764795188, 0.5498889921324265, 0.7640324874079852, 0.20082396159037014, 0.8702470173049891, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.110722205199
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.009169790765983923, 0.013864181670805026, -0.0033341527847024557]
###############
Output Layer:
###############
[0.20419570854374464, 0.3954232833407924, 0.3703417316700563]
Weights updated in this iteration:
[-0.05354179786421708, -0.1130157791000507, -0.059222818141647156, -0.009593339703427902, -0.028836210659315617, -0.011910818238943292, -0.0546220868480249, -0.16721571671296376, -0.039083272191799044, 0.2269098671938184, 0.33456422901582683, 0.44474430941726684, -0.00790106552929705, 10.984710491776225, 0.5461592069911152, 0.7630841402986783, 0.1978308095115446, 0.868236342737806, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.106073435777
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.009990321407985989, 0.012657047420913371, -0.004511621780096845]
###############
Output Layer:
###############
[0.2036366430408536, 0.3822575203964016, 0.36954834697337374]
Weights updated in this iteration:
[-0.06356776329275714, -0.09785711194243918, -0.06286827760806177, -0.024343168213780946, -0.0065353417191711845, -0.017273883008546074, -0.06900943917431646, -0.1454628903394841, -0.04431453995432505, 0.2271694343655919, 0.3341717787934516, 0.4448386885182379, -0.0071657626145398175, 10.983598757228139, 0.5464265644830584, 0.76368359275127, 0.1969244728330055, 0.8684543047518924, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0995016202122
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.011218317084978054, 0.010868941894007174, -0.006275841580424938]
###############
Output Layer:
###############
[0.20280500046980784, 0.3627477045383636, 0.3683621734966363]
Weights updated in this iteration:
[-0.07311809220490875, -0.08575750463352962, -0.06718119910152932, -0.03824956563157961, 0.011083103612371827, -0.02355400182661233, -0.08273007327926263, -0.12807979429890187, -0.050510768197305475, 0.227450746223604, 0.3338153760931263, 0.44496572874585183, -0.006405530823164991, 10.98263559604029, 0.5467698845993808, 0.7643342607396805, 0.19610012141844554, 0.8687481459355983, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0914196129474
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.012867432133565244, 0.008511050424174849, -0.008649132005723884]
###############
Output Layer:
###############
[0.2017077992373225, 0.33702520540626363, 0.36678998021143655]
Weights updated in this iteration:
[-0.08199569646215349, -0.07715637809208167, -0.07214758020041791, -0.05094269299624554, 0.02338092535646602, -0.030654893689213477, -0.09550609712390713, -0.1157016577431225, -0.05765803467848618, 0.22776416660991133, 0.33351171664569157, 0.4451410649105939, -0.005620431412143303, 10.981874947200113, 0.5472090912549901, 0.7650608368049224, 0.19539617331510073, 0.8691546129882807, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0828594904483
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.014843547939583483, 0.005768925639671959, -0.011499448305640512]
###############
Output Layer:
###############
[0.2004342077179384, 0.3071264278736664, 0.3649539786629101]
Weights updated in this iteration:
[-0.09004962822389954, -0.07182917538173862, -0.07756121038314288, -0.062118599706429044, 0.030773131086319815, -0.038167029087844084, -0.10712295323578479, -0.10801778988881584, -0.06546656406960183, 0.2281199342848531, 0.33327639722925395, 0.4453802021022263, -0.004823959304635846, 10.98134812769191, 0.5477444577776345, 0.7658880310279177, 0.19484903293235611, 0.8697106300672747, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0748963298269
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.01699611329835315, 0.0029106672765299747, -0.014612408732187613]
###############
Output Layer:
###############
[0.19911150429651384, 0.2759821124870178, 0.36303055846188587]
Weights updated in this iteration:
[-0.09724247654085148, -0.06903368421973145, -0.08313358349284453, -0.07166953797317252, 0.03448509084649334, -0.04556623858383621, -0.1175249845021549, -0.10397505393906313, -0.07352512401148026, 0.22852536862893652, 0.33311882569768303, 0.44569429623255785, -0.0040433601146722085, 10.981044748829953, 0.5483491959486532, 0.7668339218900041, 0.19448141367658034, 0.8704434213861435, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0681341223209
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.01919865270512913, 0.00015081708184186762, -0.017806277444126228]
###############
Output Layer:
###############
[0.1978409370639361, 0.2459284312522856, 0.3611609908141012]
Weights updated in this iteration:
[-0.10365303128983554, -0.06793584546067354, -0.08864505806046218, -0.07970216191516126, 0.035860716864171346, -0.05247228717105461, -0.1268208319117228, -0.10238309474299993, -0.08151722899902558, 0.2289837122368533, 0.33304033211991313, 0.4460883571581999, -0.0033098643125299243, 10.980919134097261, 0.5489798189785247, 0.7679069785547604, 0.19429764749044462, 0.8713659818741192, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0626910884867
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.021381483504992516, -0.002399914293157668, -0.020979756828158214]
###############
Output Layer:
###############
[0.19667438985391603, 0.21816288717950064, 0.35941849576331814]
Weights updated in this iteration:
[-0.1094209446402946, -0.06789053499679913, -0.09399465560615428, -0.08644221595517913, 0.035913664083821836, -0.05872352125858507, -0.13520643564597024, -0.10231722073068741, -0.08929466989145962, 0.22949509407867505, 0.33303631490483687, 0.4465626512191021, -0.0026478949898799823, 10.980913933926029, 0.5495937792183209, 0.7691081091455625, 0.1942882118798631, 0.8724800009084202, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.044087714005272234, 0.40840986933596723]
###############
Hidden middle Layer:
###############
[-0.021381483504992516, -0.002399914293157668, -0.020979756828158214]
###############
Output Layer:
###############
[0.19667438985391603, 0.21816288717950064, 0.35941849576331814]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.1094209446402946, -0.06789053499679913, -0.09399465560615428, -0.08644221595517913, 0.035913664083821836, -0.05872352125858507, -0.13520643564597024, -0.10231722073068741, -0.08929466989145962, 0.22949509407867505, 0.33303631490483687, 0.4465626512191021, -0.0026478949898799823, 10.980913933926029, 0.5495937792183209, 0.7691081091455625, 0.1942882118798631, 0.8724800009084202, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.11249220081
0.569913930495
0.11249220081
self.sum1_output_product1_level3 =  0.0641108723139
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0641108723139
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0611272023299
self.product1_product2_sum1_level2 =  0.149005505034
0.569913930495
0.149005505034
self.sum1_output_product1_level3 =  0.0849203130394
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0849203130394
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0809681879177
self.product1_product2_sum1_level2 =  0.160910096387
0.569913930495
0.160910096387
self.sum1_output_product1_level3 =  0.0917049054881
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0917049054881
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0874370307266
self.product1_product2_sum1_level2 =  0.164791402072
0.569913930495
0.164791402072
self.sum1_output_product1_level3 =  0.0939169156667
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0939169156667
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0895460956771
self.product1_product2_sum1_level2 =  0.166056841042
0.569913930495
0.166056841042
self.sum1_output_product1_level3 =  0.0946381069641
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0946381069641
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0902337232941
self.product1_product2_sum1_level2 =  0.166469417612
0.569913930495
0.166469417612
self.sum1_output_product1_level3 =  0.0948732400988
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0948732400988
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0904579135162
self.product1_product2_sum1_level2 =  0.166603931746
0.569913930495
0.166603931746
self.sum1_output_product1_level3 =  0.0949499015772
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0949499015772
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0905310072292
self.product1_product2_sum1_level2 =  0.166647787974
0.569913930495
0.166647787974
self.sum1_output_product1_level3 =  0.0949748958524
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0949748958524
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0905548382903
self.product1_product2_sum1_level2 =  0.16666208661
0.569913930495
0.16666208661
self.sum1_output_product1_level3 =  0.0949830448446
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0949830448446
####################################################################################################################
self.cell_input_product1_level1 =  0.37443061212
self.forget_feedback_product2_level1 =  0.0905626080347
self.product1_product2_sum1_level2 =  0.166666748457
0.569913930495
0.166666748457
self.sum1_output_product1_level3 =  0.094985701696
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.094985701696
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.094985701696
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6672279960738114,
   0.6672279960738114,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3586139980369057,
   0.3586139980369057,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7554034240843558,
   0.7554034240843558,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.40270171204217786,
   0.40270171204217786,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8435788520949002,
   0.8435788520949002,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4467894260474501,
   0.4467894260474501,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.6672279960738114, 0.05, 0.05, 0.05, 0.05],
  [0.6672279960738114, 0.05, 0.05, 0.05, 0.05],
  [0.6672279960738114, 0.05, 0.05, 0.05, 0.05],
  [0.6672279960738114, 0.05, 0.05, 0.05, 0.05],
  [0.6672279960738114, 0.05, 0.05, 0.05, 0.05]],
 [[0.7554034240843558, 0.05, 0.05, 0.05, 0.05],
  [0.7554034240843558, 0.05, 0.05, 0.05, 0.05],
  [0.7554034240843558, 0.05, 0.05, 0.05, 0.05],
  [0.7554034240843558, 0.05, 0.05, 0.05, 0.05],
  [0.7554034240843558, 0.05, 0.05, 0.05, 0.05]],
 [[0.8435788520949002, 0.05, 0.05, 0.05, 0.05],
  [0.8435788520949002, 0.05, 0.05, 0.05, 0.05],
  [0.8435788520949002, 0.05, 0.05, 0.05, 0.05],
  [0.8435788520949002, 0.05, 0.05, 0.05, 0.05],
  [0.8435788520949002, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.0950424415583048, 0.10060643078485161, 0.10652911651764113]
Scheduled Classes by Deep Learning for process id  6494  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lower']
========================================================================================
Process id: 6535
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.8.920785316 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 106265, 136270, 14, 3.965983042031789, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[0.06458165580906777, 0.09254979132638362, 0.11441031295897051]
###############
Output Layer:
###############
[0.4172361718375954, 1.4747425016842244, 0.7753980122992442]
Error before Backpropagation:
1.31060931706
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
88.6988099869
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[0.529328170763862, -1.245634454965258, 0.4472843176998061]
###############
Output Layer:
###############
[0.06838065368725083, -13.237241169415922, 0.8512648564623659]
Weights updated in this iteration:
[0.33311770135583574, 0.48604907267714426, 0.6284225690960643, -0.8873803336629763, -1.2342966560852655, -1.5602250851534656, 0.26443257623660166, 0.35265821421719346, 0.5299968194437753, 0.22391921520764083, 0.3312858325389287, 0.4392275216171791, 0.06396772231774955, 11.091669983960067, 0.6733226926123296, 0.7622911375765351, 0.19895268943297506, 0.8663432835318782, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
6.13730845596e+19
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[1195.821664796451, -3536110.7979313256, 2376.087615263507]
###############
Output Layer:
###############
[-1181966.4534308335, -11079086896.04521, -1127219.2135905954]
Weights updated in this iteration:
[2113.593544252572, -4972.515852239482, 1786.3415212086857, -6251793.985890626, 14711947.456400331, -5282790.508397277, 4200.363893833771, -9883.475356594672, 3549.630017490697, 0.2226249921832359, 0.33433144560214745, 0.4381338982387238, -1326.444040945338, 3132.678941636862, -1120.2309902670108, 0.7112712978262308, 0.3190144465619686, 0.8232313248788241, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
5.29616243731e+197
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[2.4359064589999887e+54, 2.1402173271063246e+62, 2.2154762014104987e+54]
###############
Output Layer:
###############
[1.2496821609463403e+87, 1.0291902095637211e+99, 1.083950146598629e+87]
Weights updated in this iteration:
[-2.1048761308917396e+52, 6.224235129592779e+55, -4.182371212623036e+52, -1.849370015873439e+60, 5.468689416672879e+63, -3.6746827057231245e+60, -1.9144014983326092e+52, 5.66099110688226e+55, -3.8038997157693193e+52, -1.974614162097166e+21, 5.839043283706052e+24, -3.9235417734981715e+21, -1.626216350689779e+33, 4.808811687171959e+36, -3.2312782452145345e+33, -1.71274214945992e+21, 5.064673259459838e+24, -3.4032042814379293e+21, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03965983042031789, 0.7798121376678653]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.109526928481
0.569913930495
0.109526928481
self.sum1_output_product1_level3 =  0.0624209223055
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0624209223055
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0595159012766
self.product1_product2_sum1_level2 =  0.145073452072
0.569913930495
0.145073452072
self.sum1_output_product1_level3 =  0.0826793812811
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0826793812811
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0788315473752
self.product1_product2_sum1_level2 =  0.156662839732
0.569913930495
0.156662839732
self.sum1_output_product1_level3 =  0.089284334754
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.089284334754
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0851291114661
self.product1_product2_sum1_level2 =  0.160441378186
0.569913930495
0.160441378186
self.sum1_output_product1_level3 =  0.0914377764561
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0914377764561
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0871823336714
self.product1_product2_sum1_level2 =  0.161673311509
0.569913930495
0.161673311509
self.sum1_output_product1_level3 =  0.0921398724184
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0921398724184
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0878517546353
self.product1_product2_sum1_level2 =  0.162074964088
0.569913930495
0.162074964088
self.sum1_output_product1_level3 =  0.092368779818
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.092368779818
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0880700088631
self.product1_product2_sum1_level2 =  0.162205916624
0.569913930495
0.162205916624
self.sum1_output_product1_level3 =  0.0924434114929
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0924434114929
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0881411672381
self.product1_product2_sum1_level2 =  0.162248611649
0.569913930495
0.162248611649
self.sum1_output_product1_level3 =  0.0924677439824
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0924677439824
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0881643673125
self.product1_product2_sum1_level2 =  0.162262531694
0.569913930495
0.162262531694
self.sum1_output_product1_level3 =  0.0924756772098
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0924756772098
####################################################################################################################
self.cell_input_product1_level1 =  0.364546371021
self.forget_feedback_product2_level1 =  0.0881719313336
self.product1_product2_sum1_level2 =  0.162267070107
0.569913930495
0.162267070107
self.sum1_output_product1_level3 =  0.0924782637144
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0924782637144
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0924782637144
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6052376258844506,
   0.6052376258844506,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3276188129422253,
   0.3276188129422253,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6845572867250863,
   0.6845572867250863,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3672786433625431,
   0.3672786433625431,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7638769475657221,
   0.7638769475657221,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.406938473782861,
   0.406938473782861,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.6052376258844506, 0.05, 0.05, 0.05, 0.05],
  [0.6052376258844506, 0.05, 0.05, 0.05, 0.05],
  [0.6052376258844506, 0.05, 0.05, 0.05, 0.05],
  [0.6052376258844506, 0.05, 0.05, 0.05, 0.05],
  [0.6052376258844506, 0.05, 0.05, 0.05, 0.05]],
 [[0.6845572867250863, 0.05, 0.05, 0.05, 0.05],
  [0.6845572867250863, 0.05, 0.05, 0.05, 0.05],
  [0.6845572867250863, 0.05, 0.05, 0.05, 0.05],
  [0.6845572867250863, 0.05, 0.05, 0.05, 0.05],
  [0.6845572867250863, 0.05, 0.05, 0.05, 0.05]],
 [[0.7638769475657221, 0.05, 0.05, 0.05, 0.05],
  [0.7638769475657221, 0.05, 0.05, 0.05, 0.05],
  [0.7638769475657221, 0.05, 0.05, 0.05, 0.05],
  [0.7638769475657221, 0.05, 0.05, 0.05, 0.05],
  [0.7638769475657221, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.09131513951156675, 0.09611036070027376, 0.10115907429908169]
Scheduled Classes by Deep Learning for process id  6535  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6547
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.9.773649401 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 108202, 137776, 15, 4.085365936899758, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[0.06491907906155012, 0.09315506854277289, 0.11509959448117003]
###############
Output Layer:
###############
[0.4200103651177784, 1.4845002029311336, 0.7806558179237972]
Error before Backpropagation:
1.32915718594
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
105.677185472
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[0.5557949461339624, -1.3561695376172338, 0.46014572487471833]
###############
Output Layer:
###############
[0.04012807357310433, -14.457926229999671, 0.8542554195578606]
Weights updated in this iteration:
[0.34829713828748465, 0.5084365397746304, 0.6557907547966384, -0.9558316794584617, -1.3342648415179819, -1.682898831630312, 0.27079559420078064, 0.3622227221513853, 0.541604509145438, 0.22383222190890717, 0.33114960040810776, 0.43906471306437417, 0.06651339110123108, 11.095442812754213, 0.6779262622635295, 0.7623225014135407, 0.1989832400983296, 0.8663880235717238, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
3.2415053853e+20
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[995.2566128166789, -5771493.500245249, 3215.682636145131]
###############
Output Layer:
###############
[-1914058.2957972747, -25461757005.59514, -1892491.5187180932]
Weights updated in this iteration:
[1807.2256435224317, -4408.3693922741195, 1496.5795880022117, -10483935.645148134, 25581362.726200424, -8679709.479291731, 5840.733150860153, -14250.67979719356, 4835.892848274505, 0.22361540006075306, 0.3316786573698557, 0.43888520505124534, -1803.2684939999092, 4411.329774114089, -1492.3131488452948, 0.7094374077814375, 0.32802571066553543, 0.8226041606314766, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
1.85076784526e+205
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[3.6088718653514367e+56, 6.386128489011117e+64, 3.52800366189971e+56]
###############
Output Layer:
###############
[2.584589521670023e+90, 6.084024729174e+102, 2.498204227492297e+90]
Weights updated in this iteration:
[-1.5397989709029163e+54, 8.92929485502191e+57, -4.975103656808047e+54, -2.7247722951436874e+62, 1.5800955641544448e+66, -8.803762611680115e+62, -1.5052948984116615e+54, 8.729205724690306e+57, -4.863620703208362e+54, -6.979121964436541e+21, 4.047193109440179e+25, -2.2549602813552466e+22, -1.6428585763250029e+34, 9.52693754854599e+37, -5.308090124193435e+34, -6.745857262654991e+21, 3.911922899442906e+25, -2.17959225651775e+22, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.04085365936899758, 0.7853472302868424]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.110336600443
0.569913930495
0.110336600443
self.sum1_output_product1_level3 =  0.062882365636
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.062882365636
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0599558693944
self.product1_product2_sum1_level2 =  0.146147104905
0.569913930495
0.146147104905
self.sum1_output_product1_level3 =  0.0832912709872
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0832912709872
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0794149602118
self.product1_product2_sum1_level2 =  0.157822559396
0.569913930495
0.157822559396
self.sum1_output_product1_level3 =  0.0899452751461
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0899452751461
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.085759292208
self.product1_product2_sum1_level2 =  0.161629158594
0.569913930495
0.161629158594
self.sum1_output_product1_level3 =  0.0921147090568
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0921147090568
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0878277623569
self.product1_product2_sum1_level2 =  0.162870240683
0.569913930495
0.162870240683
self.sum1_output_product1_level3 =  0.0928220190283
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0928220190283
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0885021546741
self.product1_product2_sum1_level2 =  0.163274876073
0.569913930495
0.163274876073
self.sum1_output_product1_level3 =  0.0930526263741
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0930526263741
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0887220297339
self.product1_product2_sum1_level2 =  0.163406801109
0.569913930495
0.163406801109
self.sum1_output_product1_level3 =  0.0931278122898
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0931278122898
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0887937165558
self.product1_product2_sum1_level2 =  0.163449813202
0.569913930495
0.163449813202
self.sum1_output_product1_level3 =  0.0931523254808
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0931523254808
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.088817088922
self.product1_product2_sum1_level2 =  0.163463836622
0.569913930495
0.163463836622
self.sum1_output_product1_level3 =  0.0931603176231
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0931603176231
####################################################################################################################
self.cell_input_product1_level1 =  0.367245277563
self.forget_feedback_product2_level1 =  0.0888247091162
self.product1_product2_sum1_level2 =  0.163468408739
0.569913930495
0.163468408739
self.sum1_output_product1_level3 =  0.093162923336
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.093162923336
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.093162923336
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6219512311659663,
   0.6219512311659663,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3359756155829831,
   0.3359756155829831,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7036585499039614,
   0.7036585499039614,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.37682927495198065,
   0.37682927495198065,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7853658686419565,
   0.7853658686419565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4176829343209782,
   0.4176829343209782,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.6219512311659663, 0.05, 0.05, 0.05, 0.05],
  [0.6219512311659663, 0.05, 0.05, 0.05, 0.05],
  [0.6219512311659663, 0.05, 0.05, 0.05, 0.05],
  [0.6219512311659663, 0.05, 0.05, 0.05, 0.05],
  [0.6219512311659663, 0.05, 0.05, 0.05, 0.05]],
 [[0.7036585499039614, 0.05, 0.05, 0.05, 0.05],
  [0.7036585499039614, 0.05, 0.05, 0.05, 0.05],
  [0.7036585499039614, 0.05, 0.05, 0.05, 0.05],
  [0.7036585499039614, 0.05, 0.05, 0.05, 0.05],
  [0.7036585499039614, 0.05, 0.05, 0.05, 0.05]],
 [[0.7853658686419565, 0.05, 0.05, 0.05, 0.05],
  [0.7853658686419565, 0.05, 0.05, 0.05, 0.05],
  [0.7853658686419565, 0.05, 0.05, 0.05, 0.05],
  [0.7853658686419565, 0.05, 0.05, 0.05, 0.05],
  [0.7853658686419565, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.0923063798124477, 0.09730149544072839, 0.10257577006395106]
Scheduled Classes by Deep Learning for process id  6547  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 6559
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.10.475374117 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 101445, 163109, 11, 3.822914690034498, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.055708249772855406, 0.07851593887679446, 0.09543633935808041]
###############
Output Layer:
###############
[0.3393840439402662, 1.2319651215862137, 0.6266962328258286]
Error before Backpropagation:
0.878631789594
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.208043310442
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.13360159656300843, -0.02076845448364287, 0.1767155246229423]
###############
Output Layer:
###############
[0.3190224408293044, 0.16166269952736984, 0.6576419162696504]
Weights updated in this iteration:
[0.07958709822807288, 0.1210769701680903, 0.1752128266193425, -0.04569708541218796, -0.026010837087302202, -0.06395101583670398, 0.10561188383856536, 0.12334014273368321, 0.24439472458212969, 0.22613581504511462, 0.3345537669741288, 0.4433800887983287, 0.018657579582628415, 11.02629623770396, 0.5919631491548986, 0.7630053253631781, 0.20014161370197311, 0.8680171043775213, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.180703344608
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.11473964549125078, -0.03253533101352851, 0.15716751010126057]
###############
Output Layer:
###############
[0.30291445999370287, 0.029830874236337457, 0.6248029642717008]
Weights updated in this iteration:
[0.056492015609467205, 0.12466711567907059, 0.14466484107014052, -0.06010476702406252, -0.023771153674313074, -0.08300813102201521, 0.08167676659165836, 0.12706087217024056, 0.21273562087370945, 0.21774706514974448, 0.33585780356394973, 0.43228424580822, 0.016816799518763113, 11.026582388153912, 0.5895283398182601, 0.7459305036163223, 0.20279590577174375, 0.845432149104058, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.164866496251
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.09773212218074316, -0.03486636403777474, 0.13828596739177704]
###############
Output Layer:
###############
[0.2920835205481611, 0.003937663562625537, 0.5978407577116177]
Weights updated in this iteration:
[0.0362711081256025, 0.13040091340491106, 0.11696674537165505, -0.06287622305940394, -0.022985285467093015, -0.08680440250747083, 0.059227764387720684, 0.13342646371884123, 0.18198553649315735, 0.21113486288602476, 0.33773274564223016, 0.42322701574946064, 0.016916981510109612, 11.026553980760495, 0.5896655666161076, 0.7315455061626794, 0.206874885201907, 0.8257279376876612, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.151275000636
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.0818201093246921, -0.03516819318010156, 0.11978392729863915]
###############
Output Layer:
###############
[0.28346142540378094, 0.00036128447922295637, 0.5745106094968062]
Weights updated in this iteration:
[0.01790509324922059, 0.13695306991512027, 0.09097977120552293, -0.063224601267004, -0.02286100001070492, -0.08729733989981653, 0.0378722799076562, 0.14104512664634786, 0.15176861533611652, 0.2058386416280353, 0.33962219584841885, 0.41573313283932084, 0.016938471373174734, 11.026546314157416, 0.5896959736761076, 0.7196125387135637, 0.21113202377465115, 0.8088433974696171, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.139803156976
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.06681331770115473, -0.03519487691900858, 0.1017229561526905]
###############
Output Layer:
###############
[0.27618296021070476, -0.0001853880902833449, 0.5540883354642542]
Weights updated in this iteration:
[0.0011206970201046301, 0.14416739526702715, 0.0664075613386114, -0.06325444578393506, -0.02284817214057053, -0.08734103201262156, 0.01767192638766933, 0.14972771019164816, 0.12219547375251448, 0.20162647570340736, 0.34143268310108316, 0.409566558618756, 0.016940233676696507, 11.026545556678219, 0.5896985536731116, 0.7099219505832574, 0.21529726479190514, 0.7946564608636626, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.130131387974
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.052524377172462305, -0.035181623232867595, 0.08406248903883831]
###############
Output Layer:
###############
[0.26995926737030446, -0.0002820627221976624, 0.5361484045307737]
Weights updated in this iteration:
[-0.014297268023568887, 0.15228903030513105, 0.042933783414330746, -0.06324014487273656, -0.022855705351414995, -0.08731925894382567, -0.0013839626105078967, 0.1597656740430009, 0.09318297403378523, 0.1983383628839763, 0.34316474386318013, 0.4045604226017998, 0.016939488058601154, 11.026545949443284, 0.5896974184731331, 0.702260843524678, 0.2193328632214887, 0.782992462065, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.121911641247
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.03871459968300308, -0.03516185310301036, 0.06663617963641225]
###############
Output Layer:
###############
[0.26459986712800887, -0.0002989418871612304, 0.5202908579600013]
Weights updated in this iteration:
[-0.02853730348228564, 0.16182722169931493, 0.020143359441656936, -0.06321975878393786, -0.022869360262409044, -0.08728663208638461, -0.019353206792358482, 0.17180174591320121, 0.06442414863899357, 0.1958544076097562, 0.3448285347216222, 0.400584983496135, 0.016938594717758795, 11.026546547816508, 0.5896959887283401, 0.696433047696381, 0.22323640912049109, 0.7736653829198589, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.11480920606
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.02501811512829111, -0.03514080502873958, 0.049068274551595986]
###############
Output Layer:
###############
[0.2599320623237602, -0.00029964055145315083, 0.506069239652605]
Weights updated in this iteration:
[-0.04176062556479621, 0.1738370710173013, -0.0026168311039052525, -0.0631994378421526, -0.022887816399778896, -0.08725165536085286, -0.03631420626076448, 0.18720627486794997, 0.035230608820835646, 0.19408708489020335, 0.34643367444563733, 0.3975430397544995, 0.01693789664442974, 11.026547181829311, 0.5896947871935151, 0.6922752717534357, 0.22701263587055767, 0.7665089533844736, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.10841325568
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[0.010632249824435364, -0.03511839020913175, 0.030375207598630535]
###############
Output Layer:
###############
[0.25573151909318237, -0.00029672746499326275, 0.49278542340174203]
Weights updated in this iteration:
[-0.054097024048571335, 0.19116495409930676, -0.026812330432120134, -0.06318021632569991, -0.022914815218776922, -0.08721395601213047, -0.052344185545629765, 0.20972221476892133, 0.0037908532448170315, 0.19298049789153543, 0.347988002490261, 0.3953726798218097, 0.016937444476235637, 11.026547816951274, 0.5896939003516016, 0.689673338151059, 0.23066734930320346, 0.7614057554931958, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.101418467503
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[-0.00840520157021098, -0.03508870216984818, 0.005374949018907643]
###############
Output Layer:
###############
[0.2512072226040704, -0.00029217521611929476, 0.47764565427861305]
Weights updated in this iteration:
[-0.06563094020655677, 0.22926155406299817, -0.059763502903522454, -0.06316222970822191, -0.022974225133100337, -0.0871625701578745, -0.06749069231320944, 0.25975122497395253, -0.039481106599648694, 0.19252369245627612, 0.34949683380912444, 0.3940676352491836, 0.016937254190817517, 11.026548445465245, 0.5896933567263883, 0.6886029322446602, 0.23420290696256937, 0.7583477195968852, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.03822914690034498, 0.6219460606097763]
###############
Hidden middle Layer:
###############
[-0.00840520157021098, -0.03508870216984818, 0.005374949018907643]
###############
Output Layer:
###############
[0.2512072226040704, -0.00029217521611929476, 0.47764565427861305]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.06563094020655677, 0.22926155406299817, -0.059763502903522454, -0.06316222970822191, -0.022974225133100337, -0.0871625701578745, -0.06749069231320944, 0.25975122497395253, -0.039481106599648694, 0.19252369245627612, 0.34949683380912444, 0.3940676352491836, 0.016937254190817517, 11.026548445465245, 0.5896933567263883, 0.6886029322446602, 0.23420290696256937, 0.7583477195968852, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.108546985614
0.569913930495
0.108546985614
self.sum1_output_product1_level3 =  0.0618624392149
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0618624392149
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0589834095534
self.product1_product2_sum1_level2 =  0.143774014172
0.569913930495
0.143774014172
self.sum1_output_product1_level3 =  0.08193881352
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.08193881352
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0781254450601
self.product1_product2_sum1_level2 =  0.155259235476
0.569913930495
0.155259235476
self.sum1_output_product1_level3 =  0.088484401136
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.088484401136
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0843664061347
self.product1_product2_sum1_level2 =  0.159003812121
0.569913930495
0.159003812121
self.sum1_output_product1_level3 =  0.0906184875296
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0906184875296
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0864011738124
self.product1_product2_sum1_level2 =  0.160224672728
0.569913930495
0.160224672728
self.sum1_output_product1_level3 =  0.0913142729965
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0913142729965
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0870645779665
self.product1_product2_sum1_level2 =  0.16062271522
0.569913930495
0.16062271522
self.sum1_output_product1_level3 =  0.0915411229579
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0915411229579
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0872808705076
self.product1_product2_sum1_level2 =  0.160752490745
0.569913930495
0.160752490745
self.sum1_output_product1_level3 =  0.0916150838373
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0916150838373
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0873513893053
self.product1_product2_sum1_level2 =  0.160794802023
0.569913930495
0.160794802023
self.sum1_output_product1_level3 =  0.0916391976244
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0916391976244
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0873743808556
self.product1_product2_sum1_level2 =  0.160808596954
0.569913930495
0.160808596954
self.sum1_output_product1_level3 =  0.0916470595473
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0916470595473
####################################################################################################################
self.cell_input_product1_level1 =  0.361279894801
self.forget_feedback_product2_level1 =  0.0873818768907
self.product1_product2_sum1_level2 =  0.160813094575
0.569913930495
0.160813094575
self.sum1_output_product1_level3 =  0.0916496228042
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0916496228042
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0916496228042
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5852080566048298,
   0.5852080566048298,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.31760402830241485,
   0.31760402830241485,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.6616663504055197,
   0.6616663504055197,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3558331752027598,
   0.3558331752027598,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7381246442062096,
   0.7381246442062096,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.39406232210310477,
   0.39406232210310477,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.5852080566048298, 0.05, 0.05, 0.05, 0.05],
  [0.5852080566048298, 0.05, 0.05, 0.05, 0.05],
  [0.5852080566048298, 0.05, 0.05, 0.05, 0.05],
  [0.5852080566048298, 0.05, 0.05, 0.05, 0.05],
  [0.5852080566048298, 0.05, 0.05, 0.05, 0.05]],
 [[0.6616663504055197, 0.05, 0.05, 0.05, 0.05],
  [0.6616663504055197, 0.05, 0.05, 0.05, 0.05],
  [0.6616663504055197, 0.05, 0.05, 0.05, 0.05],
  [0.6616663504055197, 0.05, 0.05, 0.05, 0.05],
  [0.6616663504055197, 0.05, 0.05, 0.05, 0.05]],
 [[0.7381246442062096, 0.05, 0.05, 0.05, 0.05],
  [0.7381246442062096, 0.05, 0.05, 0.05, 0.05],
  [0.7381246442062096, 0.05, 0.05, 0.05, 0.05],
  [0.7381246442062096, 0.05, 0.05, 0.05, 0.05],
  [0.7381246442062096, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.09013983227684777, 0.09470219962320625, 0.09948970084223226]
Scheduled Classes by Deep Learning for process id  6559  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 7467
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/*NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.25.1917773774 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 3205, 8336, 7, 1.7661023566688554, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[0.04193691371477856, 0.05558241446170101, 0.06650811757935343]
###############
Output Layer:
###############
[0.22155814638807608, 0.8467136608061583, 0.4023034618833252]
Error before Backpropagation:
0.376573179925
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.286749754492
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[0.028397592856785225, 0.04539361537756112, 0.04763179070134658]
###############
Output Layer:
###############
[0.21401239683918538, 0.7316466761640308, 0.3875393909905695]
Weights updated in this iteration:
[-0.01138396468296114, -0.005341913664091841, 0.022086983722268166, 0.026907839834639534, 0.0776717467580372, 0.06247930270542483, 0.003186783256191039, -0.01851388938063307, 0.07271895928962342, 0.22861448576175547, 0.338163664900177, 0.44780270087370394, -0.004282064223798422, 10.994324630800026, 0.5532090369640538, 0.7668507453839428, 0.20582603582834877, 0.875005569610664, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.169677335075
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[0.013624257825584005, 0.028149842162094853, 0.026786021069982596]
###############
Output Layer:
###############
[0.20418540193141685, 0.5394917713659064, 0.37125034595032974]
Weights updated in this iteration:
[-0.03333147309600803, -0.040425054704322304, -0.01472596491103504, 0.0012902080054136393, 0.03672190526955365, 0.019510390385721353, -0.027782033425657267, -0.06801761176455783, 0.02077440885193682, 0.22773549717147856, 0.3367585996825186, 0.44632835758087314, -0.008026882787324734, 10.988338529992513, 0.5469277857702652, 0.7648452576393675, 0.20262025920563365, 0.8716417291265258, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0869250080224
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[0.0014669698633425696, 0.009562284330090936, 0.009321980198236286]
###############
Output Layer:
###############
[0.19485755932081714, 0.3340817089794786, 0.3574905941945003]
Weights updated in this iteration:
[-0.04867397579606134, -0.07212505883717142, -0.04489014749264857, -0.02216729806918449, -0.011744961841280231, -0.026608323749556448, -0.04982166060737689, -0.11355491988212707, -0.022556679389473304, 0.22734987570724755, 0.3359618455773493, 0.44557020511501677, -0.009649874237511015, 10.984985176456632, 0.5437368975958315, 0.7639508190148434, 0.2007722094677057, 0.8698832148537471, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0422932643925
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.0175478092127422, -0.017401133905433668, -0.018516493153805862]
###############
Output Layer:
###############
[0.1813881418766826, 0.03786562453366413, 0.3373423310048578]
Weights updated in this iteration:
[-0.056106525104928286, -0.1205733273154787, -0.09212089108938502, -0.03270683338375431, -0.08044578242617878, -0.09358266450197347, -0.06070323988025244, -0.18448531700055892, -0.09170456650158659, 0.22731193372327602, 0.33571452486010095, 0.4453290996689522, -0.009739323113618657, 10.98440211363597, 0.5431684873832795, 0.7638606880921703, 0.20018470080030207, 0.8693104705204089, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.042142803111
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.018974256155448118, -0.017751119005055545, -0.020663971380379068]
###############
Output Layer:
###############
[0.18109200263574718, 0.03402991681210138, 0.3365408033991195]
Weights updated in this iteration:
[-0.059477031092951285, -0.12391566055580512, -0.09567745762686317, -0.03353380201376635, -0.08126583874728256, -0.0944552839150516, -0.06577744769346672, -0.18951711148562908, -0.09705888356164129, 0.22770639255651248, 0.33610568656548034, 0.44574533363847685, -0.009753473589072047, 10.984388081438833, 0.543153555765396, 0.7648309331143457, 0.20114683592197266, 0.8703342755488781, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0420069712127
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.020430030766975398, -0.0180738081236857, -0.022855825786848232]
###############
Output Layer:
###############
[0.18081419767794513, 0.030493395598908957, 0.33576081970243504]
Weights updated in this iteration:
[-0.06282180667289472, -0.12704482192468078, -0.09932009564605616, -0.034275209835664865, -0.08195945320942424, -0.09526271635864182, -0.07081343405804422, -0.19422846314722159, -0.10254333983002199, 0.22813154102676902, 0.336503428697162, 0.4462083428731904, -0.009769671636406567, 10.984372927565948, 0.5431359152326766, 0.765875426061615, 0.20212399773822623, 0.8714717837701521, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0418837419362
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.02191004281366434, -0.01836920565731257, -0.025084285619427677]
###############
Output Layer:
###############
[0.18055604406235196, 0.027256112696210477, 0.335006529123061]
Weights updated in this iteration:
[-0.06614206066807599, -0.12998214660692312, -0.10303458573897463, -0.03493790365858712, -0.08254571767162025, -0.09600409627820687, -0.07581275327221945, -0.19865120422170482, -0.1081362618532593, 0.22858792059597222, 0.3369071734031505, 0.446718911459472, -0.009787493159374912, 10.984357161422947, 0.5431159776403711, 0.7669952149997425, 0.20311463992753997, 0.8727245327835924, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0417713040889
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.023410122383208696, -0.018638015666444723, -0.027342960185991177]
###############
Output Layer:
###############
[0.18031836131642298, 0.02431045939107132, 0.3342810985732318]
Weights updated in this iteration:
[-0.06943904482010604, -0.13274631190101235, -0.10680922428512438, -0.03552871387826432, -0.08304104826143198, -0.0966805008121139, -0.08077703276894986, -0.20281321667377314, -0.11381974708647816, 0.2290759800259138, 0.33731635854796443, 0.4472776790640828, -0.00980651427080368, 10.984341214274389, 0.5430942008234139, 0.7681911058324146, 0.2041172652914369, 0.8740936796844689, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0416681413576
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.024926866659139946, -0.01888142208204539, -0.02962661213820238]
###############
Output Layer:
###############
[0.18010159098367876, 0.021643566719708462, 0.3335869026147532]
Weights updated in this iteration:
[-0.07271403843337558, -0.13535370460372653, -0.11063440804585663, -0.03605428332933844, -0.08345948142904114, -0.09729436452202772, -0.08570795334475378, -0.20673897893325396, -0.11957904910025992, 0.22959609681827917, 0.3377304505704581, 0.4478851740832422, -0.00982633179773285, 10.984325436510765, 0.5430710540050872, 0.7694637185446136, 0.20513045844263095, 0.8755800879207113, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0415730536868
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.026457508474618478, -0.019100915710033164, -0.031930962414860264]
###############
Output Layer:
###############
[0.17990589358251144, 0.019239194853301694, 0.33292568247653626]
Weights updated in this iteration:
[-0.075968337650254, -0.13781874756060458, -0.11450227728102826, -0.0365209489688415, -0.08381296793142165, -0.09784901593846292, -0.09060723495649833, -0.210450051218235, -0.12540204795734897, 0.23014859465136406, 0.3381489526200154, 0.4485418406122462, -0.009846577453237639, 10.98431010097854, 0.5430469912060468, 0.7708135331978729, 0.20615290625162477, 0.87718439847195, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.017661023566688555, 0.38447696737044146]
###############
Hidden middle Layer:
###############
[-0.026457508474618478, -0.019100915710033164, -0.031930962414860264]
###############
Output Layer:
###############
[0.17990589358251144, 0.019239194853301694, 0.33292568247653626]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.075968337650254, -0.13781874756060458, -0.11450227728102826, -0.0365209489688415, -0.08381296793142165, -0.09784901593846292, -0.09060723495649833, -0.210450051218235, -0.12540204795734897, 0.23014859465136406, 0.3381489526200154, 0.4485418406122462, -0.009846577453237639, 10.98431010097854, 0.5430469912060468, 0.7708135331978729, 0.20615290625162477, 0.87718439847195, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.093465216152
0.569913930495
0.093465216152
self.sum1_output_product1_level3 =  0.0532671287018
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0532671287018
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0507881180862
self.product1_product2_sum1_level2 =  0.12377506983
0.569913930495
0.12377506983
self.sum1_output_product1_level3 =  0.0705411365439
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0705411365439
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.067258207079
self.product1_product2_sum1_level2 =  0.133657123225
0.569913930495
0.133657123225
self.sum1_output_product1_level3 =  0.076173056436
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.076173056436
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0726280218128
self.product1_product2_sum1_level2 =  0.136879012065
0.569913930495
0.136879012065
self.sum1_output_product1_level3 =  0.0780092557686
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0780092557686
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0743787658609
self.product1_product2_sum1_level2 =  0.137929458494
0.569913930495
0.137929458494
self.sum1_output_product1_level3 =  0.0786079198216
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0786079198216
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0749495685559
self.product1_product2_sum1_level2 =  0.138271940111
0.569913930495
0.138271940111
self.sum1_output_product1_level3 =  0.0788031048661
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0788031048661
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0751356698407
self.product1_product2_sum1_level2 =  0.138383600882
0.569913930495
0.138383600882
self.sum1_output_product1_level3 =  0.0788667418949
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0788667418949
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0751963452519
self.product1_product2_sum1_level2 =  0.138420006129
0.569913930495
0.138420006129
self.sum1_output_product1_level3 =  0.0788874897521
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0788874897521
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.07521612752
self.product1_product2_sum1_level2 =  0.13843187549
0.569913930495
0.13843187549
self.sum1_output_product1_level3 =  0.0788942542663
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0788942542663
####################################################################################################################
self.cell_input_product1_level1 =  0.311007329926
self.forget_feedback_product2_level1 =  0.0752225772189
self.product1_product2_sum1_level2 =  0.138435745309
0.569913930495
0.138435745309
self.sum1_output_product1_level3 =  0.0788964597302
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0788964597302
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0788964597302
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.29725432993363976,
   0.29725432993363976,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.1736271649668199,
   0.1736271649668199,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.33257637706701687,
   0.33257637706701687,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.19128818853350843,
   0.19128818853350843,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3678984242003939,
   0.3678984242003939,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20894921210019696,
   0.20894921210019696,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.29725432993363976, 0.05, 0.05, 0.05, 0.05],
  [0.29725432993363976, 0.05, 0.05, 0.05, 0.05],
  [0.29725432993363976, 0.05, 0.05, 0.05, 0.05],
  [0.29725432993363976, 0.05, 0.05, 0.05, 0.05],
  [0.29725432993363976, 0.05, 0.05, 0.05, 0.05]],
 [[0.33257637706701687, 0.05, 0.05, 0.05, 0.05],
  [0.33257637706701687, 0.05, 0.05, 0.05, 0.05],
  [0.33257637706701687, 0.05, 0.05, 0.05, 0.05],
  [0.33257637706701687, 0.05, 0.05, 0.05, 0.05],
  [0.33257637706701687, 0.05, 0.05, 0.05, 0.05]],
 [[0.3678984242003939, 0.05, 0.05, 0.05, 0.05],
  [0.3678984242003939, 0.05, 0.05, 0.05, 0.05],
  [0.3678984242003939, 0.05, 0.05, 0.05, 0.05],
  [0.3678984242003939, 0.05, 0.05, 0.05, 0.05],
  [0.3678984242003939, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.07444853061454183, 0.07627624431167745, 0.07812607378156101]
Scheduled Classes by Deep Learning for process id  7467  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 8390
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/*NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.36.1248002292 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 22239, 39040, 9, 2.177765157668696, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[0.05240109090429659, 0.07228488094871496, 0.08881491265032354]
###############
Output Layer:
###############
[0.31297004280760055, 1.1341357396161924, 0.5768175994217681]
Error before Backpropagation:
0.735415503677
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.584438830546
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[0.07332298980574566, 0.0574824787549965, 0.11256628762333201]
###############
Output Layer:
###############
[0.30918437761669704, 0.9808833580833697, 0.5828565564055588]
Weights updated in this iteration:
[0.031015661434231866, 0.05199013281243334, 0.09161956635557436, 0.02813126010302188, 0.078489261105777, 0.06279888658077129, 0.056857817943841725, 0.05391075624582675, 0.1604367538946767, 0.22681169960796294, 0.33560188709261696, 0.44459613118858665, 0.008562666712002601, 11.011811802639969, 0.5745129134328383, 0.7637730884126162, 0.2014102635077906, 0.8694459714641276, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.551318373569
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[0.05328554821793066, 0.055723374623478664, 0.08975710695398852]
###############
Output Layer:
###############
[0.29980160037878467, 0.960110830099195, 0.5586622847094542]
Weights updated in this iteration:
[0.008542101003545032, 0.03437170285712815, 0.05711790086071217, 0.02615828701510144, 0.07694252407403529, 0.05976995586372976, 0.031275535001159815, 0.0338552042333829, 0.12116254225359963, 0.22243938936509997, 0.3321741595638913, 0.4378837107911033, 0.007296549941096349, 11.01081921449568, 0.572569156570548, 0.7549867523200854, 0.19452210497645941, 0.8559570883952927, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.493868310284
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[0.02964738285056722, 0.051410815390352874, 0.06199329453746549]
###############
Output Layer:
###############
[0.28996345893345826, 0.9105377568872108, 0.5335551202469756]
Weights updated in this iteration:
[-0.015521560565309248, 0.009207123092841222, 0.01658374634086747, 0.02176810003278776, 0.07235148500126351, 0.052374883546787016, 0.0030119631262126093, 0.004298567275384444, 0.07355382791127965, 0.21942146035082027, 0.329018159585023, 0.4328001452395886, 0.005459665681381719, 11.008898292347606, 0.569475007735164, 0.748829459052693, 0.18808311408905648, 0.8455854050997, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.356975014614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[-0.005142882900045866, 0.037645964311724026, 0.019938705163213298]
###############
Output Layer:
###############
[0.2758153076231421, 0.7562417352079007, 0.5008190720016352]
Weights updated in this iteration:
[-0.04383149022968309, -0.03988444681148687, -0.042612905750292814, 0.01056720928162424, 0.052928289047744076, 0.02895358788238512, -0.03120917435469965, -0.05504348629968872, 0.0019967172565459324, 0.2178346587135991, 0.3262665249316888, 0.4294821099929495, 0.0034055840377631786, 11.005336358644755, 0.5651798802705011, 0.7455567034072752, 0.1824079070344448, 0.8387420049352345, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
1.24026686727
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[0.07149631926734452, 0.10837589944825465, 0.11586156360441298]
###############
Output Layer:
###############
[0.3148468521100021, 1.5329752694774679, 0.5693118618282617]
Weights updated in this iteration:
[-0.07619732593894007, 0.19703385751524496, 0.08286785438175695, -0.019303058096099217, 0.27157900335302276, 0.14475915179780402, -0.0717187750456002, 0.24148727505173775, 0.15905045543288363, 0.2180871714372047, 0.3244181287760951, 0.4285031305287491, 0.004065648999649233, 11.000504675232643, 0.5626208407357888, 0.7460849005849485, 0.17854149737925665, 0.836694210342522, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
93.7985396579
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[0.4342280473874438, -1.2654785942393518, 0.5281599882494837]
###############
Output Layer:
###############
[-0.04928520893025484, -13.62690403503163, 0.5993634333808242]
Weights updated in this iteration:
[0.3031746217182438, 0.7720952938815557, 0.6976495311678628, -1.4561827536469478, -1.906478986205372, -2.183740132416292, 0.3594937278810829, 0.8951299147974936, 0.8578410413935423, 0.21369395983827774, 0.3177587893390553, 0.42138382186625123, 0.09010979849440659, 11.130932537664068, 0.7020575250523172, 0.7376822751469037, 0.16580458869770964, 0.8230775468329817, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
1.30427926407e+20
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[-1209.5830798157383, -4663705.916492371, 5619.052393960746]
###############
Output Layer:
###############
[-1506388.1052986018, -16151032422.269648, -1495954.054762084]
Weights updated in this iteration:
[-1922.174601779119, 5603.483593488263, -2337.6493225329873, -7409704.184406378, 21594227.781492718, -9012567.192878969, 8927.081372654508, -26014.41070414217, 10858.60263907267, 0.2119135491222177, 0.32294747225275494, 0.4192182738125703, -1184.5125501537887, 3463.439731722707, -1440.1531122987767, 0.6845710322449182, 0.32058763739824536, 0.7584773129333066, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
4.02194130505e+200
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[-1.2556664891099837e+55, -1.4434469865817425e+63, -1.2383319010468087e+55]
###############
Output Layer:
###############
[-2.3011393834698827e+88, -2.836173938620001e+100, -2.2536531542637306e+88]
Weights updated in this iteration:
[-1.5440992060723786e+53, -5.953476634368862e+56, 7.173028860254559e+53, -1.7750137996979013e+61, -6.843798080217654e+64, 8.245730042805514e+61, -1.522782778583012e+53, -5.871288357548899e+56, 7.074004556001674e+53, 4.134725031778833e+21, 1.5941973656504475e+25, -1.9207640199237493e+22, 5.0960839064029517e+33, 1.9648618653671553e+37, -2.367358055178853e+34, 4.049400995358493e+21, 1.5612995663911218e+25, -1.881127202981573e+22, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.021777651576686957, 0.5696465163934427]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.096610335584
0.569913930495
0.096610335584
self.sum1_output_product1_level3 =  0.0550595760792
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0550595760792
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.052497146361
self.product1_product2_sum1_level2 =  0.127945606226
0.569913930495
0.127945606226
self.sum1_output_product1_level3 =  0.0729179833341
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0729179833341
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0695244372738
self.product1_product2_sum1_level2 =  0.138161980774
0.569913930495
0.138161980774
self.sum1_output_product1_level3 =  0.078740437508
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.078740437508
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0750759189725
self.product1_product2_sum1_level2 =  0.141492869793
0.569913930495
0.141492869793
self.sum1_output_product1_level3 =  0.080638757561
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.080638757561
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0768858926912
self.product1_product2_sum1_level2 =  0.142578854024
0.569913930495
0.142578854024
self.sum1_output_product1_level3 =  0.0812576751026
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0812576751026
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0774760062933
self.product1_product2_sum1_level2 =  0.142932922186
0.569913930495
0.142932922186
self.sum1_output_product1_level3 =  0.0814594634801
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0814594634801
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0776684035971
self.product1_product2_sum1_level2 =  0.143048360568
0.569913930495
0.143048360568
self.sum1_output_product1_level3 =  0.0815252534223
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0815252534223
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0777311317266
self.product1_product2_sum1_level2 =  0.143085997446
0.569913930495
0.143085997446
self.sum1_output_product1_level3 =  0.0815467032032
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0815467032032
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0777515832515
self.product1_product2_sum1_level2 =  0.143098268361
0.569913930495
0.143098268361
self.sum1_output_product1_level3 =  0.0815536965685
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0815536965685
####################################################################################################################
self.cell_input_product1_level1 =  0.321491061366
self.forget_feedback_product2_level1 =  0.0777582511511
self.product1_product2_sum1_level2 =  0.1431022691
0.569913930495
0.1431022691
self.sum1_output_product1_level3 =  0.0815559766458
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0815559766458
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0815559766458
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.35488712207361744,
   0.35488712207361744,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.20244356103680872,
   0.20244356103680872,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3984424252269913,
   0.3984424252269913,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.22422121261349565,
   0.22422121261349565,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4419977283803652,
   0.4419977283803652,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24599886419018263,
   0.24599886419018263,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.35488712207361744, 0.05, 0.05, 0.05, 0.05],
  [0.35488712207361744, 0.05, 0.05, 0.05, 0.05],
  [0.35488712207361744, 0.05, 0.05, 0.05, 0.05],
  [0.35488712207361744, 0.05, 0.05, 0.05, 0.05],
  [0.35488712207361744, 0.05, 0.05, 0.05, 0.05]],
 [[0.3984424252269913, 0.05, 0.05, 0.05, 0.05],
  [0.3984424252269913, 0.05, 0.05, 0.05, 0.05],
  [0.3984424252269913, 0.05, 0.05, 0.05, 0.05],
  [0.3984424252269913, 0.05, 0.05, 0.05, 0.05],
  [0.3984424252269913, 0.05, 0.05, 0.05, 0.05]],
 [[0.4419977283803652, 0.05, 0.05, 0.05, 0.05],
  [0.4419977283803652, 0.05, 0.05, 0.05, 0.05],
  [0.4419977283803652, 0.05, 0.05, 0.05, 0.05],
  [0.4419977283803652, 0.05, 0.05, 0.05, 0.05],
  [0.4419977283803652, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.0774419491368066, 0.07974529031096365, 0.08208896653094946]
Scheduled Classes by Deep Learning for process id  8390  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 8770
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/*NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.40.945998663 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 164363, 371895, 7, 2.79065195756549, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[0.04539165298604318, 0.06165529306568045, 0.07362132913066687]
###############
Output Layer:
###############
[0.2502852256525392, 0.9457062540805161, 0.4568246606199523]
Error before Backpropagation:
0.483780740399
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.44080343017
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[0.03194705065990905, 0.05797635272785133, 0.055842372842647175]
###############
Output Layer:
###############
[0.2443894894744744, 0.9035296686775858, 0.44228071931795276]
Weights updated in this iteration:
[-0.0078138523681544, -0.001196481426332531, 0.02710747456841344, 0.0381254712888053, 0.09237894796785234, 0.08009393668227738, 0.009443207847136034, -0.010997092600234824, 0.08179292371480815, 0.228123741589408, 0.3374514860208425, 0.44695687138728246, -0.002064295077432024, 10.99719607659998, 0.5566518922021817, 0.7658683444546694, 0.20438798949286724, 0.8732988126065633, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.359478694406
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[0.014535735224395148, 0.049757953070557887, 0.03258478152025508]
###############
Output Layer:
###############
[0.23653456785039284, 0.8111253970779382, 0.42483055260724095]
Weights updated in this iteration:
[-0.028965232197477687, -0.039581240968611285, -0.009864427819050343, 0.028141705651774138, 0.07426077097910468, 0.06264264926887174, -0.01881026615355304, -0.06227047592092819, 0.03240679896600765, 0.22685896221400248, 0.335156210129117, 0.4447460794679608, -0.004413210181685443, 10.992933350376465, 0.5525460673977639, 0.7630922559312738, 0.19935004438959358, 0.8684463029956465, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.190553093437
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.012322984709427904, 0.026972226687630184, -0.0037575566179039906]
###############
Output Layer:
###############
[0.22222414875517543, 0.5589709424950238, 0.39851914374077857]
Weights updated in this iteration:
[-0.05369081914993465, -0.12422055065812485, -0.06529181668987405, 0.007165633063649906, 0.002456598755822298, 0.015620552905288514, -0.05226627760550093, -0.17679530882791591, -0.042591596840712675, 0.22631681934392503, 0.33330037558959, 0.44353075690590765, -0.006085883056601034, 10.987207545859004, 0.5487964338675309, 0.761903004633312, 0.19527906281490345, 0.8657803564082506, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.179721957444
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.013667997216182535, 0.025053836020591233, -0.005622869177323606]
###############
Output Layer:
###############
[0.22130747712151955, 0.5380018078523432, 0.39718013571209865]
Weights updated in this iteration:
[-0.0719449173736011, -0.08426645780833693, -0.0708579039002015, -0.018870178121474176, 0.05944310144491847, 0.00768164549543719, -0.0775817289537135, -0.1213855109125404, -0.05031085025144898, 0.22672623982294168, 0.33240424675140795, 0.4436555984655387, -0.004570063167250666, 10.983889758923677, 0.5492586416192309, 0.7628143206952757, 0.1932843980492678, 0.8660582372772428, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.162358456941
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.015929377802194524, 0.021792199369012995, -0.008764727825614795]
###############
Output Layer:
###############
[0.21976733179437496, 0.5023912033049973, 0.39495130251201993]
Weights updated in this iteration:
[-0.08925168653353904, -0.0525426470904112, -0.07797772528509708, -0.04383209428968798, 0.1051990190491251, -0.002587422225984761, -0.10162696162948472, -0.07730990078419774, -0.06020280410069995, 0.22717684784386025, 0.33157826915665906, 0.4438409738290102, -0.0029461663449014963, 10.980913108866028, 0.5499266955590378, 0.763819568386544, 0.1914417498237256, 0.866471785522052, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.139689027388
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.019145518772383626, 0.0171032247003592, -0.013246829372216311]
###############
Output Layer:
###############
[0.21758593176060856, 0.4512649146597102, 0.3918299844917176]
Weights updated in this iteration:
[-0.10494018407839158, -0.031079984170168513, -0.08660991498396371, -0.0667051487543141, 0.13649052140791498, -0.01517272840757916, -0.12349087859127862, -0.0473989498772943, -0.07223285849149769, 0.2276951792567748, 0.3308691654094995, 0.4441261722726705, -0.0011844524852551279, 10.978502994664627, 0.5508960330107672, 0.764980383853582, 0.18985369519896536, 0.8671104941866773, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.116581034255
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.022915869914158744, 0.011619083421666676, -0.018524435890489088]
###############
Output Layer:
###############
[0.21507951673466782, 0.3915671896522227, 0.3882798010941143]
Weights updated in this iteration:
[-0.11836429980775019, -0.01908784802319578, -0.09589809216285554, -0.08623111448881068, 0.15393360996659766, -0.028682789925771994, -0.14228148933581405, -0.030612774394583087, -0.0852341261662939, 0.2283065925351799, 0.3303229729135077, 0.4445492105549563, 0.0006704980905993341, 10.976845915695701, 0.5521794776676799, 0.766357441722951, 0.18862353108352817, 0.8680632837583817, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0976283674092
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.026690263124375117, 0.006292590581318126, -0.02383474800150594]
###############
Output Layer:
###############
[0.21269274657950843, 0.3336892017774383, 0.384914082397096]
Weights updated in this iteration:
[-0.129364607665949, -0.013510337976786548, -0.10479037940563411, -0.10175495044997356, 0.16180469431699257, -0.04123174857819274, -0.15775816725219846, -0.022765600690318397, -0.09774496380561361, 0.22902260308219832, 0.3299599325267467, 0.44512800994266627, 0.0024806991567414044, 10.975928085507439, 0.5536427847000664, 0.7679809623568338, 0.18780035388132968, 0.8693756844663586, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.08392296131
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.03017938952154447, 0.0016598415388663045, -0.02876784158989031]
###############
Output Layer:
###############
[0.2106623388994411, 0.2834231932285394, 0.38204300822126663]
Weights updated in this iteration:
[-0.13835494677504928, -0.011390743915942877, -0.11281886777665331, -0.11369203158661309, 0.1646190226725736, -0.051891715452235865, -0.17046913818534778, -0.01976881734649677, -0.10909602421420875, 0.22983913110171147, 0.329767425000649, 0.4458571799583598, 0.004104861820774132, 10.975545167188702, 0.5550931828332838, 0.7698445419692606, 0.1873609897783215, 0.8710398850115519, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0744197243595
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.03335730877662958, -0.0022215840653701108, -0.03327978494484596]
###############
Output Layer:
###############
[0.20900378078778037, 0.24134079579709328, 0.3796767352797444]
Weights updated in this iteration:
[-0.1459258451181626, -0.010974350749756974, -0.12003566068336088, -0.12293892438573299, 0.16512759415187628, -0.06070611333970398, -0.1812181414570939, -0.01917763102978492, -0.11934227598646724, 0.2307457563409057, 0.32971756136006325, 0.4467214005962198, 0.005474282365898333, 10.975469850187894, 0.5563985529529792, 0.7719253296606454, 0.18724654817103661, 0.8730233502716832, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0279065195756549, 0.44196076849648425]
###############
Hidden middle Layer:
###############
[-0.03335730877662958, -0.0022215840653701108, -0.03327978494484596]
###############
Output Layer:
###############
[0.20900378078778037, 0.24134079579709328, 0.3796767352797444]
Software Analytics - BackPropagation - Weights updated in this iteration: [-0.1459258451181626, -0.010974350749756974, -0.12003566068336088, -0.12293892438573299, 0.16512759415187628, -0.06070611333970398, -0.1812181414570939, -0.01917763102978492, -0.11934227598646724, 0.2307457563409057, 0.32971756136006325, 0.4467214005962198, 0.005474282365898333, 10.975469850187894, 0.5563985529529792, 0.7719253296606454, 0.18724654817103661, 0.8730233502716832, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.101188246376
0.569913930495
0.101188246376
self.sum1_output_product1_level3 =  0.057668591212
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.057668591212
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0549847399648
self.product1_product2_sum1_level2 =  0.13401607318
0.569913930495
0.13401607318
self.sum1_output_product1_level3 =  0.0763776270158
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0763776270158
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0728230718375
self.product1_product2_sum1_level2 =  0.144719072304
0.569913930495
0.144719072304
self.sum1_output_product1_level3 =  0.0824774153144
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0824774153144
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0786389807472
self.product1_product2_sum1_level2 =  0.14820861765
0.569913930495
0.14820861765
self.sum1_output_product1_level3 =  0.0844661558181
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0844661558181
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0805351668192
self.product1_product2_sum1_level2 =  0.149346329293
0.569913930495
0.149346329293
self.sum1_output_product1_level3 =  0.0851145535325
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0851145535325
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0811533886098
self.product1_product2_sum1_level2 =  0.149717262367
0.569913930495
0.149717262367
self.sum1_output_product1_level3 =  0.0853259534588
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0853259534588
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0813549501485
self.product1_product2_sum1_level2 =  0.149838199291
0.569913930495
0.149838199291
self.sum1_output_product1_level3 =  0.0853948770961
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0853948770961
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0814206661334
self.product1_product2_sum1_level2 =  0.149877628882
0.569913930495
0.149877628882
self.sum1_output_product1_level3 =  0.0854173485693
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0854173485693
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0814420918018
self.product1_product2_sum1_level2 =  0.149890484283
0.569913930495
0.149890484283
self.sum1_output_product1_level3 =  0.0854246750414
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0854246750414
####################################################################################################################
self.cell_input_product1_level1 =  0.336750764005
self.forget_feedback_product2_level1 =  0.0814490773056
self.product1_product2_sum1_level2 =  0.149894675585
0.569913930495
0.149894675585
self.sum1_output_product1_level3 =  0.0854270637229
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0854270637229
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0854270637229
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44069127405916864,
   0.44069127405916864,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.24534563702958434,
   0.24534563702958434,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.4965043132104784,
   0.4965043132104784,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.2732521566052392,
   0.2732521566052392,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.5523173523617882,
   0.5523173523617882,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3011586761808941,
   0.3011586761808941,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.44069127405916864, 0.05, 0.05, 0.05, 0.05],
  [0.44069127405916864, 0.05, 0.05, 0.05, 0.05],
  [0.44069127405916864, 0.05, 0.05, 0.05, 0.05],
  [0.44069127405916864, 0.05, 0.05, 0.05, 0.05],
  [0.44069127405916864, 0.05, 0.05, 0.05, 0.05]],
 [[0.4965043132104784, 0.05, 0.05, 0.05, 0.05],
  [0.4965043132104784, 0.05, 0.05, 0.05, 0.05],
  [0.4965043132104784, 0.05, 0.05, 0.05, 0.05],
  [0.4965043132104784, 0.05, 0.05, 0.05, 0.05],
  [0.4965043132104784, 0.05, 0.05, 0.05, 0.05]],
 [[0.5523173523617882, 0.05, 0.05, 0.05, 0.05],
  [0.5523173523617882, 0.05, 0.05, 0.05, 0.05],
  [0.5523173523617882, 0.05, 0.05, 0.05, 0.05],
  [0.5523173523617882, 0.05, 0.05, 0.05, 0.05],
  [0.5523173523617882, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.08201803594686369, 0.085085980971295, 0.0882382228747596]
Scheduled Classes by Deep Learning for process id  8770  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 9932
Process cmdline: ['/opt/google/chrome/chrome --type=renderer --lang=en-US --force-fieldtrials=AffiliationBasedMatching/EnabledThroughFieldTrial/AutofillEnabled/Default/CaptivePortalInterstitial/Enabled/ChildAccountDetection/Disabled/*ClientSideDetectionModel/Model0/DataReductionProxyUseDataSaverOnVPN/Enabled/*DomRel-Enable/enable/*EnableSessionCrashedBubbleUI/Enabled/EnforceCTForProblematicRoots/disabled/*ExtensionContentVerification/Enforce/InstanceID/Enabled/*NewVideoRendererTrial/Enabled/PasswordBranding/SmartLockBrandingSavePromptOnly/*PasswordGeneration/Disabled/ReportCertificateErrors/ShowAndPossiblySend/SHA1IdentityUIWarning/Enabled/SHA1ToolbarUIJanuary2016/Warning/SHA1ToolbarUIJanuary2017/Error/*SafeBrowsingIncidentReportingService/Default/*SlimmingPaint/EnableSlimmingPaint/*UMA-Population-Restrict/normal/*UMA-Uniformity-Trial-1-Percent/group_86/*UMA-Uniformity-Trial-10-Percent/group_07/*UMA-Uniformity-Trial-100-Percent/group_01/*UMA-Uniformity-Trial-20-Percent/group_03/*UMA-Uniformity-Trial-5-Percent/group_14/*UMA-Uniformity-Trial-50-Percent/default/ --enable-offline-auto-reload --enable-offline-auto-reload-visible-only --enable-delegated-renderer --num-raster-threads=1 --gpu-rasterization-msaa-sample-count=8 --content-image-texture-target=3553 --video-image-texture-target=3553 --disable-accelerated-video-decode --channel=6299.43.1325973486 --v8-natives-passed-by-fd --v8-snapshot-passed-by-fd']
Process executable: /opt/google/chrome/chrome
Process name: chrome
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 69330, 100340, 11, 4.951557438257406, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
get_expected_priority(): proc_name: chrome
mongod 0.01
java 0.8
gnome 0.1
python 0.9
chrome 0.3
Expected output layer: [0.03, 0.06, 0.09]
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[0.059832101184687986, 0.08570570939427148, 0.10395391914893858]
###############
Output Layer:
###############
[0.37382916979242253, 1.349695233075874, 0.6921055921889734]
Error before Backpropagation:
1.07203171818
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
5.73907691582
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[0.268960303784443, -0.3359636191684989, 0.2943336662341512]
###############
Output Layer:
###############
[0.2738024368678667, -3.2526769252719605, 0.7569975953328762]
Weights updated in this iteration:
[0.17448595942483033, 0.2586157574073329, 0.3417823774967192, -0.28865629124358255, -0.37607670887845623, -0.4882286565659745, 0.1827397049517051, 0.23549267839951954, 0.38016183409448895, 0.22518448220651274, 0.33310207463185026, 0.4416333884745375, 0.03642057310893695, 11.052170172750781, 0.6232780938218723, 0.7623231850001608, 0.19900344693864466, 0.8666620929231402, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
726104824.056
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[20.829888244636432, -1430.7242145813454, 19.316312140867502]
###############
Output Layer:
###############
[-495.1441344819983, -38103.179584606034, -327.9097667063494]
Weights updated in this iteration:
[29.789095204278418, -36.73357662997122, 32.75019503349745, -2060.5258811155986, 2573.1068672737656, -2255.085532281787, 27.58074728568057, -33.9878988604676, 30.362861530961187, 0.21214626159694233, 0.34938837309996623, 0.42736515922588175, -12.288115739128195, 26.446991511706557, -12.863938816536558, 0.7293229125069929, 0.24022473805243258, 0.8305486179866134, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
2.23543417127e+89
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[1.429390908672234e+24, 8.447833552346839e+27, 6.27541768535857e+23]
###############
Output Layer:
###############
[1.4702719010045175e+39, 6.686455221198188e+44, 4.275676735689102e+38]
Weights updated in this iteration:
[-5.1784104095661915e+23, 3.55684921541247e+25, -4.80212810985458e+23, -3.060488837612066e+27, 2.1021310517856432e+29, -2.8381024898766694e+27, -2.2734640376594307e+23, 1.5615542490952986e+25, -2.108265799446828e+23, -2533870358.636166, 174041724873.63037, -2349750041.4917984, -1152345404902270.0, 7.915013537241291e+16, -1068611759879134.2, -736871222.6918843, 50612825666.66535, -683327455.9498317, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[-1.2658002534712522e+260, -2.6179523927140163e+271, -1.0704825424967739e+259]
###############
Output Layer:
###############
[-inf, -inf, -inf]
Weights updated in this iteration:
[-4.3209483661972256e+257, -2.5537207746638473e+261, -1.8970147095690713e+257, -8.936668390655094e+268, -5.2816543479279263e+272, -3.923442252686218e+268, -3.654209880555327e+256, -2.1596721127146402e+260, -1.6042982483882073e+256, 4.543013083105746e+141, 2.684963092976533e+145, 1.9945072039823977e+141, 4.273058485493487e+158, 2.5254174086236412e+162, 1.8759897399477718e+158, 1.1172892078315213e+140, 6.603283398774211e+143, 4.905205715208558e+139, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.049515574382574055, 0.690950767390871]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.115980928943
0.569913930495
0.115980928943
self.sum1_output_product1_level3 =  0.0660991470767
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0660991470767
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0630229443364
self.product1_product2_sum1_level2 =  0.153631678371
0.569913930495
0.153631678371
self.sum1_output_product1_level3 =  0.0875568336691
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0875568336691
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0834820069341
self.product1_product2_sum1_level2 =  0.16590711593
0.569913930495
0.16590711593
self.sum1_output_product1_level3 =  0.0945527765367
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0945527765367
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0901523640782
self.product1_product2_sum1_level2 =  0.169909330216
0.569913930495
0.169909330216
self.sum1_output_product1_level3 =  0.0968336942114
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0968336942114
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0923271296237
self.product1_product2_sum1_level2 =  0.171214189543
0.569913930495
0.171214189543
self.sum1_output_product1_level3 =  0.0975773517193
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0975773517193
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0930361778914
self.product1_product2_sum1_level2 =  0.171639618504
0.569913930495
0.171639618504
self.sum1_output_product1_level3 =  0.0978198096104
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0978198096104
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0932673519813
self.product1_product2_sum1_level2 =  0.171778322958
0.569913930495
0.171778322958
self.sum1_output_product1_level3 =  0.097898859211
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.097898859211
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0933427226751
self.product1_product2_sum1_level2 =  0.171823545374
0.569913930495
0.171823545374
self.sum1_output_product1_level3 =  0.097924632096
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.097924632096
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0933672961102
self.product1_product2_sum1_level2 =  0.171838289435
0.569913930495
0.171838289435
self.sum1_output_product1_level3 =  0.0979330349417
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0979330349417
####################################################################################################################
self.cell_input_product1_level1 =  0.386059705898
self.forget_feedback_product2_level1 =  0.0933753078941
self.product1_product2_sum1_level2 =  0.171843096506
0.569913930495
0.171843096506
self.sum1_output_product1_level3 =  0.0979357745581
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0979357745581
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0979357745581
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.7432180413560369,
   0.7432180413560369,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.3966090206780184,
   0.3966090206780184,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.8422491901211849,
   0.8422491901211849,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.44612459506059243,
   0.44612459506059243,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.941280338886333,
   0.941280338886333,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.49564016944316647,
   0.49564016944316647,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.7432180413560369, 0.05, 0.05, 0.05, 0.05],
  [0.7432180413560369, 0.05, 0.05, 0.05, 0.05],
  [0.7432180413560369, 0.05, 0.05, 0.05, 0.05],
  [0.7432180413560369, 0.05, 0.05, 0.05, 0.05],
  [0.7432180413560369, 0.05, 0.05, 0.05, 0.05]],
 [[0.8422491901211849, 0.05, 0.05, 0.05, 0.05],
  [0.8422491901211849, 0.05, 0.05, 0.05, 0.05],
  [0.8422491901211849, 0.05, 0.05, 0.05, 0.05],
  [0.8422491901211849, 0.05, 0.05, 0.05, 0.05],
  [0.8422491901211849, 0.05, 0.05, 0.05, 0.05]],
 [[0.941280338886333, 0.05, 0.05, 0.05, 0.05],
  [0.941280338886333, 0.05, 0.05, 0.05, 0.05],
  [0.941280338886333, 0.05, 0.05, 0.05, 0.05],
  [0.941280338886333, 0.05, 0.05, 0.05, 0.05],
  [0.941280338886333, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.09981748435928056, 0.10643684207192532, 0.11358649783147605]
Scheduled Classes by Deep Learning for process id  9932  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lower']
========================================================================================
Process id: 11530
Process cmdline: ['python', 'DeepLearning_SchedulerAnalytics.py']
Process executable: /usr/bin/python2.7
Process name: python
========================================================================================
Process perf variables: [cpu_percent,num_involuntary_ctx_switches,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, 4114, 4139, 1, 0.3066140444522107, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
get_expected_priority(): proc_name: python
mongod 0.01
java 0.8
gnome 0.1
python 0.9
get_expected_priority(): proc_name: python
mongod 0.01
java 0.8
gnome 0.1
python 0.9
get_expected_priority(): proc_name: python
mongod 0.01
java 0.8
gnome 0.1
python 0.9
Expected output layer: [0.09, 0.18, 0.27]
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[0.07573227527709524, 0.10777201854909105, 0.13933957619263046]
###############
Output Layer:
###############
[0.521342861782781, 1.762109744508714, 0.9756306823095061]
Error before Backpropagation:
1.59352128395
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
4933.13746015
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[1.7552108663813488, -8.930324359160396, 0.2993525955289701]
###############
Output Layer:
###############
[-2.090949505280351, -99.12511437415162, 0.38242190281432087]
Weights updated in this iteration:
[0.9261723396400545, 1.3267736159462082, 1.7416626195075586, -4.887371811893131, -6.917244004568651, -8.983375661534604, 0.12028869964448587, 0.14521757200336938, 0.28060220520730916, 0.22184823611849197, 0.32839950278753344, 0.43500159515456605, 0.1609044583721402, 11.228977648022129, 0.8560476092266525, 0.7687294631052423, 0.20819194226915627, 0.8776623431449155, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
8.62584274483e+35
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[-97188290.96875675, -149246867388.4536, 3551511.1083257305]
###############
Output Layer:
###############
[-18833460442992.8, -1.3134567174484196e+18, -66530341516.73062]
Weights updated in this iteration:
[-631418976.2710918, 3212591935.950629, -107689002.07583173, -969636380163.6105, 4933405752608.91, -165372248255.2886, 23073677.614304647, -117396392.57712138, 3935234.148241308, -24.518759581261936, 126.20595466372016, -3.784528551879087, -1729926.2992238745, 8801691.244946541, -295039.46408287616, 0.7221263558901815, 0.44530354609214573, 0.8697141465084025, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[-4.932231577484355e+99, -2.3989143528099644e+109, -6.154924308689443e+94]
###############
Output Layer:
###############
[-2.3917200254532505e+160, -8.112734532535487e+174, -1.054336461019657e+153]
Weights updated in this iteration:
[-1.0556581680799288e+99, -1.6211178635666307e+102, 3.857647534656117e+97, -5.134457884395733e+108, -7.884712729750069e+111, 1.8762634911981456e+107, -1.3173542276568683e+94, -2.0229905244648057e+97, 4.813952510624213e+92, 6.492385780670582e+47, 9.970010069977667e+50, -2.3724854084531654e+46, 2.2022227418281126e+62, 3.381835838795813e+65, -8.047490549169536e+60, 2.8620235540609946e+40, 4.395056704648586e+43, -1.0458573088609108e+39, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.003066140444522107, 0.9939598936941291]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0820172518574
0.569913930495
0.0820172518574
self.sum1_output_product1_level3 =  0.0467427743745
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0467427743745
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0445674021196
self.product1_product2_sum1_level2 =  0.108594675955
0.569913930495
0.108594675955
self.sum1_output_product1_level3 =  0.0618896186044
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0618896186044
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0590093240352
self.product1_product2_sum1_level2 =  0.117259829104
0.569913930495
0.117259829104
self.sum1_output_product1_level3 =  0.0668280100941
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0668280100941
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0637178866375
self.product1_product2_sum1_level2 =  0.120084966666
0.569913930495
0.120084966666
self.sum1_output_product1_level3 =  0.0684380953459
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0684380953459
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0652530397777
self.product1_product2_sum1_level2 =  0.12100605855
0.569913930495
0.12100605855
self.sum1_output_product1_level3 =  0.0689630384419
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0689630384419
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0657535524316
self.product1_product2_sum1_level2 =  0.121306366142
0.569913930495
0.121306366142
self.sum1_output_product1_level3 =  0.0691341879222
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0691341879222
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0659167367486
self.product1_product2_sum1_level2 =  0.121404276732
0.569913930495
0.121404276732
self.sum1_output_product1_level3 =  0.0691899885315
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0691899885315
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.065969940441
self.product1_product2_sum1_level2 =  0.121436198948
0.569913930495
0.121436198948
self.sum1_output_product1_level3 =  0.0692081814468
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0692081814468
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0659872866722
self.product1_product2_sum1_level2 =  0.121446606687
0.569913930495
0.121446606687
self.sum1_output_product1_level3 =  0.0692141129621
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0692141129621
####################################################################################################################
self.cell_input_product1_level1 =  0.272847448944
self.forget_feedback_product2_level1 =  0.0659929421394
self.product1_product2_sum1_level2 =  0.121449999967
0.569913930495
0.121449999967
self.sum1_output_product1_level3 =  0.0692160468398
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0692160468398
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0692160468398
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09292596622330951,
   0.09292596622330951,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07146298311165475,
   0.07146298311165475,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.09905824711235371,
   0.09905824711235371,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07452912355617686,
   0.07452912355617686,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.10519052800139791,
   0.10519052800139791,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.07759526400069897,
   0.07759526400069897,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.09292596622330951, 0.05, 0.05, 0.05, 0.05],
  [0.09292596622330951, 0.05, 0.05, 0.05, 0.05],
  [0.09292596622330951, 0.05, 0.05, 0.05, 0.05],
  [0.09292596622330951, 0.05, 0.05, 0.05, 0.05],
  [0.09292596622330951, 0.05, 0.05, 0.05, 0.05]],
 [[0.09905824711235371, 0.05, 0.05, 0.05, 0.05],
  [0.09905824711235371, 0.05, 0.05, 0.05, 0.05],
  [0.09905824711235371, 0.05, 0.05, 0.05, 0.05],
  [0.09905824711235371, 0.05, 0.05, 0.05, 0.05],
  [0.09905824711235371, 0.05, 0.05, 0.05, 0.05]],
 [[0.10519052800139791, 0.05, 0.05, 0.05, 0.05],
  [0.10519052800139791, 0.05, 0.05, 0.05, 0.05],
  [0.10519052800139791, 0.05, 0.05, 0.05, 0.05],
  [0.10519052800139791, 0.05, 0.05, 0.05, 0.05],
  [0.10519052800139791, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06420165115377656, 0.06450319243114055, 0.06480500432131285]
Scheduled Classes by Deep Learning for process id  11530  - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
