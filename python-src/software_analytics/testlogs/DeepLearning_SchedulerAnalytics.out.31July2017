========================================================================================
Process id: 1
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=3942, involuntary=6974), 1, 0.06122724796830654, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.02140822670327105, 0.026061497548862347, 0.02128577220733444]
###############
Output Layer:
###############
[0.033784801308365545, 0.30667647303748585, 0.041957249046779804]
Error before Backpropagation:
0.0597077221829
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0587312187977
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.02136199772158003, 0.025760380399152696, 0.02122884616544197]
###############
Output Layer:
###############
[0.033673859994793894, 0.3032929861899212, 0.04186604209965753]
Weights updated in this iteration:
[0.009379772647052198, 0.02224496064701535, 0.0553833203312637, 0.038960086381745286, 0.09408197477917807, 0.08398319458181285, 0.032236256413503916, 0.02007024986780499, 0.11924062500681178, 0.23003928526426878, 0.3400478242702041, 0.4500390605535863, -0.0013504552577607788, 10.998356011131712, 0.5586572693109374, 0.7701446080175814, 0.21017603987233419, 0.8801437808635091, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0578082553218
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021317664969566368, 0.025472494355303153, 0.02117419540614237]
###############
Output Layer:
###############
[0.03356780403527472, 0.300057883732401, 0.04177888959184242]
Weights updated in this iteration:
[0.00877933274591759, 0.021520891650208384, 0.05478662303455427, 0.03506097658468641, 0.08938004788547599, 0.08010838834211775, 0.03149607009480506, 0.01917766101050589, 0.11850505234651001, 0.23007843857439803, 0.34009503915038336, 0.4500779698170566, -0.0026743580299767545, 10.996759520041868, 0.5573416185625534, 0.770288681959656, 0.2103497782839668, 0.8802869567777009, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0569344161139
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021275094778958746, 0.02519689484469714, 0.02112166220067298]
###############
Output Layer:
###############
[0.033466284826059706, 0.29696056775528756, 0.04169550264475236]
Weights updated in this iteration:
[0.00819745866373923, 0.020825609859498873, 0.05420866501075552, 0.0312939224561109, 0.08487879199449305, 0.07636668678437929, 0.030778015740805634, 0.018319657307700566, 0.11779183055484001, 0.23011746519117246, 0.34014167208494317, 0.45011673378162786, -0.003973006100876737, 10.995207764457813, 0.5560517104954034, 0.7704322450357903, 0.21052132191464498, 0.8804295536630311, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0561057597076
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021234166412088678, 0.024932727960248417, 0.021071103879455454]
###############
Output Layer:
###############
[0.03336898725163083, 0.2939914584055318, 0.04161562019282989]
Weights updated in this iteration:
[0.007633036373790932, 0.02015714331513848, 0.053648313244021736, 0.027650931316924133, 0.08056426058544684, 0.07274996831419658, 0.03008079164857326, 0.017493908594583513, 0.11709963473116416, 0.23015637002287173, 0.34018774854022865, 0.45015535803790796, -0.005247601057805683, 10.993698213716302, 0.554786307713294, 0.7705753187872325, 0.21069076953908947, 0.8805715955894781, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0553187571281
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021194770503607626, 0.024679219745297347, 0.02102239103752586]
###############
Output Layer:
###############
[0.03327562572033462, 0.29114187345537257, 0.04153900567532671]
Weights updated in this iteration:
[0.007085047426903362, 0.01951370579690407, 0.05310453244280847, 0.024124684427473432, 0.07642381293569621, 0.06925080036617098, 0.029403206082505167, 0.016698301351958305, 0.11642725251597572, 0.23019515765746248, 0.34023329219486076, 0.4501938478124458, -0.006499257568072538, 10.992228544002666, 0.5535442629898328, 0.7707179232498148, 0.21085821280964123, 0.8807131049563367, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0545702395138
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.02115680772380496, 0.024435666959597535, 0.020975405992603644]
###############
Output Layer:
###############
[0.03318594074931385, 0.2884039245357212, 0.04146544418962959]
Weights updated in this iteration:
[0.006552558525801014, 0.018893674992607325, 0.052576374334174523, 0.020708465807766294, 0.07244596330531788, 0.06586236623894436, 0.028744165436142433, 0.015930913522498192, 0.11577357192174126, 0.23023383239114664, 0.3402783251076267, 0.45023220800014596, -0.0077290115793130915, 10.990796616751041, 0.5523245107070769, 0.7708600770971424, 0.211023736943778, 0.8808541026503434, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.053857353463
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021120187627613533, 0.024201429095016443, 0.020930041455241827]
###############
Output Layer:
###############
[0.0330996960126516, 0.2857704274276104, 0.041394740033141364]
Weights updated in this iteration:
[0.006034712457536109, 0.01829557369936636, 0.05206296835793604, 0.017396099925983767, 0.0686202505327572, 0.06257840109290475, 0.028102663917722814, 0.015189992790844506, 0.11513757073624184, 0.23027239825374965, 0.3403228678660087, 0.4502704431930972, -0.008937827586412522, 10.989400459743305, 0.5511260592747873, 0.771001797766876, 0.21118742133001003, 0.8809946081852054, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0531775228205
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021084827659862113, 0.023975921449409354, 0.02088619937774345]
###############
Output Layer:
###############
[0.033016675781524646, 0.2832348242545243, 0.04132671457349371]
Weights updated in this iteration:
[0.005530720180899855, 0.017718053576794113, 0.05156351355056839, 0.014181897002853504, 0.06493712491319936, 0.05939313580962102, 0.02747777453416323, 0.014473937783422462, 0.11451830726503373, 0.23031085903136006, 0.34036693971795856, 0.4503085577062815, -0.010126605087060977, 10.988038250519647, 0.5499479844011957, 0.7711431015724582, 0.21134934006349615, 0.8811346398251114, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0525284158553
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021050652292532595, 0.023758609099554982, 0.020843789954359787]
###############
Output Layer:
###############
[0.03293668269726341, 0.28079111578415133, 0.041261204397210774]
Weights updated in this iteration:
[0.005039853899629996, 0.017159881056167173, 0.05107727144397861, 0.011060604873978908, 0.06138784976095245, 0.05630124761534576, 0.026868641185385567, 0.013781281739080647, 0.11391491221834706, 0.2303492182865712, 0.3404105586891276, 0.450346555600561, -0.011296184330323603, 10.98670830177557, 0.5487894231050389, 0.7712840038022639, 0.21150956242093552, 0.8812742146943948, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0519079169693
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021017592273974657, 0.023549001640523176, 0.020802730750000286]
###############
Output Layer:
###############
[0.032859535828086275, 0.2784338023466496, 0.041198059695354006]
Weights updated in this iteration:
[0.004561440979965586, 0.016619925078151444, 0.05060355983303369, 0.00802736651438445, 0.057964415489657, 0.053297816540132874, 0.026274471711393875, 0.013110678278661301, 0.11332658157941629, 0.23038747937662446, 0.3404537416874341, 0.4503844407032843, -0.012447351447668105, 10.985409048471245, 0.5476495683745132, 0.7714245188078466, 0.211668153282818, 0.8814133488751841, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.06122724796830654, 0.0]
###############
Hidden middle Layer:
###############
[0.021017592273974657, 0.023549001640523176, 0.020802730750000286]
###############
Output Layer:
###############
[0.032859535828086275, 0.2784338023466496, 0.041198059695354006]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.004561440979965586, 0.016619925078151444, 0.05060355983303369, 0.00802736651438445, 0.057964415489657, 0.053297816540132874, 0.026274471711393875, 0.013110678278661301, 0.11332658157941629, 0.23038747937662446, 0.3404537416874341, 0.4503844407032843, -0.012447351447668105, 10.985409048471245, 0.5476495683745132, 0.7714245188078466, 0.211668153282818, 0.8814133488751841, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0800691497859
0.569913930495
0.0800691497859
self.sum1_output_product1_level3 =  0.0456325238659
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.0456325238659
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0435088218036
self.product1_product2_sum1_level2 =  0.106011425694
0.569913930495
0.106011425694
self.sum1_output_product1_level3 =  0.0604173882946
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0604173882946
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0576056101756
self.product1_product2_sum1_level2 =  0.114469498717
0.569913930495
0.114469498717
self.sum1_output_product1_level3 =  0.0652377619356
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0652377619356
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0622016473878
self.product1_product2_sum1_level2 =  0.117227121044
0.569913930495
0.117227121044
self.sum1_output_product1_level3 =  0.066809369315
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.066809369315
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0637001133858
self.product1_product2_sum1_level2 =  0.118126200643
0.569913930495
0.118126200643
self.sum1_output_product1_level3 =  0.067321767303
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.067321767303
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0641886647711
self.product1_product2_sum1_level2 =  0.118419331474
0.569913930495
0.118419331474
self.sum1_output_product1_level3 =  0.0674888266472
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0674888266472
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0643479493037
self.product1_product2_sum1_level2 =  0.118514902194
0.569913930495
0.118514902194
self.sum1_output_product1_level3 =  0.0675432937316
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0675432937316
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0643998815325
self.product1_product2_sum1_level2 =  0.118546061531
0.569913930495
0.118546061531
self.sum1_output_product1_level3 =  0.067561051872
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.067561051872
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0644168132229
self.product1_product2_sum1_level2 =  0.118556220545
0.569913930495
0.118556220545
self.sum1_output_product1_level3 =  0.0675668416357
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0675668416357
####################################################################################################################
self.cell_input_product1_level1 =  0.266353775372
self.forget_feedback_product2_level1 =  0.0644223335356
self.product1_product2_sum1_level2 =  0.118559532733
0.569913930495
0.118559532733
self.sum1_output_product1_level3 =  0.0675687292976
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0675687292976
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0675687292976
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05857181471556292,
   0.05857181471556292,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05428590735778146,
   0.05428590735778146,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05979635967492905,
   0.05979635967492905,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05489817983746453,
   0.05489817983746453,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.06102090463429518,
   0.06102090463429518,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05551045231714759,
   0.05551045231714759,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05857181471556292, 0.05, 0.05, 0.05, 0.05],
  [0.05857181471556292, 0.05, 0.05, 0.05, 0.05],
  [0.05857181471556292, 0.05, 0.05, 0.05, 0.05],
  [0.05857181471556292, 0.05, 0.05, 0.05, 0.05],
  [0.05857181471556292, 0.05, 0.05, 0.05, 0.05]],
 [[0.05979635967492905, 0.05, 0.05, 0.05, 0.05],
  [0.05979635967492905, 0.05, 0.05, 0.05, 0.05],
  [0.05979635967492905, 0.05, 0.05, 0.05, 0.05],
  [0.05979635967492905, 0.05, 0.05, 0.05, 0.05],
  [0.05979635967492905, 0.05, 0.05, 0.05, 0.05]],
 [[0.06102090463429518, 0.05, 0.05, 0.05, 0.05],
  [0.06102090463429518, 0.05, 0.05, 0.05, 0.05],
  [0.06102090463429518, 0.05, 0.05, 0.05, 0.05],
  [0.06102090463429518, 0.05, 0.05, 0.05, 0.05],
  [0.06102090463429518, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.06251695080162543, 0.0625768777437246, 0.06263681323808379]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 2
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=148, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 3
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=40856, involuntary=4), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 5
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=6, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 7
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=216708, involuntary=42), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 8
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 9
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=1656, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 10
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=1310, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 11
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=1311, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 12
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=1762, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 13
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=51895, involuntary=1939), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 15
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=7, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 16
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 17
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=245, involuntary=0), 1, 0.0, 0.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314, 0.24, 0.039599999999999996]
Error before Backpropagation:
0.04268506
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
0.0426763744183
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.0314014258101952, 0.23996643839999998, 0.0396051845004288]
Weights updated in this iteration:
[0.009703284148356304, 0.022703284148356304, 0.055703284148356305, 0.04122052672582102, 0.09722052672582103, 0.08622052672582102, 0.03262896577386047, 0.020628965773860468, 0.11962896577386047, 0.23003564525488002, 0.34003564525488, 0.45003564525488, -0.00083904, 10.99916096, 0.55916096, 0.77012961251072, 0.21012961251072, 0.88012961251072, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
0.0426676917706
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140285164834137, 0.23993288490822118, 0.039610369493869674]
Weights updated in this iteration:
[0.009406673933342076, 0.022406673933342076, 0.05540667393334208, 0.03944193533315997, 0.09544193533315998, 0.08444193533315997, 0.03225803338372283, 0.020258033383722823, 0.11925803338372282, 0.2300712912085342, 0.3400712912085342, 0.4500712912085342, -0.0016778772944697658, 10.99832212270553, 0.5583221227055303, 0.7702592373467418, 0.21025923734674185, 0.8802592373467418, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
0.0426590120555
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140427751443581, 0.2398993395221821, 0.03961555498032257]
Weights updated in this iteration:
[0.009110169311790144, 0.022110169311790145, 0.05511016931179015, 0.03766422534663333, 0.09366422534663334, 0.08266422534663333, 0.03188720279617899, 0.019887202796178977, 0.11888720279617897, 0.23010693786089528, 0.3401069378608953, 0.4501069378608953, -0.0025165119454471944, 10.997483488054552, 0.5574834880545528, 0.7703888745080643, 0.21038887450806426, 0.8803888745080642, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
0.0426503352717
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031405703408475844, 0.23986580223940196, 0.039620740959787425]
Weights updated in this iteration:
[0.008813770240554018, 0.02181377024055402, 0.054813770240554026, 0.035887396291133584, 0.0918873962911336, 0.08088739629113359, 0.031516473977834865, 0.019516473977834854, 0.11851647397783485, 0.23014258521189598, 0.340142585211896, 0.450142585211896, -0.0033549440149502575, 10.996645055985049, 0.5566450559850498, 0.7705185239946856, 0.2105185239946856, 0.8805185239946856, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
0.0426416614177
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140712933045876, 0.23983227305740087, 0.03962592743226416]
Weights updated in this iteration:
[0.00851747667650789, 0.021517476676507892, 0.054517476676507894, 0.03411144769182908, 0.0901114476918291, 0.0791114476918291, 0.0311458468953104, 0.01914584689531039, 0.1181458468953104, 0.23017823326146902, 0.340178233261469, 0.450178233261469, -0.004193173564977007, 10.995806826435022, 0.555806826435023, 0.7706481858066039, 0.2106481858066039, 0.8806481858066039, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
0.0426329904922
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03140855528038188, 0.23979875197369974, 0.03963111439775267]
Weights updated in this iteration:
[0.008221288576546613, 0.021221288576546617, 0.054221288576546615, 0.03233637907416387, 0.08833637907416389, 0.07733637907416388, 0.03077532151523953, 0.01877532151523952, 0.11777532151523952, 0.2302138820095471, 0.34021388200954705, 0.45021388200954704, -0.005031200657505577, 10.994968799342493, 0.5549687993424944, 0.7707778599438169, 0.21077785994381693, 0.8807778599438169, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
0.0426243224938
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031409981258242514, 0.2397652389858202, 0.03963630185625289]
Weights updated in this iteration:
[0.007925205897585705, 0.02092520589758571, 0.05392520589758571, 0.030562189963857508, 0.08656218996385753, 0.07556218996385752, 0.03040489780427019, 0.018404897804270178, 0.11740489780427017, 0.2302495314560629, 0.34024953145606285, 0.45024953145606283, -0.005869025354494193, 10.994130974645504, 0.5541309746455059, 0.770907546406322, 0.2109075464063221, 0.880907546406322, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
0.0426156574211
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141140726403796, 0.23973173409128473, 0.039641489807764656]
Weights updated in this iteration:
[0.007629228596561328, 0.02062922859656133, 0.053629228596561336, 0.028788879886904956, 0.08478887988690498, 0.07378887988690497, 0.030034575729064306, 0.018034575729064296, 0.11703457572906428, 0.2302851816009491, 0.34028518160094906, 0.45028518160094905, -0.006706647717881169, 10.993293352282118, 0.5532933522821188, 0.7710372451941163, 0.21103724519411646, 0.8810372451941163, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
0.0426069952729
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.031412833297765536, 0.23969823728761655, 0.039646678252287876]
Weights updated in this iteration:
[0.007333356630430283, 0.020333356630430285, 0.05333335663043029, 0.027016448369576382, 0.0830164483695764, 0.0720164483695764, 0.0296643552562978, 0.017664355256297788, 0.11666435525629777, 0.23032083244413842, 0.34032083244413835, 0.45032083244413834, -0.00754406780958492, 10.992455932190413, 0.5524559321904151, 0.7711669563071967, 0.21116695630719684, 0.8811669563071967, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
0.0425983360476
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Weights updated in this iteration:
[0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, 0.0]
###############
Hidden middle Layer:
###############
[0.02, 0.02, 0.02]
###############
Output Layer:
###############
[0.03141425935942254, 0.2396647485723398, 0.03965186718982239]
Software Analytics - BackPropagation - Weights updated in this iteration: [0.0070375899561699986, 0.02003758995617, 0.05303758995617001, 0.02524489493841702, 0.08124489493841705, 0.07024489493841704, 0.029294236352660565, 0.017294236352660555, 0.11629423635266053, 0.23035648398556352, 0.34035648398556345, 0.45035648398556344, -0.008381285691503959, 10.991618714308494, 0.5516187143084961, 0.7712966797455595, 0.21129667974555968, 0.8812966797455595, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0795828898384
0.569913930495
0.0795828898384
self.sum1_output_product1_level3 =  0.045355397548
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.045355397548
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0432445927283
self.product1_product2_sum1_level2 =  0.105366628301
0.569913930495
0.105366628301
self.sum1_output_product1_level3 =  0.0600499092782
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0600499092782
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0572552333459
self.product1_product2_sum1_level2 =  0.113773012672
0.569913930495
0.113773012672
self.sum1_output_product1_level3 =  0.0648408248361
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.0648408248361
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0618231834312
self.product1_product2_sum1_level2 =  0.116513782723
0.569913930495
0.116513782723
self.sum1_output_product1_level3 =  0.0664028278685
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0664028278685
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.063312492061
self.product1_product2_sum1_level2 =  0.117407367901
0.569913930495
0.117407367901
self.sum1_output_product1_level3 =  0.0669120945095
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0669120945095
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0637980578298
self.product1_product2_sum1_level2 =  0.117698707362
0.569913930495
0.117698707362
self.sum1_output_product1_level3 =  0.0670781329269
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0670781329269
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0639563689488
self.product1_product2_sum1_level2 =  0.117793694033
0.569913930495
0.117793694033
self.sum1_output_product1_level3 =  0.0671322671542
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0671322671542
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640079838114
self.product1_product2_sum1_level2 =  0.117824662951
0.569913930495
0.117824662951
self.sum1_output_product1_level3 =  0.0671499167717
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0671499167717
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640248120295
self.product1_product2_sum1_level2 =  0.117834759882
0.569913930495
0.117834759882
self.sum1_output_product1_level3 =  0.0671556711533
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0671556711533
####################################################################################################################
self.cell_input_product1_level1 =  0.264732908881
self.forget_feedback_product2_level1 =  0.0640302986066
self.product1_product2_sum1_level2 =  0.117838051828
0.569913930495
0.117838051828
self.sum1_output_product1_level3 =  0.0671575472793
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0671575472793
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0671575472793
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lower', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 18
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference: [0.062097695970929206, 0.062097695970929206, 0.062097695970929206]
Scheduled Classes by Deep Learning - [BackPropagation, LSTM, GRU, Convolution] =  ['Lowest', 'Lowest', 'Lower', 'Lowest']
========================================================================================
Process id: 19
========================================================================================
Process perf variables: [cpu_percent,num_ctx_switches,num_threads,memory_percent,nice]
[0.0, pctxsw(voluntary=2, involuntary=0), 1, 0.0, -1.0]
##########################################################################################
BackPropagation
##########################################################################################
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[-0.036000000000000004, -0.06799999999999999, -0.09999999999999999]
###############
Output Layer:
###############
[-0.4614, -1.2879999999999998, -0.902]
Error before Backpropagation:
1.61269498
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 0
198237.207614
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[12.736388846716375, 55.75296265751567, 32.39694151605343]
###############
Output Layer:
###############
[22.998253916190336, 628.615788770419, 28.57176044504955]
Weights updated in this iteration:
[-4.588059984817896, -8.662224415767135, -12.716388846716375, -20.052546556705643, -37.85925460711065, -55.732962657515664, -11.665898945779235, -22.076920230916325, -32.37694151605343, 0.243384925421984, 0.365282636908192, 0.4871803483944, 0.13770479923199996, 11.260109065216, 0.9425133311999998, 0.838679059328, 0.33972711206399997, 1.0707751648000001, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 1
2.3412064484e+54
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[200625487233024.16, 1.5645238017997232e+17, 312395324973746.8]
###############
Output Layer:
###############
[1.0112362713924559e+23, 2.1638883632410007e+27, 1.9494501293349952e+23]
Weights updated in this iteration:
[-78873007709555.89, -345263002444400.4, -200625487233024.12, -6.1506989756395096e+16, -2.6924404902639184e+17, -1.5645238017997232e+17, -122813702361080.94, -537611393933966.9, -312395324973746.8, 147612.1443445784, 646164.7452379643, 375473.80069309205, 3158666913.188166, 13826920700.497316, 8034549548.883748, 284565.5162108218, 1245669.2676325794, 723834.5942709482, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 2
inf
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[3.491106554819554e+155, 1.5985548039650047e+164, 1.2974244749765744e+156]
###############
Output Layer:
###############
[2.586232831037484e+250, 2.534044626303184e+263, 1.8528748794858596e+251]
Weights updated in this iteration:
[-2.242046847538254e+155, -1.748399820033553e+158, -3.491106554819554e+155, -1.0266185527620251e+164, -8.005808151881653e+166, -1.5985548039650047e+164, -8.332276338069421e+155, -6.497701181376339e+158, -1.2974244749765744e+156, 2.0746460830040972e+83, 1.617856844579511e+86, 3.230445673897873e+83, 2.03278130840544e+96, 1.585209728193693e+99, 3.1652577456536925e+96, 1.4863548110940797e+84, 1.1590937482311262e+87, 2.3144132913616946e+84, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 3
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 4
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 5
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 6
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 7
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 8
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Recomputing Neural Network after backpropagation weight update
Error after Backpropagation- iteration : 9
nan
Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Weights updated in this iteration:
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
Software Analytics - BackPropagation - Error after Backpropagation- iteration : 10
Software Analytics - BackPropagation - Layers in this iteration:
###############
Input Layer:
###############
[0.0, 0.0, -1.0]
###############
Hidden middle Layer:
###############
[nan, nan, nan]
###############
Output Layer:
###############
[nan, nan, nan]
Software Analytics - BackPropagation - Weights updated in this iteration: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.92]
##################################################################################
LSTM Recurrent Neural Network
##################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.000271695290383
self.product1_product2_sum1_level2 =  0.0677584594264
0.569913930495
0.0677584594264
self.sum1_output_product1_level3 =  0.038616489936
Iteration:  0  Final LSTM Recurrent Neural Network out value =  0.038616489936
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0368193086195
self.product1_product2_sum1_level2 =  0.0896870274239
0.569913930495
0.0896870274239
self.sum1_output_product1_level3 =  0.0511138863136
Iteration:  1  Final LSTM Recurrent Neural Network out value =  0.0511138863136
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.048735085919
self.product1_product2_sum1_level2 =  0.0968364938036
0.569913930495
0.0968364938036
self.sum1_output_product1_level3 =  0.055188466799
Iteration:  2  Final LSTM Recurrent Neural Network out value =  0.055188466799
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0526200386073
self.product1_product2_sum1_level2 =  0.0991674654166
0.569913930495
0.0991674654166
self.sum1_output_product1_level3 =  0.0565169199928
Iteration:  3  Final LSTM Recurrent Neural Network out value =  0.0565169199928
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0538866666258
self.product1_product2_sum1_level2 =  0.0999274422277
0.569913930495
0.0999274422277
self.sum1_output_product1_level3 =  0.0569500413643
Iteration:  4  Final LSTM Recurrent Neural Network out value =  0.0569500413643
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0542996308665
self.product1_product2_sum1_level2 =  0.100175220772
0.569913930495
0.100175220772
self.sum1_output_product1_level3 =  0.0570912538085
Iteration:  5  Final LSTM Recurrent Neural Network out value =  0.0570912538085
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544342713937
self.product1_product2_sum1_level2 =  0.100256005088
0.569913930495
0.100256005088
self.sum1_output_product1_level3 =  0.0571372939157
Iteration:  6  Final LSTM Recurrent Neural Network out value =  0.0571372939157
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544781688303
self.product1_product2_sum1_level2 =  0.10028234355
0.569913930495
0.10028234355
self.sum1_output_product1_level3 =  0.0571523045721
Iteration:  7  Final LSTM Recurrent Neural Network out value =  0.0571523045721
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544924809024
self.product1_product2_sum1_level2 =  0.100290930794
0.569913930495
0.100290930794
self.sum1_output_product1_level3 =  0.0571571985617
Iteration:  8  Final LSTM Recurrent Neural Network out value =  0.0571571985617
####################################################################################################################
self.cell_input_product1_level1 =  0.225318140841
self.forget_feedback_product2_level1 =  0.0544971471295
self.product1_product2_sum1_level2 =  0.10029373053
0.569913930495
0.10029373053
self.sum1_output_product1_level3 =  0.0571587941704
Iteration:  9  Final LSTM Recurrent Neural Network out value =  0.0571587941704
####################################################################################################################
Software Analytics - LSTM Recurrent Neural Network - Final LSTM Recurrent Neural Network out value =  0.0571587941704
##################################################################################
GRU Recurrent Neural Network
##################################################################################
Software Analytics - GRU Recurrent Neural Network - state at time t [0.1, 0.1, 0.1]
Software Analytics - GRU Recurrent Neural Network - Cellvars: [0.01, 0.2, 0.3]
Software Analytics - GRU Recurrent Neural Network - Update gate: 0.1
Software Analytics - GRU Recurrent Neural Network - Reset gate: 0.1
##################################################################################
Convolution Neural Network + BackPropagation
##################################################################################
##########################################
Set of Convolution Maps
##########################################
Example 11:
###########
[[[0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.09000000000000001,
   -0.09000000000000001,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05],
  [0.05,
   -0.020000000000000004,
   -0.020000000000000004,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]],
 [[0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.11, -0.11, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.03, -0.03, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, -0.13, -0.13, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05,
   -0.039999999999999994,
   -0.039999999999999994,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05,
   0.05]]]
##########################################
Max Pooling Map
##########################################
Example 11:
###########
[[[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]],
 [[0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05],
  [0.05, 0.05, 0.05, 0.05, 0.05]]]
####################################################################################################
Final layer that connects all neurons in max pooling map and does backpropagation
####################################################################################################
###########################################################################################
Inference from Max Pooling Layer
###########################################################################################
Example 11:
###########
Software Analytics - Convolution Network + BackPropgation - max pooling inference:
