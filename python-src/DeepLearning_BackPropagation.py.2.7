# -------------------------------------------------------------------------------------------------------
# ASFER - Software for Mining Large Datasets
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
# --------------------------------------------------------------------------------------------------------
# Copyleft (Copyright+):
# Srinivasan Kannan (alias) Ka.Shrinivaasan (alias) Shrinivas Kannan
# Ph: 9791499106, 9003082186
# Krishna iResearch Open Source Products Profiles:
# http://sourceforge.net/users/ka_shrinivaasan,
# https://github.com/shrinivaasanka,
# https://www.openhub.net/accounts/ka_shrinivaasan
# Personal website(research): https://sites.google.com/site/kuja27/
# emails: ka.shrinivaasan@gmail.com, shrinivas.kannan@gmail.com,
# kashrinivaasan@live.com
# -----------------------------------------------------------------------------------------------------------------------------------

# Backpropagation on a Multilayered Neural Network example - Reference: http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

import math


class BackPropagation(object):
    def __init__(self, inlayer, hiddenlayer, expectedoutlayer, wgts):
        self.input_layer = inlayer
        self.hidden_layer = hiddenlayer
        self.output_layer = []
        for i in xrange(len(expectedoutlayer)):
            self.output_layer.append(0.0)
        self.expected_output_layer = expectedoutlayer
        self.weights = wgts
        # self.weight_input_neuron_map={0:0,1:1,2:2,3:0,4:1,5:2,6:0,7:1,8:2,9:0,10:1,11:2,12:0,13:1,14:2,15:0,16:1,17:2}
        self.weight_input_neuron_map = {}
        neuronindex = 0
        for k in xrange(len(self.weights)):
            self.weight_input_neuron_map[k] = neuronindex % len(
                self.input_layer)
            neuronindex += 1
        #print "weight_input_neuron_map:",self.weight_input_neuron_map
        self.bias = 0.02

    def sigmoid(self, input):
        # 1/(1+e^(-z))
        return 1/(1+math.exp(-1*input))

    ######################################################################################
    def backpropagation_pde_update_hidden_to_output(self, output_index, weight_index):
        # From PDE chain rule:
        # doe(error)/doe(weight)=doe(error)/doe(output) * doe(output)/doe(input) * doe(input)/doe(weight)
        doe_error_weight = self.doeError_doeOutput(
            output_index) * self.doeOutput_doeInput(output_index) * self.doeInput_doeWeight(weight_index)
        self.weights[weight_index] = self.weights[weight_index] - \
            doe_error_weight

    def doeError_doeOutput(self, index):
        return -1.0 * (self.expected_output_layer[index] - self.output_layer[index])

    def doeOutput_doeInput(self, index):
        return (self.output_layer[index] * (1.0 - self.output_layer[index]))

    def doeInput_doeWeight(self, index):
        return self.hidden_layer[self.weight_input_neuron_map[index]]

    ######################################################################################
    def backpropagation_pde_update_input_to_hidden(self, output_index, weight_index):
        # From PDE chain rule:
        # doe(error)/doe(weight)=doe(error)/doe(output) * doe(output)/doe(input) * doe(input)/doe(weight)
        doe_error_weight = self.doeError_doeOutput_all(
            output_index) * self.doeOutput_doeInput(output_index) * self.doeInput_doeWeight(weight_index)
        self.weights[weight_index] = self.weights[weight_index] - \
            doe_error_weight

    def doeError_doeOutput_all(self, index):
        dE_dO = 0.0
        for i in xrange(len(self.input_layer)):
            dE_dO += ((self.doeError_doeOutput(i) * self.doeOutput_doeInput(i)
                       * self.output_layer[i]) / self.hidden_layer[0])
        return dE_dO

    def compute_neural_network(self):
        for i in xrange(len(self.input_layer)):
            hidden_layer = 0.0
            for k in xrange(len(self.input_layer)):
                hidden_layer += self.input_layer[k] * \
                    self.weights[i*len(self.input_layer)+k]
            hidden_layer += self.bias
            self.hidden_layer[i] = hidden_layer

        i = j = 0
        for i in xrange(len(self.hidden_layer)):
            output_layer = 0.0
            for k in xrange(len(self.hidden_layer)-1):
                output_layer += self.hidden_layer[k]*self.weights[len(
                    self.weights)/2 + i*len(self.input_layer)+k]
            output_layer += self.input_layer[len(self.hidden_layer)-1]*self.weights[len(
                self.weights)/2 + i*len(self.input_layer)+len(self.hidden_layer)-1] + self.bias
            self.output_layer[i] = output_layer

    def print_layers(self):
        print "###############"
        print "Input Layer:"
        print "###############"
        print self.input_layer
        print "###############"
        print "Hidden middle Layer:"
        print "###############"
        print self.hidden_layer
        print "###############"
        print "Output Layer:"
        print "###############"
        print self.output_layer

    def output_error(self, output_layer, expected_output_layer):
        sum_of_squared_error = 0.0
        for i in xrange(len(output_layer)):
            sum_of_squared_error = sum_of_squared_error + \
                (output_layer[i]-expected_output_layer[i]) * \
                (output_layer[i]-expected_output_layer[i])
        return 0.5*sum_of_squared_error

#######################################################################################################
# Software Analytics with Backpropagation - Example learning from top output for CPU and Memory usage:
# ---------------------------------------------------------------------------------------------------
# top - 14:34:33 up  4:46,  2 users,  load average: 0.25, 0.26, 0.41
# Tasks: 212 total,   1 running, 211 sleeping,   0 stopped,   0 zombie
# %Cpu(s):  5.1 us,  3.9 sy,  0.0 ni, 89.4 id,  1.7 wa,  0.0 hi,  0.0 si,  0.0 st
# KiB Mem:   2930068 total,  2803660 used,   126408 free,   156176 buffers
# KiB Swap:  3103740 total,   166656 used,  2937084 free.  1017176 cached Mem
#
#  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
# 4273 shriniv+   9 -11  231168   7592   6412 S   4.6  0.3   6:18.91 pulseaudio
# -----------------------------------------------------------------------------------------
# In above %CPU=4.6 and %MEM=0.3 are used as input to backpropagation as below with expected outputs
# for 2 neurons in output layers being 0.09 and 0.01.
# 4.6% is converted to 0.046 and 0.3% to 0.003.
#######################################################################################################


if __name__ == "__main__":
    iter = 0
    # weights=[0.01,0.023,0.056,0.043,0.099,0.088,0.033,0.021,0.12,0.23,0.34,0.45,11,0.56,0.77,0.21,0.88,0.92]
    # parameters - initial conditions - inputlayer,hiddenlayer,expectedoutputlayer,weights_array - for arbitrary number of variables
    # inputlayer=[0.046,0.003,0.1]
    # hiddenlayer=[0.8,0.9,0.3]
    # expectedoutput=[0.09,0.01,0.21]
    weights = [0.01, 0.023, 0.056, 0.043, 0.099, 0.088, 0.033, 0.021, 0.12, 0.23, 0.34, 0.45, 0.11, 0.56, 0.77, 0.21, 0.88, 0.92, 0.01, 0.023, 0.056, 0.043, 0.099, 0.088, 0.033, 0.021, 0.12, 0.23, 0.34, 0.45, 0.11, 0.56, 0.01, 0.023, 0.056,
               0.043, 0.099, 0.088, 0.033, 0.021, 0.12, 0.23, 0.34, 0.45, 0.11, 0.56, 0.77, 0.21, 0.88, 0.92, 0.01, 0.023, 0.056, 0.043, 0.099, 0.088, 0.033, 0.021, 0.12, 0.23, 0.34, 0.45, 0.11, 0.56, 0.033, 0.021, 0.12, 0.23, 0.34, 0.45, 0.11, 0.56]
    # parameters - initial conditions - inputlayer,hiddenlayer,expectedoutputlayer,weights_array - for arbitrary number of variables
    inputlayer = [0.23, 0.11, 0.05, 0.046, 0.003, 0.1]
    hiddenlayer = [0.06, 0.8, 0.9, 0.3, 0.23, 0.11]
    expectedoutput = [0.3, 0.53, 0.11, 0.09, 0.01, 0.21]
    bpnn = BackPropagation(inputlayer, hiddenlayer, expectedoutput, weights)
    bpnn.compute_neural_network()
    bpnn.print_layers()
    print "Error before Backpropagation:"
    print bpnn.output_error(bpnn.output_layer, bpnn.expected_output_layer)
    while iter < 100000:
        for m in xrange(len(inputlayer)):
            for l in xrange(len(inputlayer)):
                bpnn.backpropagation_pde_update_hidden_to_output(
                    m, len(weights)/2 + len(inputlayer)*m + l)

        for m in xrange(len(inputlayer)):
            for l in xrange(len(inputlayer)):
                bpnn.backpropagation_pde_update_input_to_hidden(
                    m, len(inputlayer)*m+l)

        print "Recomputing Neural Network after backpropagation weight update"
        bpnn.compute_neural_network()
        print "Error after Backpropagation- iteration :", iter
        print bpnn.output_error(bpnn.output_layer, bpnn.expected_output_layer)
        print "Layers in this iteration:"
        bpnn.print_layers()
        print "Weights updated in this iteration:"
        print bpnn.weights
        iter = iter+1
