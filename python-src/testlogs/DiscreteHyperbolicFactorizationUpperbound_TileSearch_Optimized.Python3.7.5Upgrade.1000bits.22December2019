spark-submit DiscreteHyperbolicFactorizationUpperbound_TileSearch_Optimized.py 99320930203909302903909209302309029309029039203819821892891217288172871728718728172817287187281728718728178728173183617637939029309112882189434889893898293112982918998767676767565656545455343787878787876767566565645454534434333878738728738278372243439848343884384983984989483849839483984983984983984977
19/12/22 13:10:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/12/22 13:10:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
('Spark Python version:', '3.7.5 (default, Nov  7 2019, 10:50:52) \n[GCC 8.3.0]')
('factors of ', 99320930203909302903909209302309029309029039203819821892891217288172871728718728172817287187281728718728178728173183617637939029309112882189434889893898293112982918998767676767565656545455343787878787876767566565645454534434333878738728738278372243439848343884384983984989483849839483984983984983984977, '(', 1003.212454334434, ' bits integer) are:')
=================================================
('Factor is = ', 249, '(at ', 'Sun, 22 Dec 2019 07:40:29 GMT', ')')
=================================================
=================================================
('Factor is = ', 4731, '(at ', 'Sun, 22 Dec 2019 07:41:22 GMT', ')')
=================================================
[...truncated...]
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 789, in foreach
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 1055, in count
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 1046, in sum
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 917, in fold
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py", line 816, in collect
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1255, in __call__
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 985, in send_command
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1152, in send_command
  File "/usr/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/home/ksrinivasan/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py", line 270, in signal_handler
KeyboardInterrupt
Exception in thread "serve RDD 5995" java.net.SocketTimeoutException: Accept timed out
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
	at java.net.ServerSocket.implAccept(ServerSocket.java:545)
	at java.net.ServerSocket.accept(ServerSocket.java:513)
	at org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:883)

